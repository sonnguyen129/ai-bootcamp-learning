{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2658fca8",
   "metadata": {},
   "source": [
    "# Vector Search with Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339b56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs_08 as docs\n",
    "\n",
    "github_data = docs.read_github_data('evidentlyai', 'docs')\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7f108",
   "metadata": {},
   "source": [
    "Run Qdrant:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "    -p 6333:6333 -p 6334:6334 \\\n",
    "    -v \"qdrant_storage:/qdrant/storage:z\" \\\n",
    "    qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65763977",
   "metadata": {},
   "source": [
    "Install the python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f872ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m172 packages\u001b[0m \u001b[2min 1.11s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)                                                  \u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/60.15 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m 16.00 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m 32.00 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)---\u001b[2m------\u001b[0m\u001b[0m 48.00 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)---------\u001b[2m\u001b[0m\u001b[0m 60.15 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)---------\u001b[2m\u001b[0m\u001b[0m 60.15 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)---------\u001b[2m\u001b[0m\u001b[0m 60.15 KiB/60.15 KiB         \u001b[1A\n",
      "\u001b[2mportalocker         \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/21.90 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)---------\u001b[2m\u001b[0m\u001b[0m 60.15 KiB/60.15 KiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/21.90 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/21.90 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)---------\u001b[2m\u001b[0m\u001b[0m 21.90 KiB/21.90 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)---------\u001b[2m\u001b[0m\u001b[0m 21.90 KiB/21.90 KiB         \u001b[1A\n",
      "\u001b[2mportalocker         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.90 KiB/21.90 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[2A\n",
      "\u001b[2mportalocker         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.90 KiB/21.90 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[2A\n",
      "\u001b[2mportalocker         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.90 KiB/21.90 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[2A\n",
      "\u001b[2mportalocker         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.90 KiB/21.90 KiB\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[3A\n",
      "\u001b[2mportalocker         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 21.90 KiB/21.90 KiB\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[3A\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[2A\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[2A\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[2A\n",
      "\u001b[2mhyperframe          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/12.70 KiB\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[3A\n",
      "\u001b[2mhyperframe          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 12.70 KiB/12.70 KiB\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[3A\n",
      "\u001b[2mhyperframe          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 12.70 KiB/12.70 KiB\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/15)-------------\u001b[0m\u001b[0m     0 B/416.46 KiB          \u001b[3A\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/39.12 KiB\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/102.85 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/416.46 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m     0 B/10.94 MiB           \u001b[4A\n",
      "\u001b[2mflatbuffers         \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/30.15 KiB\n",
      "\u001b[2mhpack               \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 16.00 KiB/33.55 KiB\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 2.81 KiB/39.12 KiB\n",
      "\u001b[2mcoloredlogs         \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 16.00 KiB/44.94 KiB\n",
      "\u001b[2mh2                  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/60.33 KiB\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 16.00 KiB/84.76 KiB\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 48.00 KiB/102.85 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[12A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)------------\u001b[0m\u001b[0m 16.00 KiB/16.40 MiB         \u001b[12A\n",
      "\u001b[2mflatbuffers         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 30.15 KiB/30.15 KiB\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 18.81 KiB/39.12 KiB\n",
      "\u001b[2mcoloredlogs         \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 32.00 KiB/44.94 KiB\n",
      "\u001b[2mh2                  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 32.00 KiB/60.33 KiB\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 32.00 KiB/84.76 KiB\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 80.00 KiB/102.85 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 25.22 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[11A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)------------\u001b[0m\u001b[0m 32.00 KiB/16.40 MiB         \u001b[11A\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 34.81 KiB/39.12 KiB\n",
      "\u001b[2mcoloredlogs         \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 44.94 KiB/44.94 KiB\n",
      "\u001b[2mh2                  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 32.00 KiB/60.33 KiB\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 32.00 KiB/84.76 KiB\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 80.00 KiB/102.85 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 28.46 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[10A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)------------\u001b[0m\u001b[0m 32.00 KiB/16.40 MiB         \u001b[10A\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 34.81 KiB/39.12 KiB\n",
      "\u001b[2mh2                  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 32.00 KiB/60.33 KiB\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 32.00 KiB/84.76 KiB\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 80.00 KiB/102.85 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 28.46 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[9A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 48.00 KiB/16.40 MiB         \u001b[9A\n",
      "\u001b[2mmmh3                \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 34.81 KiB/39.12 KiB\n",
      "\u001b[2mh2                  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 32.00 KiB/60.33 KiB\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 32.00 KiB/84.76 KiB\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 80.00 KiB/102.85 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 28.46 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[9A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 48.00 KiB/16.40 MiB         \u001b[9A\n",
      "\u001b[2mh2                  \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 48.00 KiB/60.33 KiB\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 48.00 KiB/84.76 KiB\n",
      "\u001b[2mfastembed           \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 96.00 KiB/102.85 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 40.04 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[8A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 48.00 KiB/16.40 MiB         \u001b[8A\n",
      "\u001b[2mh2                  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 60.33 KiB/60.33 KiB\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 64.00 KiB/84.76 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 48.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 40.04 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 64.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[7A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 64.00 KiB/16.40 MiB         \u001b[7A\n",
      "\u001b[2mhumanfriendly       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 64.00 KiB/84.76 KiB\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 48.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 40.04 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 64.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[6A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 64.00 KiB/16.40 MiB         \u001b[6A\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 96.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 96.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 96.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 96.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 96.00 KiB/16.40 MiB         \u001b[5A\n",
      "\u001b[2mpy-rust-stemmers    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 144.00 KiB/265.64 KiB\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 160.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 144.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 144.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 160.00 KiB/16.40 MiB        \u001b[5A\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 256.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 272.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 272.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/15)-------------\u001b[0m\u001b[0m 287.89 KiB/16.40 MiB        \u001b[4A\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 272.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 272.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 272.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/15)------------\u001b[0m\u001b[0m 287.89 KiB/16.40 MiB        \u001b[4A\n",
      "\u001b[2mqdrant-client       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 272.00 KiB/329.42 KiB\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 304.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 336.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/15)------------\u001b[0m\u001b[0m 352.00 KiB/16.40 MiB        \u001b[4A\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 352.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 400.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/15)------------\u001b[0m\u001b[0m 416.00 KiB/16.40 MiB        \u001b[3A\n",
      "\u001b[2mprotobuf            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 416.00 KiB/416.46 KiB\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 400.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/15)------------\u001b[0m\u001b[0m 432.00 KiB/16.40 MiB        \u001b[3A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 416.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/15)------------\u001b[0m\u001b[0m 432.00 KiB/16.40 MiB        \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 480.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/15)------------\u001b[0m\u001b[0m 480.00 KiB/16.40 MiB        \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 720.00 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 768.00 KiB/16.40 MiB        \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 956.41 KiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 1.03 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.03 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 1.08 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1.05 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 1.11 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 1.24 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 1.30 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.48 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 1.53 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.78 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 1.77 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.03 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 2.03 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.26 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 2.30 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 2.51 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 2.58 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.67 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 2.72 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.67 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 3.06 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.89 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 3.34 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.20 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 3.34 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 3.72 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 3.65 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 3.94 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 3.89 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 4.02 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 4.30 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 4.03 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 4.70 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.48 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 4.78 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 4.77 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 5.00 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 5.17 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 5.12 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 5.39 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 5.48 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 5.53 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 5.84 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 5.58 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 6.17 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.15 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 6.17 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.20 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 6.55 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.20 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 6.61 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 6.87 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 6.87 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 6.87 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 7.23 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 6.87 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 7.29 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 6.87 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 7.48 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.06 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 7.51 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.48 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 7.51 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.54 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 7.72 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.54 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 7.87 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.54 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.11 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 7.64 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.19 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 7.78 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.19 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.05 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.19 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.16 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.19 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.19 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.58 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 8.30 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.86 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 8.76 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.86 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 9.19 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 8.86 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 9.19 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 9.17 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 9.19 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 9.19 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 9.19 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 9.20 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 9.19 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)------------\u001b[0m\u001b[0m 9.22 MiB/16.40 MiB          \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 10.07 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)m-----------\u001b[0m\u001b[0m 10.36 MiB/16.40 MiB         \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 10.53 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)2m----------\u001b[0m\u001b[0m 10.87 MiB/16.40 MiB         \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.65 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)[2m---------\u001b[0m\u001b[0m 10.97 MiB/16.40 MiB         \u001b[2A\n",
      "\u001b[2mgrpcio              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.75 MiB/10.94 MiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)[2m---------\u001b[0m\u001b[0m 11.12 MiB/16.40 MiB         \u001b[2A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)[2m---------\u001b[0m\u001b[0m 11.37 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (13/15)\u001b[2m--------\u001b[0m\u001b[0m 11.48 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)\u001b[2m--------\u001b[0m\u001b[0m 11.76 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)-\u001b[2m-------\u001b[0m\u001b[0m 12.19 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)-\u001b[2m-------\u001b[0m\u001b[0m 12.55 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)--\u001b[2m------\u001b[0m\u001b[0m 12.66 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)--\u001b[2m------\u001b[0m\u001b[0m 12.98 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)---\u001b[2m-----\u001b[0m\u001b[0m 13.31 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)----\u001b[2m----\u001b[0m\u001b[0m 13.73 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)----\u001b[2m----\u001b[0m\u001b[0m 14.08 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)-----\u001b[2m---\u001b[0m\u001b[0m 14.48 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)------\u001b[2m--\u001b[0m\u001b[0m 14.81 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)------\u001b[2m--\u001b[0m\u001b[0m 14.90 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)------\u001b[2m--\u001b[0m\u001b[0m 15.16 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)-------\u001b[2m-\u001b[0m\u001b[0m 15.48 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (14/15)-------\u001b[2m-\u001b[0m\u001b[0m 15.80 MiB/16.40 MiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m15 packages\u001b[0m \u001b[2min 3.98s\u001b[0m\u001b[0m                                                \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m15 packages\u001b[0m \u001b[2min 77ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastembed\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflatbuffers\u001b[0m\u001b[2m==25.9.23\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.75.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh2\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhpack\u001b[0m\u001b[2m==4.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhyperframe\u001b[0m\u001b[2m==6.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mloguru\u001b[0m\u001b[2m==0.7.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmmh3\u001b[0m\u001b[2m==5.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.23.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mportalocker\u001b[0m\u001b[2m==3.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.32.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpy-rust-stemmers\u001b[0m\u001b[2m==0.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mqdrant-client\u001b[0m\u001b[2m==1.15.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add 'qdrant-client[fastembed]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a58b3",
   "metadata": {},
   "source": [
    "Use it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdba9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "qd_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3985a467",
   "metadata": {},
   "source": [
    "Create the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd77ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensionality = 512\n",
    "model_name = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "collection_name = \"evidently-docs\"\n",
    "\n",
    "\n",
    "# if you want to re-insert the data, delete the collection first\n",
    "# qd_client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "\n",
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=dimensionality,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1d14a",
   "metadata": {},
   "source": [
    "Ingest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b4ba6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8705e5d4da9d4c93abee9747f721e94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1214bb89bc405596785b9c9812cfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d7f9fedaa742088b28e4f218bd7ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8f387e089341aeb19336616d53e3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eccec1019b45f8b4a6294b18212dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e44e2f56454f9aa85b83ce8ab4881a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = []\n",
    "\n",
    "for i, doc in enumerate(chunks):\n",
    "    text = doc.get('title', '') + ' ' + doc.get('description', '') + ' ' + doc['content']\n",
    "    text = text.strip()\n",
    "    vector = models.Document(text=text, model=model_name)\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "\n",
    "qd_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353451f",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c0be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, num_results=15):\n",
    "    vector = models.Document(text=query, model=model_name)\n",
    "\n",
    "    query_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=vector,\n",
    "        limit=num_results,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for point in query_points.points:\n",
    "        results.append(point.payload)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e3db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 20000, 'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 0, 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 2000, 'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 1000, 'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 14000, 'content': 'ow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 15000, 'content': ' REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 19000, 'content': 'w to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically tr', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 3000, 'content': 's/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quic', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 13000, 'content': 'eate an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 9000, 'content': 'deterministic or embeddings-based metrics like Semantic Similarity. However, you often need more custom criteria. Using **LLM-as-a-judge** is useful for this, letting you define what to detect.\\n\\nLet’s formulate what we want to Tests:\\n\\n* **Length check**. All new responses must be no longer than 200 symbols.\\n\\n* **Correctness**. All new responses should not contradict the reference answer.\\n\\n* **Style**. All new responses should match the style of the reference.\\n\\nText length is easy to check, but for Correctness and Style, we\\'ll write our custom LLM judges.\\n\\n### Correctness judge\\n\\nWe implement the correctness evaluator, using an Evidenty template for binary classification. We ask the LLM to classify each response as \"correct\" or \"incorrect\" based on the `target_response` column and provide reasoning for its decision.\\n\\n<Note>\\nYou can also use a built-in `CorrectnessLLMEval()` to use a default prompt.\\n</Note>\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\nWe recommend splitting each evaluation criterion into separate judges and using a simple grading scale, like binary classifiers, for better reliability.\\n\\n<Info>\\n  **Ideally, evaluate your judge first!** Each LLM evaluator is a small ML system you should align with your preferences. We recommend running a couple of iterations. Check the [tutorial on LLM judges](/examples/LLM_judge).\\n</Info>\\n\\n<Info>\\n  **Template parameters.** For ', 'title': 'LLM regression testing', 'description': 'How to run regression testing for LLM outputs.', 'filename': 'examples/LLM_regression_testing.mdx'}, {'start': 16000, 'content': 'view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 8000, 'content': 'n LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'quickstart_llm.mdx'}, {'start': 18000, 'content': 'me(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 3000, 'content': 'olumn eval.** For example, to evaluate whether `response`contains any toxicity:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    ToxicityLLMEval(\"response\", alias=\"toxicity\"),\\n])\\n```\\n\\nView the results as usual:\\n\\n```python\\neval_df.as_dataframe()\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_toxicity-min.png)\\n\\n**Run a multi-column eval.** Some evaluators naturally require two columns. For example, to evaluate Context Quality (\"does it have enough information to answer the question?\"), you must run this evaluation over your `context` column, and pass the `question` column as a parameter.\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    ContextQualityLLMEval(\"context\", alias=\"good_context\", question=\"question\"),\\n])\\n```\\n\\nExample output:\\n\\n![](/images/examples/llm_judge_example_context_quality-min.png)\\n\\n**Parametrize evaluators**. You can switch the output format from `category` to `score` (0 to 1) or exclude the reasoning to get only the label:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"refusal\", include_reasoning=False),\\n    ToxicityLLMEval(\"response\", alias=\"toxicity\", include_category=False),\\n    PIILLMEval(\"response\", alias=\"PII\", include_score=True), \\n])\\n```\\n\\n<Info>\\n  **Column names**. The alias you set defines the column name with the category. If you enable the score result as well, it will get the \"Alias score\" name.\\n</Info>\\n\\n## Change the evaluator LLM\\n\\nOpenAI is the default evalution provider in Evidently, but you can choose any other, including models from Anthropic, Gemini, Mistral, Ollama, etc.\\n\\n### Using parameters\\n\\nYou can pass model and provider parameters to the built-in LLM-based descriptor or to your custom `LLMEval`.\\n\\n**Change the model**. Specify a different model from OpenAI:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"response\", alias=\"Decline by Turbo\", provider=\"openai\", model=\"gpt-3.5-turbo\"),\\n])\\n```\\n\\n**Change the provider**. To use a different LLM, first ', 'title': 'Configure LLM Judges', 'description': 'How to run prompt-based evaluators for custom criteria.', 'filename': 'metrics/customize_llm_judge.mdx'}, {'start': 17000, 'content': '\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s ho', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}]\n"
     ]
    }
   ],
   "source": [
    "search_result = vector_search('how do I use llm-as-a-judge for evals')\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efe9c9",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25acc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    results = vector_search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1dd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "306d266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7354afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d75e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an evaluation (eval) report using an LLM as a judge, you'll want to follow these steps:\n",
      "\n",
      "### 1. Environment Setup\n",
      "1. **Install Required Libraries**:\n",
      "   Make sure you have Python installed, and then run:\n",
      "   ```bash\n",
      "   pip install evidently\n",
      "   ```\n",
      "\n",
      "2. **Import Necessary Modules**:\n",
      "   ```python\n",
      "   import pandas as pd\n",
      "   import os\n",
      "   from evidently import Dataset, DataDefinition, Report\n",
      "   from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "   ```\n",
      "\n",
      "3. **Set Your OpenAI API Key**:\n",
      "   ```python\n",
      "   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "   ```\n",
      "\n",
      "### 2. Create an Evaluation Dataset\n",
      "1. **Create a Toy Dataset**:\n",
      "   Construct a sample dataset with questions, target responses, new responses, and manual labels.\n",
      "   ```python\n",
      "   data = [[\"question1\", \"target response 1\", \"new response 1\", \"correct\", \"comment\"],\n",
      "           [\"question2\", \"target response 2\", \"new response 2\", \"incorrect\", \"comment\"]]\n",
      "   columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\n",
      "   golden_dataset = pd.DataFrame(data, columns=columns)\n",
      "   ```\n",
      "\n",
      "2. **Create an Evidently Dataset Object**:\n",
      "   ```python\n",
      "   definition = DataDefinition(\n",
      "       text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
      "       categorical_columns=[\"label\"]\n",
      "   )\n",
      "   eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
      "   ```\n",
      "\n",
      "### 3. Configure the LLM Judge\n",
      "1. **Define the Evaluator Prompt**:\n",
      "   Set up a prompt to evaluate correctness based on the reference response.\n",
      "   ```python\n",
      "   correctness = BinaryClassificationPromptTemplate(\n",
      "       criteria=\"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently. \n",
      "       The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details. \n",
      "       REFERENCE: {target_response}\"\"\",\n",
      "       target_category=\"incorrect\",\n",
      "       non_target_category=\"correct\",\n",
      "       include_reasoning=True\n",
      "   )\n",
      "   ```\n",
      "\n",
      "2. **Add the Evaluator to the Dataset**:\n",
      "   ```python\n",
      "   eval_dataset.add_descriptors(descriptors=[\n",
      "       LLMEval(\"new_response\",\n",
      "               template=correctness,\n",
      "               provider=\"openai\",\n",
      "               model=\"gpt-4o-mini\",\n",
      "               alias=\"Correctness\")\n",
      "   ])\n",
      "   ```\n",
      "\n",
      "### 4. Generate the Report\n",
      "1. **Run the Report**:\n",
      "   Summarize results with a report.\n",
      "   ```python\n",
      "   report = Report([TextEvals()])\n",
      "   my_eval = report.run(eval_dataset, None)\n",
      "   ```\n",
      "\n",
      "2. **View the Results**:\n",
      "   Display the report in the notebook or save it:\n",
      "   ```python\n",
      "   my_eval  # Display in Jupyter Notebook\n",
      "   # my_eval.save_html(\"report.html\")  # Save as HTML if needed\n",
      "   ```\n",
      "\n",
      "### 5. Analyze & Iterate\n",
      "1. **Review Output**:\n",
      "   Inspect the evaluation results to see how well your LLM performed compared to manual labels.\n",
      "2. **Refine Your CrIteria**:\n",
      "   Modify the evaluators or prompts based on your findings to improve accuracy and insights.\n",
      "\n",
      "### Conclusion\n",
      "Following the steps above enables you to build a systematic evaluation framework that leverages LLMs. Whether you want to evaluate outputs for correctness, style, or verbosity, you can adapt your prompt templates to suit different judging criteria.\n"
     ]
    }
   ],
   "source": [
    "result = rag(\"How can I build an eval report with llm as a judge?\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
