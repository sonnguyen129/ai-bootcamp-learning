{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cbb4f0",
   "metadata": {},
   "source": [
    "# Text Search with Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "241f11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs_08 as docs\n",
    "\n",
    "github_data = docs.read_github_data('evidentlyai', 'docs')\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17ef223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 2000,\n",
       " 'content': '\\n\\n- **Export scores** as JSON or Python dictionary.\\n- **As a DataFrame**, either as a raw metrics table or by attaching scores to existing data rows.\\n- **Generate visual reports** in Jupyter, Colab, or export as HTML\\n- **Upload to Evidently Platform** to track evaluations over time\\n\\nThis exportability makes it easy to integrate Evidently into your existing workflows and pipelines –\\xa0even if you are not using the Evidently Platform.\\n\\nHere is an example visual report showing various data quality metrics and test results. Other evaluations can be presented in the same way, or exported as raw scores:\\n\\n![](/images/concepts/report_test_preview.gif)\\n\\n**📌 Links:**\\n\\n- Quickstart for [LLM evaluation](/quickstart_llm) \\n- Quickstart for [ML evaluation](/quickstart_ml)\\n\\nOr read on through this page for conceptual introduction.\\n\\n**2. Synthetic data generation [NEW]**\\n\\n<Check>\\n  **TL;DR**: We have a nice config for structured synthetic data generation using LLMs.\\n</Check>\\n\\nPrimarily designed for LLM use cases, Evidently also helps you generate synthetic test datasets - such as RAG-style question-answer pairs from a knowledge base or synthetic inputs to cold-start your AI app testing.\\n\\n**📌 Links:** \\n- [Synthetic data](docs/library/synthetic_data_api) \\n\\n**3. Prompt optimization [NEW]**\\n\\n<Check>\\n  **TL;DR**: We help write prompts using labeled or annotated data as a target.\\n</Check>\\n\\nEvidently also includes tools for automated prompt writing. This features uses built-in evaluation capabilities to score prompt variations, optimizing them based on a target dataset and/or free-form user feedback.\\n\\nThis feature also help automatically generate LLM judge prompts to streamline the creation of custom evaluations.\\n\\n**📌 Links:** \\n- [Prompt optimization](docs/library/prompt_optimization)\\n\\n4. **Tracking and Visualization UI**\\n\\n<Check>\\n  **TL;DR**: There is also a minimal UI to store and track evaluation results.\\n</Check>\\n\\nThe Evidently library also includes a lightweight self-hostable UI for sto',\n",
       " 'title': 'Introduction',\n",
       " 'description': 'Core concepts and components of the Evidently Python library.',\n",
       " 'filename': 'docs/library/overview.mdx'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afefa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed46c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x107109400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=['content', 'filename', 'title', 'description']\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "669a1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search('how do I use llm-as-a-judge for evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3079ddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 1000, 'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 2000, 'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 20000, 'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 3000, 'content': 's/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quic', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 19000, 'content': 'w to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically tr', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 17000, 'content': '\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s ho', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 16000, 'content': 'view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 14000, 'content': 'ow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 13000, 'content': 'eate an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}]\n"
     ]
    }
   ],
   "source": [
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c1874",
   "metadata": {},
   "source": [
    "## Start Elastisearch\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    -v es9_data:/usr/share/elasticsearch/data \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:9.1.1\n",
    "```\n",
    "\n",
    "Check that it's working using curls:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:9200\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e55e7c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m155 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m136 packages\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add elasticsearch==9.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d1e39",
   "metadata": {},
   "source": [
    "## Let's use it in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "393b7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f567f",
   "metadata": {},
   "source": [
    "Define an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dbe7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"start\": {\"type\": \"integer\"},\n",
    "            \"content\": {\"type\": \"text\"},\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"filename\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50317da",
   "metadata": {},
   "source": [
    "Create an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbe6dc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'evidently-docs'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = 'evidently-docs'\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c795cfa",
   "metadata": {},
   "source": [
    "If we want to delete an index we can run:\n",
    "\n",
    "\n",
    "```python\n",
    "# python\n",
    "es_client.indices.delete(index=index_name)\n",
    "```\n",
    "\n",
    "Or if we want to recreate an index, we would have to delete it and create it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52978c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32dffc",
   "metadata": {},
   "source": [
    "Index the chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0524c1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6605893ef4452a8263492a684948f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(chunks):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb13026",
   "metadata": {},
   "source": [
    "Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c60bba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query, num_results=15):\n",
    "    es_query = {\n",
    "        \"size\": num_results,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"type\": \"best_fields\",\n",
    "                \"fields\": [\"content\", \"filename\", \"title\", \"description\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=es_query)\n",
    "\n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe19fec",
   "metadata": {},
   "source": [
    "To give some terms more importance, we can use boosting. For example, this way we make title 3 times more important than other fields:\n",
    "\n",
    "```python\n",
    "# Python\n",
    "\"fields\": [\"content\", \"filename\", \"title^3\", \"description\"],\n",
    "```\n",
    "\n",
    "This way we make title 3 times more important than other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b72e6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = elastic_search('how do I use llm-as-a-judge for evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c263da9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 3000, 'content': 's/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quic', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 2000, 'content': 'ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  - Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definiti', 'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'quickstart_llm.mdx'}, {'start': 16000, 'content': 'score and/or label as specified.</li><li>Accepts and returns a single column. </li></ul> | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |\\n\\n## LLM-based evals\\n\\nUsing an external LLMs with an evaluation prompt. You can specify the LLM to use as an evaluator.\\n\\n### Custom\\n\\nLLM judge templates.\\n\\n| Name          | Descriptor                                                                                                                                                                                                   | Parameters                                                                                                                                                                                                |\\n| :------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| **LLMEval()** | <ul><li>Scores the text using user-defined criteria.</li><li>You must specify provider, model and use prompt template to formulate the criteria. </li><li>Returns score and/or label as specified.</li></ul> | **Optional:** <ul><li>`alias`</li><li>`template`</li><li>`provider`</li><li>`model`</li><li>`additional_columns: dict`</li><li>See [custom LLM judge parameters](/metrics/customize_llm_judge).</li></ul> |\\n\\n### RAG\\n\\nRAG-specific evals for retrieval and generation. ([Tutorial](/examples/LLM_rag_evals)).\\n\\n| Name                        | Descriptor                                                                                                                                                                                               ', 'title': 'All Descriptors', 'description': 'Reference page for all row-level text and LLM evals.', 'filename': 'metrics/all_descriptors.mdx'}, {'start': 1000, 'content': 'nes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \\n       \"because wings create lift\"],\\n      [\"Why do we have seasons?\", \\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \\n       \"because Earth is tilted\"],\\n      [\"How do magnets work?\", \\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \\n       \"because of magnetic fields\"],\\n      [\"Why does the moon change shape?\", \\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \\n       \"because it rotates\"],\\n      [\"What movie should I watch tonight?\", \\n       \"A movie is a motion picture created to entertain, educate, or inform viewers through a combination of storytelling, visuals, and sound.\", \\n       \"watch a movie that suits your mood\"]\\n  ]\\n\\n  columns = [\"question\", \"context\", \"response\"]\\n\\n  df = pd.DataFrame(data, columns=columns)\\n\\n  eval_df = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Built-in ML evals\\n\\n<Tip>\\n  **Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#ml-based-evals).\\n</Tip>\\n\\nThere are built-in evaluators for some models. You can call them like any other descriptor:\\n\\n```python\\neval_df.add_descriptors(descriptors=[\\n    HuggingFaceToxicity(\"question\", toxic_label=\"hate\", alias=\"Toxicity\") \\n])\\n```\\n\\n## Custom ML evals\\n\\n<Info>\\n  You can also add any custom checks [directly as a Python function](/metrics/customize_descriptor).\\n</Info>\\n\\nAlternatively, use the general `HuggingFace()` descriptor to call a specific named model. The model you use must return a numerical score or a category fo', 'title': 'Use HuggingFace models', 'description': 'How to use models from HuggingFace as evaluators.', 'filename': 'metrics/customize_hf_descriptor.mdx'}, {'start': 0, 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 5000, 'content': 'ed title could get very long.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    WordCount(\"answer\", alias=\"Words\"),\\n])\\n```\\n\\n**Descriptor parameters**. Some Descriptors have required parameters. For example, if you’re testing for competitor mentions using the `Contains` Descriptor, add the list of `items`:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    Contains(\"answer\", items=[\"AcmeCorp\", \"YetAnotherCorp\"], alias=\"Competitors\")\\n])\\n```\\n\\nThese parameters are specific to each descriptors. Check the [reference table](/metrics/all_descriptors).\\n\\n**Multi-column descriptors**. Some evals use more than one column. For example, to match a new answer against reference, or measure semantic similarity. Pass both columns using parameters:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    SemanticSimilarity(columns=[\"question\", \"answer\"], alias=\"Semantic_Match\")\\n])\\n```\\n\\n**LLM-as-a-judge**. There are also built-in descriptors that prompt an external LLM to return an evaluation score. You can add them like any other descriptor, but you must also provide an API key to use the corresponding LLM.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"answer\", alias=\"Contains_Denial\")\\n])\\n```\\n\\n<Tip>\\n  **Using and customizing LLM judge**. Check the [in-depth LLM judge guide](/metrics/customize_llm_judge) on using built-in and custom LLM-based evaluators.\\n</Tip>\\n\\n**Custom evals**. Beyond custom LLM judges, you can also implement your own programmatic evals as Python functions. Check the [custom descriptor guide](/metrics/customize_descriptor).\\n\\n## Adding Descriptor Tests\\n\\nDescriptor Tests let you define pass/fail checks for each row in your dataset. Instead of just calculating values (like “How long is this text?”), you can ask:\\n\\n- Is the text under 100 characters?\\n- Is the sentiment positive?\\n\\nYou can also combine multiple tests into a single summary result per row.\\n\\n**Step 1. Imports**. Run imports:\\n\\n```python\\nfrom evidently.descriptors impo', 'title': 'Descriptors', 'description': 'How to run evaluations for text data.', 'filename': 'docs/library/descriptors.mdx'}, {'start': 7000, 'content': 'ch as articles.\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n\\n## Resources\\n\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\n\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n\\nAdditional links:\\n\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\n\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\n\\n* [\"My data drifted. What\\'s next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\n\\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)', 'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'metrics/explainer_drift.mdx'}, {'start': 15000, 'content': '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\\n| **CustomDescriptor()**        | <ul><li>Implements a custom check for specific column(s) as a Python function.</li><li>Use it to run your own programmatic checks. </li><li>Returns score and/or label as specified.</li><li>Can accept and return multiple columns. </li></ul>                        | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |\\n| **CustomColumnsDescriptor()** | <ul><li>Implements a custom check as a Python function that can be applied to any column in the dataset.</li><li>Use it to run your own programmatic checks. </li><li>Returns score and/or label as specified.</li><li>Accepts and returns a single column. </li></ul> | **Optional:** <ul><li>`alias`</li><li>`func: callable` </li></ul>See [how to add a custom descriptor](/metrics/customize_descriptor). |\\n\\n## LLM-based evals\\n\\nUsing an external LLMs with an evaluation prompt. You can specify the LLM to use as an evaluator.\\n\\n### Custom\\n\\nLLM judge templates.\\n\\n| Name          | Descriptor                                                                                                                                                                                                   | Parameters                                                                                                                                                                                                |\\n| :------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------', 'title': 'All Descriptors', 'description': 'Reference page for all row-level text and LLM evals.', 'filename': 'metrics/all_descriptors.mdx'}, {'start': 1000, 'content': ' air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \\n       \"because wings create lift\"],\\n      [\"Why do we have seasons?\", \\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \\n       \"because Earth is tilted\"],\\n      [\"How do magnets work?\", \\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \\n       \"because of magnetic fields\"],\\n      [\"Why does the moon change shape?\", \\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \\n       \"because it rotates\"],\\n      [\"What movie should I watch tonight?\", \\n       \"A movie is a motion picture created to entertain, educate, or inform viewers through a combination of storytelling, visuals, and sound.\", \\n       \"watch a movie that suits your mood\"]\\n  ]\\n  \\n  columns = [\"question\", \"context\", \"response\"]\\n  \\n  df = pd.DataFrame(data, columns=columns)\\n  \\n  eval_df = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition())\\n  ```\\n</Accordion>\\n\\n## Built-in LLM judges\\n\\n<Tip>\\n  **Available descriptors**. Check all available built-in LLM evals in the [reference table](/metrics/all_descriptors#llm-based-evals).\\n</Tip>\\n\\nThere are built-in evaluators for popular criteria, like detecting toxicity or if the text contains a refusal. These built-in descriptors:\\n\\n- Default to binary classifiers.\\n- Default to using `gpt-4o-mini` model from OpenAI.\\n- Return a label, the reasoning for the decision, and an optional score.\\n\\n**OpenAI key.** Add the token as the environment variable: [see docs](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety).\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] \\n```\\n\\n**Run a single-c', 'title': 'Configure LLM Judges', 'description': 'How to run prompt-based evaluators for custom criteria.', 'filename': 'metrics/customize_llm_judge.mdx'}, {'start': 0, 'content': 'LLM-based descriptors use an external LLM for evaluation. You can:\\n\\n- Use built-in evaluators (with pre-written prompts), or\\n- Configure custom criteria using templates.\\n\\n**Pre-requisites**:\\n\\n- You know how to use [descriptors](/docs/library/descriptors) to evaluate text data.\\n\\n## Imports\\n\\n```python\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate, MulticlassClassificationPromptTemplate \\nfrom evidently.descriptors import LLMEval, ToxicityLLMEval, ContextQualityLLMEval, DeclineLLMEval\\n```\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  To generate toy data and create a Dataset object:\\n\\n  ```python\\n  import pandas as pd\\n  from evidently import DataDefinition\\n  \\n  data = [\\n      [\"Why is the sky blue?\", \\n       \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\", \\n       \"because air scatters blue light more\"],\\n      [\"How do airplanes stay in the air?\", \\n       \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\", \\n       \"because wings create lift\"],\\n      [\"Why do we have seasons?\", \\n       \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\", \\n       \"because Earth is tilted\"],\\n      [\"How do magnets work?\", \\n       \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\", \\n       \"because of magnetic fields\"],\\n      [\"Why does the moon change shape?\", \\n       \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\", \\n       \"because it rotates\"],\\n      [\"What movie should I watch tonight?\", \\n       \"A movie is a motion picture created to entertain, educate, or inform viewers throu', 'title': 'Configure LLM Judges', 'description': 'How to run prompt-based evaluators for custom criteria.', 'filename': 'metrics/customize_llm_judge.mdx'}, {'start': 0, 'content': '<Check>\\n  Trace store and viewer are Pro features available in **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\nTracing uses the open-source `Tracely` library, based on OpenTelemetry.\\n\\n## What is LLM tracing?\\n\\nTracing lets you instrument your AI application to collect data for evaluation and analysis.\\n\\nIt captures detailed records of how your LLM app operates, including inputs, outputs and any intermediate steps and events (e.g., function calls). You define what to include.\\n\\nEvidently provides multiple ways to explore tracing data.\\n\\n<Tabs>\\n  <Tab title=\"Trace view\">\\n    See a timeline of execution steps with input-output details and latency.\\n\\n    ![](/images/tracing.png)\\n  </Tab>\\n\\n  <Tab title=\"Dataset view\">\\n    Automatically generate a tabular view for easier evaluation or labeling.\\n    ![](/images/examples/tracing_tutorial_dataset_view.png)\\n  </Tab>\\n\\n  <Tab title=\"Dialogue view\">\\n    For conversational applications, browse traces by user or session to focus on chat flows.\\n\\n    ![](/images/examples/tracing_tutorial_session_view.png)\\n  </Tab>\\n</Tabs>\\n\\nOnce you capture the data, you can also run evals on the tracing datasets.\\n\\n## Do I always need tracing?\\n\\nTracing is optional on the Evidently Platform. You can also:\\n\\n* Upload tabular datasets using Dataset API.\\n\\n* Run evals locally and send results to the platform without tracing.\\n\\nHowever, tracing is especially useful for understanding complex LLM chains and execution flows, both in experiments and production monitoring.', 'title': 'Overview', 'description': 'Introduction to Tracing.', 'filename': 'docs/platform/tracing_overview.mdx'}, {'start': 2000, 'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 6000, 'content': 'ion score. You can add them like any other descriptor, but you must also provide an API key to use the corresponding LLM.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    DeclineLLMEval(\"answer\", alias=\"Contains_Denial\")\\n])\\n```\\n\\n<Tip>\\n  **Using and customizing LLM judge**. Check the [in-depth LLM judge guide](/metrics/customize_llm_judge) on using built-in and custom LLM-based evaluators.\\n</Tip>\\n\\n**Custom evals**. Beyond custom LLM judges, you can also implement your own programmatic evals as Python functions. Check the [custom descriptor guide](/metrics/customize_descriptor).\\n\\n## Adding Descriptor Tests\\n\\nDescriptor Tests let you define pass/fail checks for each row in your dataset. Instead of just calculating values (like “How long is this text?”), you can ask:\\n\\n- Is the text under 100 characters?\\n- Is the sentiment positive?\\n\\nYou can also combine multiple tests into a single summary result per row.\\n\\n**Step 1. Imports**. Run imports:\\n\\n```python\\nfrom evidently.descriptors import ColumnTest, TestSummary\\nfrom evidently.tests import *\\n```\\n\\n**Step 2. Add tests to a descriptor**. When creating a descriptor (like `TextLength` or `Sentiment`), use the tests argument to set conditions. Each test adds a new column with a True/False result.\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    df,\\n    data_definition=DataDefinition(text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\", tests=[\\n            gte(0, alias=\"Sentiment is non-negative\")]),\\n        TextLength(\"answer\", alias=\"Length\", tests=[\\n            lte(100, alias=\"Length is under 100\")]),\\n    ]\\n)\\n```\\n\\nUse test parameters like `gte` (greater than or equal), `lte` (less than or equal), eq (equal). Check the [full list here](docs/library/tests#test-parameters).\\n\\nYou can preview the results with: `eval_dataset.as_dataframe()`:\\n![](/images/metrics/descriptors_tests-min.png)\\n\\n**Step 3. Add a Test Summary**. Use `TestSummary` to combine multiple tests into one or m', 'title': 'Descriptors', 'description': 'How to run evaluations for text data.', 'filename': 'docs/library/descriptors.mdx'}, {'start': 0, 'content': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\", 'title': 'LLM regression testing', 'description': 'How to run regression testing for LLM outputs.', 'filename': 'examples/LLM_regression_testing.mdx'}, {'start': 1000, 'content': 'oken in the “Secrets” menu section if you plan to use an LLM for evaluations. You can proceed without it, using other types of evals.\\n\\n## 3. Configure the evaluation\\n\\nYou must choose which column to evaluate and how. You can choose from the following methods:\\n\\n- **Model-based**: use built-in machine learning models, like sentiment analysis.\\n- **Regular expressions**: check for specific words or patterns.\\n- **Text stats**: measure stats like the number of symbols or sentences.\\n- **LLM-based**: use external LLMs to evaluate your text data.\\n\\nSelect specific checks one by one:\\n\\n![](/images/nocode_choose_evals-min.png)\\n\\nEach evaluation result is called a **Descriptor**. No matter the method, you’ll get a label or score for every evaluated text. Some, like “Sentiment,” work instantly, while others may need setup.\\n\\n<Note>\\n  **What other evaluators are there?** Check the list of [All Descriptors](../metrics/all_descriptors).\\n</Note>\\n\\nHere are few examples of Descriptors and how to configure them:\\n\\n### Words presence\\n\\n**Include Words**. This Descriptor checks for listed words and returns \"True\" or \"False.\"\\n\\nSet up these parameters:\\n\\n- Add a list of words.\\n- Choose whether to check for “any” or “all” of the words present.\\n- Set the **lemmatize** parameter to check for inflected and variant words automatically.\\n- Give your check a name so you can easily find it in your results.\\n\\nExample setup:\\n![](/images/nocode_includes_words-min.png)\\n\\n### Semantic Similarity\\n\\n**Semantic Similarity**. This descriptor converts texts to embeddings and calculates Cosine Similarity between your evaluated column and another column. It scores from 0 to 1 (0: completely different, 0.5: unrelated, 1: identical). It\\'s useful for checking if responses are semantically similar to a question or reference.\\n\\nSelect the column to compare against: ![](/images/nocode_semantic_similarity-min.png)\\n\\n### LLM as a judge\\n\\n**Custom LLM evaluator**. If you\\'ve added your token, use LLM-based evals (built-in or custom)', 'title': 'No code evals', 'description': 'How to evaluate your data in a no-code interface.', 'filename': 'docs/platform/evals_no_code.mdx'}]\n"
     ]
    }
   ],
   "source": [
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84137d",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc57418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    results = elastic_search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53e91101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16b4c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34f9a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c34d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag(\"How can I build an eval report with llm as a judge?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e687fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build an evaluation report using an LLM as a judge, follow these steps:\n",
      "\n",
      "### 1. Installation and Imports\n",
      "Make sure you have the required libraries installed. You will need the Evidently library for evaluation.\n",
      "\n",
      "```bash\n",
      "pip install evidently\n",
      "```\n",
      "\n",
      "Import necessary modules:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "from evidently import Dataset, DataDefinition, Report, BinaryClassification\n",
      "from evidently.descriptors import *\n",
      "from evidently.presets import TextEvals, ClassificationPreset\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "```\n",
      "\n",
      "### 2. Set Up OpenAI API Key\n",
      "Set up your OpenAI key as an environment variable.\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 3. Create the Dataset\n",
      "Construct a dataset with customer support questions, including target responses, new responses, and manual labels.\n",
      "\n",
      "```python\n",
      "data = [\n",
      "    [\"Hi there, how do I reset my password?\", \n",
      "     \"To reset your password, click on 'Forgot Password' on the login page...\", \n",
      "     \"To change your password, select 'Forgot Password'...\", \n",
      "     \"incorrect\", \"adds new information (contact support)\"],\n",
      "    # Add more rows as needed...\n",
      "]\n",
      "\n",
      "columns = [\"question\", \"target_response\", \"new_response\", \"manual_label\", \"explanation\"]\n",
      "eval_df = pd.DataFrame(data, columns=columns)\n",
      "```\n",
      "\n",
      "### 4. Run Evaluations\n",
      "Use the LLM to evaluate the responses against targets.\n",
      "\n",
      "```python\n",
      "eval_dataset = Dataset.from_pandas(\n",
      "    eval_df,\n",
      "    data_definition=DataDefinition(),\n",
      "    descriptors=[\n",
      "        LLMEval(\"new_response\", template=BinaryClassificationPromptTemplate(\n",
      "            criteria=\"Is the new response accurate based on the target response?\",\n",
      "            target_category=\"CORRECT\",\n",
      "            non_target_category=\"INCORRECT\"\n",
      "        ), provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Response Accuracy\")\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "### 5. Generate a Report\n",
      "Create and run a report to summarize evaluation results.\n",
      "\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "```\n",
      "\n",
      "### 6. View the Results\n",
      "To see the evaluation results, you can print or visualize the generated report:\n",
      "\n",
      "```python\n",
      "print(my_eval)  # or visualize it in a notebook interface\n",
      "```\n",
      "\n",
      "### 7. Optional Steps\n",
      "- **Refine Your Model**: If you're not satisfied with the accuracy, consider refining your prompt or even switching the evaluator model.\n",
      "- **Upload Results**: If you're using Evidently Cloud, you can upload results for further analysis.\n",
      "\n",
      "```python\n",
      "from evidently.ui.workspace import CloudWorkspace\n",
      "ws = CloudWorkspace()\n",
      "\n",
      "# Create and send your evaluation\n",
      "ws.add_run(\"your_project_id\", my_eval, include_data=True)\n",
      "```\n",
      "\n",
      "### 8. Additional Considerations\n",
      "- **Custom Evaluators**: Define specific criteria for your LLM depending on the context—like conciseness.\n",
      "- **Iterative Improvements**: Continuously improve your prompts and evaluation criteria as needed.\n",
      "\n",
      "By following these steps, you can create a comprehensive evaluation report that leverages an LLM as an effective judge for your responses.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
