{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2daf87c",
   "metadata": {},
   "source": [
    "# Intelligent Chunking\n",
    "\n",
    "In the previous lesson, we got data from a GitHub repository and used simple character-based chunking. For most applications, it's actually enough.\n",
    "\n",
    "But let's take a look at some alternative approaches:\n",
    "- Token-based chunking: You first tokenize the content (turn it into a sequence of words) and then do a sliding window over tokens\n",
    "    - Advantages: More precise control over LLM input size\n",
    "    - Disadvantages: Doesn't work well for documents with code\n",
    "- Paragraph splitting: Split by paragraphs\n",
    "- Section splitting: Split by sections\n",
    "- AI-powered splitting: Let AI split the text intelligently\n",
    "\n",
    "We won't cover token-based chunking here, as we're working with documents that contain code. But it's easy to implement - ask ChatGPT for help if you need it for text-only content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e75939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987d7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def read_github_data(repo_owner: str, repo_name: str) -> List[RawRepositoryFile]:\n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions,\n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640a72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = read_github_data('evidentlyai', 'docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba64afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "from typing import Dict, Any\n",
    "\n",
    "def parse_data(data_raw: List[RawRepositoryFile]) -> List[Dict[str, Any]]:\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ba4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9daafb",
   "metadata": {},
   "source": [
    "## Splitting by Paragraphs\n",
    "\n",
    "One paragraph is separated from another by two or more new lines (\\n characters), so we can split our text using regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3feb36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = parsed_data[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ef828c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595258e5",
   "metadata": {},
   "source": [
    "## Section Splitting\n",
    "\n",
    "Markdown documents have this structure:\n",
    "\n",
    "```md\n",
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3\n",
    "```\n",
    "\n",
    "What we can do is split by headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dcc2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3050b197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## 4. Get new answers\\n\\nSuppose you generate new responses using your LLM after changing a prompt. We will imitate it by adding a new column with new responses to the DataFrame:\\n\\n<Accordion title=\"New toy data generation\" defaultOpen={false}>\\n  Run this code to generate a new dataset.\\n\\n  ```python\\n  data = [\\n    [\"Why is the sky blue?\",\\n     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\\n     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\\n\\n    [\"How do airplanes stay in the air?\",\\n     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\\n     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\\n\\n    [\"Why do we have seasons?\",\\n     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\\n     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\\n\\n    [\"How do magnets work?\",\\n     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\\n     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\\n\\n    [\"Why does the moon change shape?\",\\n     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\\n     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon\\'s shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\\n  ]\\n\\n  columns = [\"question\", \"target_response\", \"response\"]\\n\\n  eval_data = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\nHere is the resulting dataset with the added new column:\\n\\n![](/images/examples/llm_regression_tutorial_new_data-min.png)\\n\\n<Info>\\n  **How to connect it with your app?** Replace this step with calling your LLM app to score the inputs and add the new responses to the DataFrame. You can also use our **`tracely`** library to instrument your app and get traces as a tabular dataset. Check the tutorial with [tracing workflow](/quickstart_tracing).\\n</Info>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002973d",
   "metadata": {},
   "source": [
    "## Intelligent chunking with \n",
    "\n",
    "This makes sense when:\n",
    "- Complex structure: Documents have complex, non-standard structure\n",
    "- Semantic coherence: You want chunks that are semantically meaningful\n",
    "- Custom logic: You need domain-specific splitting rules\n",
    "- Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "Note: This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "Simple approaches are sufficient. Use intelligent chunking only when\n",
    "- You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "- You have complex, unstructured documents\n",
    "- Quality is more important than cost\n",
    "- You have the budget for LLM processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c11b06",
   "metadata": {},
   "source": [
    "Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cbd9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_instructions = \"\"\"\n",
    "Split the provided document into logical sections that make sense for a Q&A system.\n",
    "Each section should be self-contained and cover a specific topic or concept.\n",
    "Sections should be relatively large (3000-5000 characters).\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142c59f",
   "metadata": {},
   "source": [
    "Structured output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "196b8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Section(BaseModel):\n",
    "    title: str\n",
    "    markdown: str\n",
    "\n",
    "class Document(BaseModel):\n",
    "    title: str\n",
    "    sections: list[Section]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "346fb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_type, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        { \"role\": \"system\", \"content\": instructions },\n",
    "        { \"role\": \"user\", \"content\": user_prompt }\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_type\n",
    "    )\n",
    "\n",
    "    return response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "208a78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_structured(\n",
    "    instructions=chunking_instructions,\n",
    "    user_prompt=text,\n",
    "    output_type=Document\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "871ff740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Testing for LLM Outputs\n",
      "\n",
      "------\n",
      "Introduction to Regression Testing for LLM Outputs\n",
      "In this tutorial, you will learn how to perform regression testing for outputs generated by Large Language Models (LLMs).\n",
      "\n",
      "Regression testing is essential for ensuring that changes made to your system—be it a new model, a modified prompt, or any other variation—do not lead to unintended changes in behavior. By comparing new outputs against prior ones, you can identify significant changes, ascertain updates confidently, and correct potential issues.\n",
      "\n",
      "_info_  \n",
      "**This example utilizes Evidently Cloud.** We will execute evaluations in Python and have the option to upload results, or alternatively, view reports locally. For self-hosted environments, replace `CloudWorkspace` with `Workspace`._\n",
      "\n",
      "# Tutorial Overview\n",
      "This tutorial outlines the following key steps:\n",
      "\n",
      "1. **Create a toy dataset**: Build a small dataset that includes questions, expected answers, and reference responses.\n",
      "2. **Generate new answers**: Simulate obtaining new answers using altered prompts or model parameters.\n",
      "3. **Create and run a Report with Tests**: Use LLM-as-a-judge to compare responses based on length, correctness, and style consistency.\n",
      "4. **Build a monitoring Dashboard**: Develop visual representations to monitor Test results over time.\n",
      "\n",
      "_note_  \n",
      "To simplify, this tutorial will simulate new outputs without constructing an actual LLM application._\n",
      "\n",
      "# Requirements  \n",
      "To complete this tutorial, you will need the following:\n",
      "- Basic knowledge of Python.\n",
      "- An OpenAI API key to use for the LLM evaluator.\n",
      "- An Evidently Cloud account to track test results. If you do not have one, [sign up](https://www.evidentlyai.com/register) for a free account.\n",
      "\n",
      "_info_  \n",
      "The full code for this tutorial is available in a [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or can be opened directly in Colab via this [link](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb)._\n",
      "------\n",
      "Installation and Imports\n",
      "## Installation and Imports\n",
      "To use Evidently, start by installing the required library:\n",
      "\n",
      "```bash\n",
      "pip install evidently[llm]  \n",
      "```\n",
      "\n",
      "Next, import the necessary modules into your Python environment:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from evidently.future.datasets import Dataset\n",
      "from evidently.future.datasets import DataDefinition\n",
      "from evidently.future.datasets import Descriptor\n",
      "from evidently.future.descriptors import *\n",
      "from evidently.future.report import Report\n",
      "from evidently.future.presets import TextEvals\n",
      "from evidently.future.metrics import *\n",
      "from evidently.future.tests import *\n",
      "\n",
      "from evidently.features.llm_judge import BinaryClassificationPromptTemplate\n",
      "```\n",
      "\n",
      "To connect to Evidently Cloud, include the following:\n",
      "\n",
      "```python\n",
      "from evidently.ui.workspace.cloud import CloudWorkspace\n",
      "```\n",
      "\n",
      "### Optional Panels for Monitoring\n",
      "If you wish to programmatically create monitoring panels, import these additional modules:\n",
      "\n",
      "```python\n",
      "from evidently.ui.dashboards import DashboardPanelPlot\n",
      "from evidently.ui.dashboards import DashboardPanelTestSuite\n",
      "from evidently.ui.dashboards import DashboardPanelTestSuiteCounter\n",
      "from evidently.ui.dashboards import TestSuitePanelType\n",
      "from evidently.ui.dashboards import ReportFilter\n",
      "from evidently.ui.dashboards import PanelValue\n",
      "from evidently.ui.dashboards import PlotType\n",
      "from evidently.ui.dashboards import CounterAgg\n",
      "from evidently.tests.base_test import TestStatus\n",
      "from evidently.renderers.html_widgets import WidgetSize\n",
      "```\n",
      "\n",
      "### Set Your OpenAI Key\n",
      "Finally, pass your OpenAI API key to authenticate:\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n",
      "```\n",
      "------\n",
      "Creating an Evidently Project\n",
      "## Creating a Project\n",
      "To begin utilizing Evidently Cloud, connect using your API token:\n",
      "\n",
      "```python\n",
      "ws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")\n",
      "```\n",
      "\n",
      "Next, create an Evidently project:\n",
      "\n",
      "```python\n",
      "project = ws.create_project(\"Regression testing example\", org_id=\"YOUR_ORG_ID\")\n",
      "project.description = \"My project description\"\n",
      "project.save()\n",
      "```\n",
      "------\n",
      "Preparing the Dataset\n",
      "## Preparing the Dataset\n",
      "In this section, you will create a toy dataset comprising questions and their reference answers:\n",
      "\n",
      "```python\n",
      "data = [\n",
      "    [\"Why is the sky blue?\", \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\"],\n",
      "    [\"How do airplanes stay in the air?\", \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\"],\n",
      "    [\"Why do we have seasons?\", \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\"],\n",
      "    [\"How do magnets work?\", \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\"],\n",
      "    [\"Why does the moon change shape?\", \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\"]\n",
      "]\n",
      "\n",
      "columns = [\"question\", \"target_response\"]\n",
      "\n",
      "ref_data = pd.DataFrame(data, columns=columns)\n",
      "```\n",
      "\n",
      "To preview your dataset, you can use:\n",
      "\n",
      "```python\n",
      "pd.set_option('display.max_colwidth', None)\n",
      "ref_data.head()\n",
      "```\n",
      "\n",
      "This will provide a quick glance at your data structure.\n",
      "------\n",
      "Generating New Answers\n",
      "## Generating New Answers\n",
      "To simulate generating new responses after modifying a prompt, we add a new column with updated responses:\n",
      "\n",
      "<Accordion title=\"New toy data generation\" defaultOpen={false}>\n",
      "  Run this code to generate a new dataset.\n",
      "\n",
      "  ```python\n",
      "  data = [\n",
      "    [\"Why is the sky blue?\",\n",
      "     \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\n",
      "     \"The sky appears blue because air molecules scatter the sun’s blue light more than they scatter other colors.\"],\n",
      "\n",
      "    [\"How do airplanes stay in the air?\",\n",
      "     \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\n",
      "     \"Airplanes stay airborne because the shape of their wings causes air to move faster over the top than the bottom, generating lift.\"],\n",
      "\n",
      "    [\"Why do we have seasons?\",\n",
      "     \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\n",
      "     \"Seasons occur because of the tilt of the Earth’s axis, leading to varying amounts of sunlight reaching different areas as the Earth orbits the sun.\"],\n",
      "\n",
      "    [\"How do magnets work?\",\n",
      "     \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\n",
      "     \"Magnets generate a magnetic field, which can attract metals like iron by causing the electrons in those metals to align in a particular way, creating an attractive or repulsive force.\"],\n",
      "\n",
      "    [\"Why does the moon change shape?\",\n",
      "     \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\n",
      "     \"The moon appears to change shape as it orbits Earth, which is because we see different parts of its lit-up half at different times. The sun lights up half of the moon, but as the moon moves around the Earth, we see varying portions of that lit-up side. So, the moon's shape in the sky seems to change gradually, from a thin crescent to a full circle and back to a crescent again.\"]\n",
      "  ]\n",
      "\n",
      "  columns = [\"question\", \"target_response\", \"response\"]\n",
      "\n",
      "  eval_data = pd.DataFrame(data, columns=columns)\n",
      "  ```\n",
      "</Accordion>  \n",
      "\n",
      "You now have a resulting dataset that includes both target responses and newly generated responses.\n",
      "------\n",
      "Designing the Test Suite\n",
      "## Designing the Test Suite\n",
      "To compare new answers against old ones, we need to specify evaluation metrics. You can utilize deterministic metrics or embeddings-based metrics such as Semantic Similarity. For more tailored criteria, using **LLM-as-a-judge** is advantageous, as it allows custom definitions of what to assess.\n",
      "\n",
      "### Evaluation Criteria\n",
      "Let’s outline the key evaluation metrics to be included:\n",
      "\n",
      "- **Length Check**: Ensure all new responses do not surpass 200 symbols.\n",
      "- **Correctness**: Confirm that none of the new responses contradict the reference answer.\n",
      "- **Styling Consistency**: Ensure that the style of the new responses matches that of the reference prompts.\n",
      "\n",
      "While length checks are straightforward, correctness and style require custom LLM judges.\n",
      "\n",
      "### Correctness Judge\n",
      "To implement the correctness evaluator, use the binary classification Evidently template:\n",
      "\n",
      "```python\n",
      "correctness = BinaryClassificationPromptTemplate(\n",
      "        criteria = \"\"\"\n",
      "        An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\n",
      "        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\n",
      "        REFERENCE:\n",
      "        =====\n",
      "        {target_response}\n",
      "        =====\"\"\",\n",
      "        target_category=\"incorrect\",\n",
      "        non_target_category=\"correct\",\n",
      "        uncertainty=\"unknown\",\n",
      "        include_reasoning=True,\n",
      "        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\n",
      ")\n",
      "```\n",
      "\n",
      "### Style Judge\n",
      "Similarly, create a custom judge for ensuring style matches:\n",
      "\n",
      "```python\n",
      "style_match = BinaryClassificationPromptTemplate(\n",
      "        criteria = \"\"\"\n",
      "        An ANSWER is style-matching when it matches the REFERENCE answer in STYLE, even if the meaning is different.\n",
      "        The ANSWER is style-mismatched when it diverges from the REFERENCE answer in STYLE, even if the meaning is the same.\n",
      "\n",
      "        Consider the following STYLE attributes:\n",
      "        - tone (friendly, formal, casual, sarcastic, etc.)\n",
      "        - sentence structure (simple, compound, complex, etc.)\n",
      "        - verbosity level (relative length of answers)\n",
      "        - and other similar attributes that may reflect difference in STYLE.\n",
      "\n",
      "        You must focus only on STYLE. Ignore any differences in contents.\n",
      "\n",
      "        =====\n",
      "        {target_response}\n",
      "        =====\"\"\",\n",
      "        target_category=\"style-mismatched\",\n",
      "        non_target_category=\"style-matching\",\n",
      "        uncertainty=\"unknown\",\n",
      "        include_reasoning=True,\n",
      "        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\n",
      ")\n",
      "```\n",
      "------\n",
      "Running the Evaluations\n",
      "## Running the Evaluations\n",
      "We can now run tests evaluating correctness, style, and text length in two steps:\n",
      "\n",
      "### Step 1: Score the Data\n",
      "First, we define the row-level descriptors we wish to add to the dataset:\n",
      "\n",
      "```python\n",
      "descriptors=[\n",
      "    LLMEval(\"response\",\n",
      "            template=correctness,\n",
      "            provider = \"openai\",\n",
      "            model = \"gpt-4o-mini\",\n",
      "            alias=\"Correctness\",\n",
      "            additional_columns={\"target_response\": \"target_response\"}),\n",
      "    LLMEval(\"response\",\n",
      "            template=style_match,\n",
      "            provider = \"openai\",\n",
      "            model = \"gpt-4o-mini\",\n",
      "            alias=\"Style\",\n",
      "            additional_columns={\"target_response\": \"target_response\"}),\n",
      "    TextLength(\"response\", alias=\"Length\")\n",
      "]\n",
      "```\n",
      "\n",
      "To add these descriptors to the dataset, execute:\n",
      "\n",
      "```python\n",
      "eval_dataset.add_descriptors(descriptors=descriptors)\n",
      "```\n",
      "\n",
      "To preview the results, run:\n",
      "\n",
      "```python\n",
      "eval_dataset.as_dataframe()\n",
      "```\n",
      "\n",
      "### Step 2: Create a Report\n",
      "Next, formulate a report based on the scored data:\n",
      "\n",
      "```python\n",
      "report = Report([\n",
      "    TextEvals(),\n",
      "    MaxValue(column=\"Length\", tests=[lte(200)]),\n",
      "    CategoryCount(column=\"Correctness\", category=\"incorrect\", tests=[eq(0)]),\n",
      "    CategoryCount(column=\"Style\", category=\"style-mismatched\", tests=[eq(0, is_critical=False)]),\n",
      "])\n",
      "```\n",
      "\n",
      "To run the report on the prepared dataset:\n",
      "\n",
      "```python\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "ws.add_run(project.id, my_eval, include_data=True)\n",
      "```\n",
      "------\n",
      "Re-testing After Changes\n",
      "## Re-testing After Changes\n",
      "Suppose you made yet another modification to the prompt; while the reference dataset stays unchanged, we generate a new set of answers to compare:\n",
      "\n",
      "<Accordion title=\"New toy data generation\" defaultOpen={false}>\n",
      "  ```python\n",
      "data = [\n",
      "  [\"Why is the sky blue?\",\n",
      "   \"The sky is blue because molecules in the air scatter blue light from the sun more than they scatter red light.\",\n",
      "   \"The sky looks blue because air molecules scatter the blue light from the sun more effectively than other colors.\"],\n",
      "\n",
      "  [\"How do airplanes stay in the air?\",\n",
      "   \"Airplanes stay in the air because their wings create lift by forcing air to move faster over the top of the wing than underneath, which creates lower pressure on top.\",\n",
      "   \"Airplanes fly by generating lift through the wings, which makes the air move faster above them, lowering the pressure.\"],\n",
      "\n",
      "  [\"Why do we have seasons?\",\n",
      "   \"We have seasons because the Earth is tilted on its axis, which causes different parts of the Earth to receive more or less sunlight throughout the year.\",\n",
      "   \"Seasons change because the distance between the Earth and the sun varies throughout the year.\"],  # This response contradicts the reference.\n",
      "\n",
      "  [\"How do magnets work?\",\n",
      "   \"Magnets work because they have a magnetic field that can attract or repel certain metals, like iron, due to the alignment of their atomic particles.\",\n",
      "   \"Magnets operate by creating a magnetic field, which interacts with certain metals like iron due to the specific alignment of atomic particles.\"],\n",
      "\n",
      "  [\"Why does the moon change shape?\",\n",
      "   \"The moon changes shape, or goes through phases, because we see different portions of its illuminated half as it orbits the Earth.\",\n",
      "   \"The moon's phases occur because we observe varying portions of its lit half as it moves around the Earth.\"]\n",
      "]\n",
      "\n",
      "columns = [\"question\", \"target_response\", \"response\"]\n",
      "\n",
      "eval_data_2 = pd.DataFrame(data, columns=columns)\n",
      "  ```\n",
      "</Accordion>  \n",
      "\n",
      "### Create New Dataset\n",
      "Generate a new dataset based on the new responses:\n",
      "\n",
      "```python\n",
      "eval_dataset_2 = Dataset.from_pandas(pd.DataFrame(eval_data_2),\n",
      "data_definition=DataDefinition())\n",
      "```  \n",
      "\n",
      "### Evaluate the New Dataset\n",
      "Apply previously defined descriptors and report composition with conditions to the new dataset:\n",
      "\n",
      "```python\n",
      "eval_dataset_2.add_descriptors(descriptors=descriptors)\n",
      "my_eval_2 = report.run(eval_dataset_2, None)\n",
      "ws.add_run(project.id, my_eval_2, include_data=True)\n",
      "```  \n",
      "\n",
      "### Explore the New Report\n",
      "You will be able to review the report for an overview of which responses passed or failed the tests, showcasing the reports based on truth values and styles.\n",
      "------\n",
      "Creating a Monitoring Dashboard\n",
      "## Creating a Monitoring Dashboard\n",
      "As the reports accumulate, you may wish to keep track of results over time. You can configure a Dashboard programmatically or through the UI. We will proceed with a code-driven approach:\n",
      "\n",
      "The following code snippets will create two panels within your Dashboard:\n",
      "\n",
      "```python\n",
      "project.dashboard.add_panel(\n",
      "     DashboardPanelTestSuiteCounter(\n",
      "        title=\"Latest Test run\",\n",
      "        filter=ReportFilter(metadata_values={}, tag_values=[]),\n",
      "        size=WidgetSize.FULL,\n",
      "        statuses=[TestStatus.SUCCESS],\n",
      "        agg=CounterAgg.LAST,\n",
      "    ),\n",
      "    tab=\"Tests\"\n",
      ")\n",
      "project.dashboard.add_panel(\n",
      "    DashboardPanelTestSuite(\n",
      "        title=\"Test results\",\n",
      "        filter=ReportFilter(metadata_values={}, tag_values=[]),\n",
      "        size=WidgetSize.FULL,\n",
      "        panel_type=TestSuitePanelType.DETAILED,\n",
      "    ),\n",
      "    tab=\"Tests\"\n",
      ")\n",
      "project.save()\n",
      "```\n",
      "\n",
      "When you access the UI, you will observe these panels reflecting summaries of test success rates and detailed test results. As you conduct additional tests on the same project, these panels will seamlessly update accordingly.\n",
      "------\n",
      "Conclusion and Next Steps\n",
      "## Conclusion and Next Steps\n",
      "You have now learned how to carry out regression testing for LLM outputs confidently. By creating testing suites, applying evaluations, and monitoring dashboard results, you are equipped to ensure consistent and reliable model outputs.\n",
      "\n",
      "### Suggestions for Future Enhancements:\n",
      "- Integrate your testing suite into CI/CD workflows for continuous assessment on code changes.\n",
      "- Set up alerts to inform your team via email or messaging platforms (like Slack) whenever any of the tests do not pass.\n",
      "- Personalize your LLM judges further to capture the nuanced behavior you desire in your model's responses.\n"
     ]
    }
   ],
   "source": [
    "print(result.title)\n",
    "print()\n",
    "for s in result.sections:\n",
    "    print(\"------\")\n",
    "    print(s.title)\n",
    "    print(s.markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
