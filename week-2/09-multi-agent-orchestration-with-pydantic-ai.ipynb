{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53478a33",
   "metadata": {},
   "source": [
    "# Multi-Agent Orchestration with Pydantic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c85247",
   "metadata": {},
   "source": [
    "## Data Setup and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5c4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path('../data_cache/youtube_videos/')\n",
    "data_files = sorted(data_folder.glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c007aadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f69964e0db42a4a63bc739e890adc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/191 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import docs\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "documents = []\n",
    "\n",
    "for f in tqdm(data_files):\n",
    "    filename = f.name\n",
    "    video_id, _ = filename.split('.')\n",
    "    content = f.read_text(encoding='utf-8')\n",
    "    chunks = docs.sliding_window(content, size=3000, step=1500)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk['video_id'] = video_id\n",
    "        documents.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba046e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x12e35ba10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"content\"],\n",
    "    keyword_fields=[\"video_id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a4768",
   "metadata": {},
   "source": [
    "## Search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52945e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, TypedDict, Optional\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    video_id: str\n",
    "    _id: int # added\n",
    "\n",
    "class SearchTools:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        \n",
    "    def search(self, query: str) -> List[SearchResult]:\n",
    "        \"\"\"\n",
    "        Search the index for documents matching the given query.\n",
    "    \n",
    "        Args:\n",
    "            query (str): The search query string.\n",
    "    \n",
    "        Returns:\n",
    "            List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "                - start (int): The starting position or offset within the source file.\n",
    "                - content (str): A text excerpt or snippet containing the match.\n",
    "                - video_id (str): Youtube video_id for the snippet.\n",
    "                - _id (int): The unique id for the document\n",
    "        \"\"\"\n",
    "        return self.index.search(\n",
    "            query=query,\n",
    "            num_results=5,\n",
    "            output_ids=True,\n",
    "        )\n",
    "\n",
    "    def get_document_by_id(self, _id: int) -> Optional[SearchResult]:\n",
    "        \"\"\"\n",
    "        Retrieve a document by its unique ID.\n",
    "\n",
    "        Args:\n",
    "            _id (int): The document id.\n",
    "\n",
    "        Returns:\n",
    "            SearchResult: The document corresponding to the given ID or None if it's not in the index.\n",
    "        \"\"\"\n",
    "        if _id < 0 or _id >= len(self.index.docs):\n",
    "            return None\n",
    "\n",
    "        return self.index.docs[_id]\n",
    "\n",
    "tools = SearchTools(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a691d9d",
   "metadata": {},
   "source": [
    "# Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73321e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import FunctionToolCallEvent\n",
    "\n",
    "class NamedCallback:\n",
    "\n",
    "    def __init__(self, agent):\n",
    "        self.agent_name = agent.name\n",
    "\n",
    "    async def print_function_calls(self, ctx, event):\n",
    "        # Detect nested streams\n",
    "        if hasattr(event, \"__aiter__\"):\n",
    "            async for sub in event:\n",
    "                await self.print_function_calls(ctx, sub)\n",
    "            return\n",
    "\n",
    "        if isinstance(event, FunctionToolCallEvent):\n",
    "            tool_name = event.part.tool_name\n",
    "            args = event.part.args\n",
    "            print(f\"TOOL CALL ({self.agent_name}): {tool_name}({args})\")\n",
    "\n",
    "    async def __call__(self, ctx, event):\n",
    "        return await self.print_function_calls(ctx, event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcbfa3",
   "metadata": {},
   "source": [
    "## Clarifier Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc8c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0039eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarifier_instructions = \"\"\"\n",
    "You are the CLARIFIER agent.\n",
    "\n",
    "ROLE\n",
    "Your job is to interpret and refine the user's research request so that it can be passed\n",
    "to the RESEARCH agent for structured exploration.\n",
    "\n",
    "OBJECTIVES\n",
    "1. Understand what the user truly wants to learn or achieve (their intent).\n",
    "2. Identify the core topic and any implicit goals (e.g., learn, compare, evaluate, predict, build).\n",
    "3. Ask the user one targeted clarification question — to confirm scope, focus, or purpose.\n",
    "4. Once the user responds, synthesize a refined version of their request that includes:\n",
    "   - The clarified intent (what the user ultimately wants)\n",
    "   - The initial request (in their own words)\n",
    "   - The refined research focus (a precise version suitable for the RESEARCH agent)\n",
    "   - 3–7 search queries that capture the clarified scope and intent\n",
    "   - A short instruction summary for the RESEARCH agent explaining what to explore\n",
    "\n",
    "DATA SOURCES\n",
    "- You may use your own general knowledge to infer user intent.\n",
    "- You may use the `search()` tool to quickly check ambiguous terms or context.\n",
    "\n",
    "INTENT HANDLING\n",
    "- Before searching, infer the underlying intent behind the user's request.\n",
    "  Examples:\n",
    "    - “getting into ML” → learning pathways, beginner resources, first projects\n",
    "    - “AI safety concerns” → risks, ethical challenges, mitigation strategies\n",
    "    - “startup funding trends” → investment patterns, valuations, stages\n",
    "- Generate searches that reflect this **intent**, not just literal words.\n",
    "\n",
    "CONSTRAINTS\n",
    "- Ask the user for clarification **once only**.\n",
    "- Do not fabricate information; if uncertain, clarify directly with the user.\n",
    "- The goal is to output a structured handoff ready for the RESEARCH agent's Stage 1 process.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21154922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchInstructions(BaseModel):\n",
    "    \"\"\"\n",
    "    Output of the CLARIFIER agent.\n",
    "    Provides both the user's raw input and the refined, structured guidance\n",
    "    for the RESEARCH agent to begin its first stage.\n",
    "    \"\"\"\n",
    "    initial_request: str = Field(\n",
    "        ...,\n",
    "        description=\"The user's original question or request, captured verbatim.\"\n",
    "    )\n",
    "    refined_request: str = Field(\n",
    "        ...,\n",
    "        description=\"A clarified, rephrased, and contextually grounded version of the initial request.\"\n",
    "    )\n",
    "    user_intent: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A short summary (1–2 sentences) of what the user truly wants to accomplish \"\n",
    "            \"or learn, inferred from both the initial request and clarification.\"\n",
    "        )\n",
    "    )\n",
    "    queries: List[str] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A list of 3–7 specific search queries derived from the refined request, \"\n",
    "            \"covering complementary angles or subtopics the RESEARCH agent should explore.\"\n",
    "        )\n",
    "    )\n",
    "    instructions: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Concise operational guidance for the RESEARCH agent, explaining how to use \"\n",
    "            \"the queries and what to prioritize during Stage 1 research.\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54374c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clarifier = Agent(\n",
    "    name='clarifier_v2',\n",
    "    instructions=clarifier_instructions,\n",
    "    tools=[tools.search],\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=ResearchInstructions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e72c6",
   "metadata": {},
   "source": [
    "## Reasearch Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7661b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_instructions = \"\"\"\n",
    "You are the RESEARCH agent.\n",
    "\n",
    "ROLE\n",
    "You perform structured research on a proprietary podcast/video database for a specific stage\n",
    "of exploration (Stage 1, 2, or 3).\n",
    "\n",
    "DATA SOURCE\n",
    "- You may ONLY use the `search()` function\n",
    "- Every reference must cite a real snippet with a valid `youtube_id`, `timestamp` and `_id`.\n",
    "- Do not invent data, names, or timestamps.\n",
    "\n",
    "STAGES\n",
    "\n",
    "Stage 1 — Initial Search\n",
    "- Use the user’s question or clarified keywords from context.\n",
    "- Identify 3–5 primary keywords, run one or more searches.\n",
    "- Summarize the main findings, highlighting initial insights and directions.\n",
    "\n",
    "Stage 2 — Expansion\n",
    "- Build upon Stage 1 outputs (from context).\n",
    "- Generate 5–7 related or complementary queries.\n",
    "- Summarize recurring ideas and patterns across new results.\n",
    "\n",
    "Stage 3 — Deep Dive\n",
    "- Build upon Stage 1 and Stage 2.\n",
    "- Generate 5–7 deeper or contrasting queries.\n",
    "- Explore nuances, counterpoints, or mechanisms.\n",
    "- Provide a more analytical synthesis.\n",
    "\n",
    "CONSTRAINTS\n",
    "- Use context from previous stages to guide deeper exploration.\n",
    "- You must perform the necessary amount of queries for each stage:\n",
    "    - 3-5 for stage 1\n",
    "    - 5-7 for stage 2\n",
    "    - 5-7 for stage 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8da55edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    \"\"\"\n",
    "    A single, verifiable citation to a transcript snippet or video segment.\n",
    "    Must correspond to a real snippet returned by the `search()` tool.\n",
    "    \"\"\"\n",
    "    document_id: int = Field(..., description=\"Internal ID of the transcript snippet.\")\n",
    "    quote: str = Field(..., description=\"Exact snippet that supports the keyword or insight.\")\n",
    "    timestamp: str = Field(..., description=\"Timestamp in the source video where the quote occurs, 'mm:ss' or 'h:mm:ss'\")\n",
    "    relevance_to_keyword: str = Field(..., description=\"Explanation of *how* this quote supports or illustrates the specific keyword or concept being explored.\")\n",
    "    relevance_to_user_intent:  str = Field(..., description=\"Explanation of *how* this quote help the user with their intent.\")\n",
    "\n",
    "class ResearchKeyword(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a keyword explicitly searched during this research stage.\n",
    "    Each keyword must match an actual query used in the search tool calls.\n",
    "    \"\"\"\n",
    "    keyword: str = Field(..., description=\"The exact keyword or phrase used in the search() tool call.\")\n",
    "    relevant_references: List[Reference] = Field(\n",
    "        ..., \n",
    "        description=\"List of transcript snippets directly relevant to this keyword. Each must include a 'relevance_to_keyword' explanation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class VerifiableInsight(BaseModel):\n",
    "    \"\"\"\n",
    "    A synthesized insight that can be traced back to specific evidence.\n",
    "    Each insight must be supported by at least one real reference.\n",
    "    \"\"\"\n",
    "    insight: str = Field(..., description=\"An insight derived from the research, phrased in an evidence-based, verifiable way.\")\n",
    "    references: List[Reference] = Field(..., description=\"Citations that directly support this insight. Must contain valid timestamps and IDs.\")\n",
    "\n",
    "\n",
    "class ResearchStageReport(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured output for each research stage (1–3).\n",
    "    Ensures traceability between searches, keywords, and findings.\n",
    "    \"\"\"\n",
    "    stage: int = Field(..., description=\"The research stage number (1 = Initial Search, 2 = Expansion, 3 = Deep Dive).\")\n",
    "    explored_keywords: List[ResearchKeyword] = Field(\n",
    "        ..., \n",
    "        description=\"List of the *exact* keywords used in this stage's search() calls, along with references showing their relevance.\"\n",
    "    )\n",
    "    verifiable_insights: List[VerifiableInsight] = Field(\n",
    "        ..., \n",
    "        description=\"List of data-backed insights derived from the references gathered at this stage.\"\n",
    "    )\n",
    "    stage_summary: str = Field(..., description=\"Analytical summary of what was learned at this stage, connecting evidence to emerging themes.\")\n",
    "    recommended_next_steps: str = Field(..., description=\"Guidance for what to do in the next stage — e.g., new angles, counterpoints, or subtopics.\")\n",
    "    recommended_next_keywords: List[str] = Field(\n",
    "        ..., \n",
    "        description=\"Suggested next queries based on gaps or promising directions discovered in this stage.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b57225d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = Agent(\n",
    "    name='researcher_v2',\n",
    "    instructions=researcher_instructions,\n",
    "    tools=[tools.search],\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=ResearchStageReport\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e8e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def do_research(\n",
    "    stage: int,\n",
    "    stage_instructions: str,\n",
    "    previous_stages: List[ResearchStageReport]\n",
    ") -> ResearchStageReport:\n",
    "    previous_stages_json = '\\n'.join([r.model_dump_json() for r in previous_stages])\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Current stage: {stage}\n",
    "\n",
    "    Stage instrustructions:\n",
    "    {stage_instructions}\n",
    "\n",
    "    Previous stages:\n",
    "    {previous_stages_json}\n",
    "    \"\"\"\n",
    "\n",
    "    callback = NamedCallback(researcher)\n",
    "    \n",
    "    results = await researcher.run(\n",
    "        user_prompt=user_prompt,\n",
    "        event_stream_handler=callback,\n",
    "    )\n",
    "\n",
    "    return results.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ef0cc",
   "metadata": {},
   "source": [
    "## Synthesizer and Verifier Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8796cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer_instructions = \"\"\"\n",
    "You are the SYNTHESIZER agent.\n",
    "\n",
    "ROLE\n",
    "You create a cohesive, factual article by synthesizing verified information from all\n",
    "three research stages (StageReports 1–3).\n",
    "\n",
    "DATA SOURCES\n",
    "- You will receive one or more `ResearchStageReport` objects, each containing\n",
    "  verifiable references with document_ids, timestamps, and quotes.\n",
    "- You have access to the tool `get_document_by_id` to retrieve full source text\n",
    "  for any reference.\n",
    "- You must use this tool to verify every claim that appears in your article.\n",
    "\n",
    "TASKS\n",
    "1. Carefully read all StageReports and extract recurring insights and verified facts.\n",
    "2. Use `get_document_by_id` to check each cited reference and confirm that\n",
    "   the quote or insight is correctly represented.\n",
    "3. Only include claims that are explicitly supported by at least one verified source.\n",
    "4. Synthesize related findings into 5–6 cohesive sections with a logical flow.\n",
    "5. Ensure that the article aligns with the original user intent (as passed from the clarifier).\n",
    "\n",
    "ARTICLE STRUCTURE\n",
    "- Introduction: Summarize what the article will explore and why it matters.\n",
    "- 5-6 body sections, each:\n",
    "  - Centered on one major theme or subtopic.\n",
    "  - Contains 3–4 related claims (each 3–4 sentences long).\n",
    "  - Each claim includes an in-text reference\n",
    "- Conclusion: Summarize the most important insights and actionable takeaways.\n",
    "\n",
    "VERIFICATION RULES\n",
    "- For every claim, retrieve at least one cited source using `get_document_by_id`\n",
    "  and confirm that the text supports the claim.\n",
    "- If a reference cannot be verified or is inconsistent, omit it.\n",
    "- Do not invent or infer facts beyond what’s supported by verified material.\n",
    "\n",
    "STYLE\n",
    "- Maintain factual, neutral, and coherent tone.\n",
    "- Avoid speculation, exaggeration, or unsupported synthesis.\n",
    "- Write in clear prose suitable for an informed but general audience.\n",
    "\n",
    "OUTPUT\n",
    "- A single, well-structured factual article ready for presentation.\n",
    "- All references cited\n",
    "\"\"\".strip()\n",
    "\n",
    "synthesizer = Agent(\n",
    "    name='synthesizer_v2',\n",
    "    instructions=synthesizer_instructions,\n",
    "    tools=[tools.get_document_by_id],\n",
    "    model='gpt-4o-mini',\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a381a",
   "metadata": {},
   "source": [
    "## Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e778b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_instructions = \"\"\"\n",
    "first, ask user an initial question via clarifier (clarify_tool_initial)\n",
    "then formulate requiremets for the researcher (clarify_tool_research_task)\n",
    "then execute research via researcher in three stages: 1, 2, 3 (reserch_tool)\n",
    "each research step should be done after the previous one is completed\n",
    "\n",
    "make it timeless: don't add years to queries. for example:\n",
    "\"learning machine learning\" is better than \"learning machine learning in 2023\"\n",
    "\n",
    "when the resarch it ready, output a short summary of the research\n",
    "\"\"\"\n",
    "\n",
    "orchestrator = Agent(\n",
    "    name='orchestrator',\n",
    "    instructions=orchestrator_instructions,\n",
    "    model='gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aacefe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import RunContext\n",
    "\n",
    "@orchestrator.tool\n",
    "async def clarify_tool_initial(ctx: RunContext, query: str) -> str:\n",
    "    \"\"\"Runs the clarifier once to interpret the user's request.\n",
    "\n",
    "    Args:\n",
    "        query: Raw user question.\n",
    "\n",
    "    Returns:\n",
    "        A short text summary describing the user's intent.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Clarifier (Initial) ===\")\n",
    "    callback = NamedCallback(clarifier)\n",
    "    results = await clarifier.run(user_prompt=query, event_stream_handler=callback)\n",
    "    return results.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "007bdc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@orchestrator.tool\n",
    "async def clarify_tool_research_task(ctx: RunContext, query: str) -> ResearchInstructions:\n",
    "    \"\"\"Runs the clarifier again using both the user query and prior clarifier output\n",
    "    to create a structured ResearchInstructions object.\n",
    "\n",
    "    Args:\n",
    "        query: User's original question.\n",
    "\n",
    "    Returns:\n",
    "        ResearchInstructions with refined request, intent, and search queries.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Clarifier (Research Task) ===\")\n",
    "    prior_outputs = []\n",
    "    for m in ctx.messages:\n",
    "        for p in m.parts:\n",
    "            if p.part_kind == \"tool-return\" and p.tool_name == \"clarify_tool_initial\":\n",
    "                prior_outputs.append(p.content)\n",
    "\n",
    "    prior_text = \"\\n\".join(str(x) for x in prior_outputs)\n",
    "    prompt = f\"User query:\\n{query}\\n\\nPrior clarification:\\n{prior_text}\".strip()\n",
    "\n",
    "    callback = NamedCallback(clarifier)\n",
    "    results = await clarifier.run(\n",
    "        user_prompt=prompt,\n",
    "        event_stream_handler=callback,\n",
    "        output_type=ResearchInstructions\n",
    "    )\n",
    "    return results.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bde564c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@orchestrator.tool\n",
    "async def research_tool(ctx: RunContext, stage: int, stage_instructions: str) -> ResearchStageReport:\n",
    "    \"\"\"Runs one stage of research using prior reports as context.\n",
    "\n",
    "    Args:\n",
    "        stage: Research stage number (1–3).\n",
    "        stage_instructions: Description of what this stage should focus on.\n",
    "\n",
    "    Returns:\n",
    "        ResearchStageReport with insights, references, and next steps.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== RESEARCH stage {stage} ===\")\n",
    "    \n",
    "    prior_reports: List[ResearchStageReport] = []\n",
    "\n",
    "    for m in ctx.messages:\n",
    "        for p in m.parts:\n",
    "            if p.part_kind == \"tool-return\" and p.tool_name == \"research_tool\":\n",
    "                if isinstance(p.content, ResearchStageReport):\n",
    "                    prior_reports.append(p.content)\n",
    "    \n",
    "    result = await do_research(\n",
    "        stage=stage,\n",
    "        stage_instructions=stage_instructions,\n",
    "        previous_stages=prior_reports,\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4924ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_callback = NamedCallback(orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fde370d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I get started with data engineering?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4fb1afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (orchestrator): clarify_tool_initial({\"query\":\"How do I get started with data engineering?\"})\n",
      "\n",
      "=== Clarifier (Initial) ===\n",
      "TOOL CALL (clarifier_v2): search({\"query\":\"getting started with data engineering\"})\n",
      "TOOL CALL (clarifier_v2): search({\"query\":\"data engineering skills and resources for beginners\"})\n",
      "TOOL CALL (clarifier_v2): search({\"query\":\"data engineering resources for beginners learning path\"})\n",
      "TOOL CALL (clarifier_v2): search({\"query\":\"data engineering online courses platforms\"})\n"
     ]
    },
    {
     "ename": "ModelHTTPError",
     "evalue": "status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-5NKPfgoJPcxCp9o32nbQIdJw on tokens per min (TPM): Limit 200000, Used 184786, Requested 15883. Please try again in 200ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py:487\u001b[39m, in \u001b[36mOpenAIChatModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    486\u001b[39m     extra_headers.setdefault(\u001b[33m'\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m'\u001b[39m, get_user_agent())\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m    488\u001b[39m         model=\u001b[38;5;28mself\u001b[39m._model_name,\n\u001b[32m    489\u001b[39m         messages=openai_messages,\n\u001b[32m    490\u001b[39m         parallel_tool_calls=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    491\u001b[39m         tools=tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    492\u001b[39m         tool_choice=tool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    493\u001b[39m         stream=stream,\n\u001b[32m    494\u001b[39m         stream_options={\u001b[33m'\u001b[39m\u001b[33minclude_usage\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m NOT_GIVEN,\n\u001b[32m    495\u001b[39m         stop=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    496\u001b[39m         max_completion_tokens=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    497\u001b[39m         timeout=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    498\u001b[39m         response_format=response_format \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    499\u001b[39m         seed=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    500\u001b[39m         reasoning_effort=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_reasoning_effort\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    501\u001b[39m         user=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_user\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    502\u001b[39m         web_search_options=web_search_options \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    503\u001b[39m         service_tier=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_service_tier\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    504\u001b[39m         prediction=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_prediction\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    505\u001b[39m         temperature=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    506\u001b[39m         top_p=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    507\u001b[39m         presence_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    508\u001b[39m         frequency_penalty=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    509\u001b[39m         logit_bias=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    510\u001b[39m         logprobs=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_logprobs\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    511\u001b[39m         top_logprobs=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mopenai_top_logprobs\u001b[39m\u001b[33m'\u001b[39m, NOT_GIVEN),\n\u001b[32m    512\u001b[39m         extra_headers=extra_headers,\n\u001b[32m    513\u001b[39m         extra_body=model_settings.get(\u001b[33m'\u001b[39m\u001b[33mextra_body\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2585\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2584\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2587\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2588\u001b[39m         {\n\u001b[32m   2589\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2590\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2591\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2592\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2593\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2594\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2595\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2596\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2597\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2598\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2599\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2600\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2601\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2602\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2603\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2604\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2605\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2606\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2607\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2608\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2609\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2612\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2613\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2614\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2615\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2616\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2617\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2619\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2620\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2621\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2622\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2623\u001b[39m         },\n\u001b[32m   2624\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2625\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2626\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2627\u001b[39m     ),\n\u001b[32m   2628\u001b[39m     options=make_request_options(\n\u001b[32m   2629\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2630\u001b[39m     ),\n\u001b[32m   2631\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2632\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2633\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2634\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-5NKPfgoJPcxCp9o32nbQIdJw on tokens per min (TPM): Limit 200000, Used 184786, Requested 15883. Please try again in 200ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModelHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m orchestrator_results = \u001b[38;5;28;01mawait\u001b[39;00m orchestrator.run(\n\u001b[32m      2\u001b[39m     user_prompt=question,\n\u001b[32m      3\u001b[39m     event_stream_handler=orchestrator_callback\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/agent/abstract.py:240\u001b[39m, in \u001b[36mAbstractAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, builtin_tools, event_stream_handler)\u001b[39m\n\u001b[32m    236\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    237\u001b[39m             \u001b[38;5;28mself\u001b[39m.is_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_call_tools_node(node)\n\u001b[32m    238\u001b[39m         ):\n\u001b[32m    239\u001b[39m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m node.stream(agent_run.ctx) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m                 \u001b[38;5;28;01mawait\u001b[39;00m event_stream_handler(_agent_graph.build_run_context(agent_run.ctx), stream)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent_run.result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mThe graph run did not finish properly\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m agent_run.result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mNamedCallback.__call__\u001b[39m\u001b[34m(self, ctx, event)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctx, event):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_function_calls(ctx, event)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mNamedCallback.print_function_calls\u001b[39m\u001b[34m(self, ctx, event)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_function_calls\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctx, event):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Detect nested streams\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(event, \u001b[33m\"\u001b[39m\u001b[33m__aiter__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[32m     12\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.print_function_calls(ctx, sub)\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:638\u001b[39m, in \u001b[36mCallToolsNode._run_stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    634\u001b[39m             \u001b[38;5;28mself\u001b[39m._next_node = ModelRequestNode[DepsT, NodeRunEndT](_messages.ModelRequest(parts=[e.tool_retry]))\n\u001b[32m    636\u001b[39m     \u001b[38;5;28mself\u001b[39m._events_iterator = _run_stream()\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events_iterator:\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:605\u001b[39m, in \u001b[36mCallToolsNode._run_stream.<locals>._run_stream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    603\u001b[39m alternatives: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_calls:\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_tool_calls(ctx, tool_calls):\n\u001b[32m    606\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:654\u001b[39m, in \u001b[36mCallToolsNode._handle_tool_calls\u001b[39m\u001b[34m(self, ctx, tool_calls)\u001b[39m\n\u001b[32m    651\u001b[39m output_parts: \u001b[38;5;28mlist\u001b[39m[_messages.ModelRequestPart] = []\n\u001b[32m    652\u001b[39m output_final_result: deque[result.FinalResult[NodeRunEndT]] = deque(maxlen=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m process_tool_calls(\n\u001b[32m    655\u001b[39m     tool_manager=ctx.deps.tool_manager,\n\u001b[32m    656\u001b[39m     tool_calls=tool_calls,\n\u001b[32m    657\u001b[39m     tool_call_results=\u001b[38;5;28mself\u001b[39m.tool_call_results,\n\u001b[32m    658\u001b[39m     final_result=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    659\u001b[39m     ctx=ctx,\n\u001b[32m    660\u001b[39m     output_parts=output_parts,\n\u001b[32m    661\u001b[39m     output_final_result=output_final_result,\n\u001b[32m    662\u001b[39m ):\n\u001b[32m    663\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_final_result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:835\u001b[39m, in \u001b[36mprocess_tool_calls\u001b[39m\u001b[34m(tool_manager, tool_calls, tool_call_results, final_result, ctx, output_parts, output_final_result)\u001b[39m\n\u001b[32m    832\u001b[39m deferred_calls: \u001b[38;5;28mdict\u001b[39m[Literal[\u001b[33m'\u001b[39m\u001b[33mexternal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33munapproved\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mlist\u001b[39m[_messages.ToolCallPart]] = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m calls_to_run:\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m _call_tools(\n\u001b[32m    836\u001b[39m         tool_manager=tool_manager,\n\u001b[32m    837\u001b[39m         tool_calls=calls_to_run,\n\u001b[32m    838\u001b[39m         tool_call_results=calls_to_run_results,\n\u001b[32m    839\u001b[39m         tracer=ctx.deps.tracer,\n\u001b[32m    840\u001b[39m         usage=ctx.state.usage,\n\u001b[32m    841\u001b[39m         usage_limits=ctx.deps.usage_limits,\n\u001b[32m    842\u001b[39m         output_parts=output_parts,\n\u001b[32m    843\u001b[39m         output_deferred_calls=deferred_calls,\n\u001b[32m    844\u001b[39m     ):\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Finally, we handle deferred tool calls (unless they were already included in the run because results were provided)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:965\u001b[39m, in \u001b[36m_call_tools\u001b[39m\u001b[34m(tool_manager, tool_calls, tool_call_results, tracer, usage, usage_limits, output_parts, output_deferred_calls)\u001b[39m\n\u001b[32m    963\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m    964\u001b[39m                 index = tasks.index(task)\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m event := \u001b[38;5;28;01mawait\u001b[39;00m handle_call_or_result(coro_or_task=task, index=index):\n\u001b[32m    966\u001b[39m                     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    968\u001b[39m \u001b[38;5;66;03m# We append the results at the end, rather than as they are received, to retain a consistent ordering\u001b[39;00m\n\u001b[32m    969\u001b[39m \u001b[38;5;66;03m# This is mostly just to simplify testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:930\u001b[39m, in \u001b[36m_call_tools.<locals>.handle_call_or_result\u001b[39m\u001b[34m(coro_or_task, index)\u001b[39m\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_call_or_result\u001b[39m(\n\u001b[32m    916\u001b[39m     coro_or_task: Awaitable[\n\u001b[32m    917\u001b[39m         \u001b[38;5;28mtuple\u001b[39m[\n\u001b[32m   (...)\u001b[39m\u001b[32m    926\u001b[39m     index: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    927\u001b[39m ) -> _messages.HandleResponseEvent | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    928\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    929\u001b[39m         tool_part, tool_user_content = (\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m             (\u001b[38;5;28;01mawait\u001b[39;00m coro_or_task) \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(coro_or_task) \u001b[38;5;28;01melse\u001b[39;00m coro_or_task.result()\n\u001b[32m    931\u001b[39m         )\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.CallDeferred:\n\u001b[32m    933\u001b[39m         deferred_calls_by_index[index] = \u001b[33m'\u001b[39m\u001b[33mexternal\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:984\u001b[39m, in \u001b[36m_call_tool\u001b[39m\u001b[34m(tool_manager, tool_call, tool_call_result)\u001b[39m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tool_call_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m         tool_result = \u001b[38;5;28;01mawait\u001b[39;00m tool_manager.handle_call(tool_call)\n\u001b[32m    985\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_call_result, ToolApproved):\n\u001b[32m    986\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m tool_call_result.override_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_tool_manager.py:112\u001b[39m, in \u001b[36mToolManager.handle_call\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors)\u001b[39m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool(call, allow_partial, wrap_validation_errors)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_function_tool(\n\u001b[32m    113\u001b[39m         call,\n\u001b[32m    114\u001b[39m         allow_partial,\n\u001b[32m    115\u001b[39m         wrap_validation_errors,\n\u001b[32m    116\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.tracer,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.trace_include_content,\n\u001b[32m    118\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.instrumentation_version,\n\u001b[32m    119\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx.usage,\n\u001b[32m    120\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_tool_manager.py:236\u001b[39m, in \u001b[36mToolManager._call_function_tool\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors, tracer, include_content, instrumentation_version, usage)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\n\u001b[32m    232\u001b[39m     instrumentation_names.get_tool_span_name(call.tool_name),\n\u001b[32m    233\u001b[39m     attributes=span_attributes,\n\u001b[32m    234\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m         tool_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_tool(call, allow_partial, wrap_validation_errors)\n\u001b[32m    237\u001b[39m         usage.tool_calls += \u001b[32m1\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ToolRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_tool_manager.py:159\u001b[39m, in \u001b[36mToolManager._call_tool\u001b[39m\u001b[34m(self, call, allow_partial, wrap_validation_errors)\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m         args_dict = validator.validate_python(call.args \u001b[38;5;129;01mor\u001b[39;00m {}, allow_partial=pyd_allow_partial)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.toolset.call_tool(name, args_dict, ctx, tool)\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationError, ModelRetry) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/toolsets/combined.py:90\u001b[39m, in \u001b[36mCombinedToolset.call_tool\u001b[39m\u001b[34m(self, name, tool_args, ctx, tool)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_tool\u001b[39m(\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, tool_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]\n\u001b[32m     88\u001b[39m ) -> Any:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, _CombinedToolsetTool)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tool.source_toolset.call_tool(name, tool_args, ctx, tool.source_tool)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/toolsets/function.py:350\u001b[39m, in \u001b[36mFunctionToolset.call_tool\u001b[39m\u001b[34m(self, name, tool_args, ctx, tool)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_tool\u001b[39m(\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, tool_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]\n\u001b[32m    348\u001b[39m ) -> Any:\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, FunctionToolsetTool)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tool.call_func(tool_args, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_function_schema.py:52\u001b[39m, in \u001b[36mFunctionSchema.call\u001b[39m\u001b[34m(self, args_dict, ctx)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_async:\n\u001b[32m     51\u001b[39m     function = cast(Callable[[Any], Awaitable[\u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;28mself\u001b[39m.function)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m function(*args, **kwargs)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     function = cast(Callable[[Any], \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mself\u001b[39m.function)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mclarify_tool_initial\u001b[39m\u001b[34m(ctx, query)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Clarifier (Initial) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m callback = NamedCallback(clarifier)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m clarifier.run(user_prompt=query, event_stream_handler=callback)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results.output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/agent/abstract.py:239\u001b[39m, in \u001b[36mAbstractAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, deferred_tool_results, model, deps, model_settings, usage_limits, usage, infer_name, toolsets, builtin_tools, event_stream_handler)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    236\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event_stream_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    237\u001b[39m             \u001b[38;5;28mself\u001b[39m.is_model_request_node(node) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_call_tools_node(node)\n\u001b[32m    238\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m node.stream(agent_run.ctx) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m    240\u001b[39m                 \u001b[38;5;28;01mawait\u001b[39;00m event_stream_handler(_agent_graph.build_run_context(agent_run.ctx), stream)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent_run.result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mThe graph run did not finish properly\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.1/lib/python3.13/contextlib.py:214\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/_agent_graph.py:407\u001b[39m, in \u001b[36mModelRequestNode.stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._did_stream, \u001b[33m'\u001b[39m\u001b[33mstream() should only be called once per node\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    406\u001b[39m model_settings, model_request_parameters, message_history, run_context = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prepare_request(ctx)\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ctx.deps.model.request_stream(\n\u001b[32m    408\u001b[39m     message_history, model_settings, model_request_parameters, run_context\n\u001b[32m    409\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m streamed_response:\n\u001b[32m    410\u001b[39m     \u001b[38;5;28mself\u001b[39m._did_stream = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    411\u001b[39m     ctx.state.usage.requests += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.1/lib/python3.13/contextlib.py:214\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py:424\u001b[39m, in \u001b[36mOpenAIChatModel.request_stream\u001b[39m\u001b[34m(self, messages, model_settings, model_request_parameters, run_context)\u001b[39m\n\u001b[32m    419\u001b[39m check_allow_model_requests()\n\u001b[32m    420\u001b[39m model_settings, model_request_parameters = \u001b[38;5;28mself\u001b[39m.prepare_request(\n\u001b[32m    421\u001b[39m     model_settings,\n\u001b[32m    422\u001b[39m     model_request_parameters,\n\u001b[32m    423\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._completions_create(\n\u001b[32m    425\u001b[39m     messages, \u001b[38;5;28;01mTrue\u001b[39;00m, cast(OpenAIChatModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[32m    426\u001b[39m )\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m response:\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_streamed_response(response, model_request_parameters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/kasteion/ai-bootcamp/.venv/lib/python3.13/site-packages/pydantic_ai/models/openai.py:517\u001b[39m, in \u001b[36mOpenAIChatModel._completions_create\u001b[39m\u001b[34m(self, messages, stream, model_settings, model_request_parameters)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (status_code := e.status_code) >= \u001b[32m400\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code=status_code, model_name=\u001b[38;5;28mself\u001b[39m.model_name, body=e.body) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mModelHTTPError\u001b[39m: status_code: 429, model_name: gpt-4o-mini, body: {'message': 'Rate limit reached for gpt-4o-mini in organization org-5NKPfgoJPcxCp9o32nbQIdJw on tokens per min (TPM): Limit 200000, Used 184786, Requested 15883. Please try again in 200ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}"
     ]
    }
   ],
   "source": [
    "orchestrator_results = await orchestrator.run(\n",
    "    user_prompt=question,\n",
    "    event_stream_handler=orchestrator_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b327319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages(messages):\n",
    "    contents = []\n",
    "    \n",
    "    for m in messages:\n",
    "        print(m.kind)\n",
    "\n",
    "        for p in m.parts:\n",
    "            print(p.part_kind)\n",
    "            kind = p.part_kind\n",
    "            if kind == 'user-prompt' or kind == 'text':\n",
    "                print(p.content)\n",
    "            if kind == 'tool-call': \n",
    "                print(p.tool_name, p.args)\n",
    "            if kind == 'tool-return':\n",
    "                print(type(p.content), p.content)\n",
    "            print()\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac59d37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orchestrator_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m print_messages(\u001b[43morchestrator_results\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'orchestrator_results' is not defined"
     ]
    }
   ],
   "source": [
    "print_messages(orchestrator_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = NamedCallback(orchestrator)\n",
    "\n",
    "message_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input('You ', )\n",
    "    if user_input.lower().strip() == 'stop':\n",
    "        break\n",
    "\n",
    "    results = await orchestrator.run(\n",
    "        user_prompt=user_input,\n",
    "        message_history=message_history,\n",
    "        event_stream_handler=callback,\n",
    "    )\n",
    "\n",
    "    new_messages = results.new_messages()\n",
    "    message_history.extend(new_messages)\n",
    "    print_messages(new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_reports: List[ResearchStageReport] = []\n",
    "\n",
    "for m in message_history:\n",
    "    for p in m.parts:\n",
    "        if p.part_kind == \"tool-return\" and p.tool_name == \"research_tool\":\n",
    "            if isinstance(p.content, ResearchStageReport):\n",
    "                prior_reports.append(p.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prior_reports)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
