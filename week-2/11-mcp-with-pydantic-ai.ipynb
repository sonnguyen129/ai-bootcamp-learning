{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73420d74",
   "metadata": {},
   "source": [
    "# MCP with Pydantic AI. Using MCP SSE Transport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa3eef",
   "metadata": {},
   "source": [
    "In practice you probably won't ever need to implement the client yourself when we integrate tools into our agents: frameworks like Agents SDK and PydanticAI can do it.\n",
    "\n",
    "Let's see how to do it with PydanticAI.\n",
    "\n",
    "Make sure you have Pydantic AI with MCP support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47db1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m259 packages\u001b[0m \u001b[2min 1.22s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe package `pydantic-ai==1.1.0` does not have an extra named `mcp`\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m239 packages\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add 'pydantic-ai[mcp]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d20bbb",
   "metadata": {},
   "source": [
    "At the moment of writing, there are problems with running this code on Windows. If you're on Windows, skip to \"Running MCP with SSE\".\n",
    "\n",
    "We will run it in Terminal (because of async-io) - but the code with SSE we will create later will also work in Jupyter. If you only use Jupyter, also skip to \"Running MCP with SSE\".\n",
    "\n",
    "Here's our script test.py - create it in a separate folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.mcp import MCPServerStdio\n",
    "from toyaikit.chat.interface import StdOutputInterface\n",
    "from toyaikit.chat.runners import PydanticAIRunner\n",
    "\n",
    "mcp_client = MCPServerStdio(\n",
    "    command=\"uv\",\n",
    "    args=[\"run\", \"python\", \"main.py\"],\n",
    "    cwd=\"faq-mcp\"\n",
    ")\n",
    "\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    toolsets=[mcp_client],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "\n",
    "chat_interface = StdOutputInterface()\n",
    "runner = PydanticAIRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=agent\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(runner.run())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0409ae",
   "metadata": {},
   "source": [
    "We'll deal with dependencies using uv, so let's create an empty project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b446bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv init \n",
    "!uv add pydantic-ai[mcp] openai toyaikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34143bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ecfb4",
   "metadata": {},
   "source": [
    "## Running MCP with SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541958c",
   "metadata": {},
   "source": [
    "Previously we used Standard Input/Output as the transport for MCP. We can also use HTTP (SSE) for that.\n",
    "\n",
    "The only thing we need to change is how we run our server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefff3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp.run(transport=\"sse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b4fe03",
   "metadata": {},
   "source": [
    "It's now available at \"http://localhost:8000/sse\".\n",
    "\n",
    "When it comes to our code, we only need to change this part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.mcp import MCPServerSSE\n",
    "\n",
    "mcp_client = MCPServerSSE(\n",
    "    url='http://localhost:8000/sse'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1da56",
   "metadata": {},
   "source": [
    "How it will use HTTP for communication.\n",
    "\n",
    "Now our Pydantic AI agents can use MCP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f800356-f940-40bf-8ce5-4a05138cd590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.mcp import MCPServerSSE\n",
    "from toyaikit.chat.interface import StdOutputInterface\n",
    "from toyaikit.chat.runners import PydanticAIRunner\n",
    "\n",
    "mcp_client = MCPServerSSE(\n",
    "    url='http://localhost:8000/sse'\n",
    ")\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    toolsets=[mcp_client],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "\n",
    "chat_interface = StdOutputInterface()\n",
    "runner = PydanticAIRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4cea195-4dfd-4d08-920c-fc327e00b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  how do I install kafka?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Function Call ---\n",
      "Function: search\n",
      "Arguments: \"{\\\"query\\\":\\\"install kafka\\\"}\"\n",
      "Result: [{'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.', 'section': 'Module 6: streaming with kafka', 'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable', 'course': 'data-engineering-zoomcamp'}, {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent', 'course': 'data-engineering-zoomcamp'}, {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build', 'course': 'data-engineering-zoomcamp'}]\n",
      "-------------------\n",
      "\n",
      "\n",
      "--- Function Call ---\n",
      "Function: search\n",
      "Arguments: \"{\\\"query\\\":\\\"how to install kafka\\\"}\"\n",
      "Result: [{'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py', 'course': 'data-engineering-zoomcamp'}, {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.', 'section': 'Module 6: streaming with kafka', 'question': 'Kafka- python videos have low audio and hard to follow up', 'course': 'data-engineering-zoomcamp'}, {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.', 'section': 'Module 6: streaming with kafka', 'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable', 'course': 'data-engineering-zoomcamp'}, {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent', 'course': 'data-engineering-zoomcamp'}, {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar', 'section': 'Module 6: streaming with kafka', 'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build', 'course': 'data-engineering-zoomcamp'}]\n",
      "-------------------\n",
      "\n",
      "\n",
      "--- Function Call ---\n",
      "Function: add_entry\n",
      "Arguments: \"{\\\"question\\\":\\\"how do I install kafka?\\\",\\\"answer\\\":\\\"To install Apache Kafka, follow these steps: 1. **Download Apache Kafka**: Visit the [Apache Kafka website](https://kafka.apache.org/downloads) and choose the version you want to install. 2. **Extract the Package**: Unzip the downloaded package into your desired directory. 3. **Start Zookeeper**: Kafka requires Zookeeper, so start it using the command: `bin/zookeeper-server-start.sh config/zookeeper.properties` (on Unix) or `bin\\\\to\\\\run\\\\to/windows\\\\find\\\\the\\\\folder\\\\to\\\\run\\\\bin\\\\to\\\\run\\\\the\\\\bat\\\\file` on Windows. 4. **Start Kafka Server**: In another terminal, start the Kafka server using: `bin/kafka-server-start.sh config/server.properties` (on Unix) or `bin\\\\to\\\\run\\\\to/windows\\\\find\\\\the\\\\folder\\\\to\\\\run\\\\bin\\\\to\\\\run\\\\the\\\\bat\\\\file` on Windows. 5. **Create a Topic**: Create a new topic by running: `bin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1` for Unix or equivalent commands for Windows. 6. **Send and Consume Messages**: Use the Kafka console producer and consumer to send and consume messages. You can execute the commands `bin/kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092` to produce messages and `bin/kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092` to consume them. For Python integration, consider using the confluent-kafka library: `pip install confluent-kafka`.\\\"}\"\n",
      "Result: []\n",
      "-------------------\n",
      "\n",
      "\n",
      "Assistant: To install Apache Kafka, you can follow these steps:\n",
      "\n",
      "1. **Download Apache Kafka**: Go to the [Apache Kafka website](https://kafka.apache.org/downloads) and select the version you want to install.\n",
      "\n",
      "2. **Extract the Package**: Unzip the downloaded package into your desired directory.\n",
      "\n",
      "3. **Start Zookeeper**: Kafka requires Zookeeper to run. Start it by executing:\n",
      "   - For Unix: `bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
      "   - For Windows: `bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties`\n",
      "\n",
      "4. **Start Kafka Server**: Open another terminal window and start the Kafka server:\n",
      "   - For Unix: `bin/kafka-server-start.sh config/server.properties`\n",
      "   - For Windows: `bin\\windows\\kafka-server-start.bat config\\server.properties`\n",
      "\n",
      "5. **Create a Topic**: After starting the server, you can create a topic using the following command:\n",
      "   - For Unix: `bin/kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`\n",
      "   - For Windows: `bin\\windows\\kafka-topics.bat --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`\n",
      "   \n",
      "6. **Send and Consume Messages**: Use the Kafka console producer and consumer to interact with your Kafka topic:\n",
      "   - To produce messages: \n",
      "     - For Unix: `bin/kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9092`\n",
      "     - For Windows: `bin\\windows\\kafka-console-producer.bat --topic my-topic --bootstrap-server localhost:9092`\n",
      "   - To consume messages: \n",
      "     - For Unix: `bin/kafka-console-consumer.sh --topic my-topic --from-beginning --bootstrap-server localhost:9092`\n",
      "     - For Windows: `bin\\windows\\kafka-console-consumer.bat --topic my-topic --from-beginning --bootstrap-server localhost:9092`\n",
      "     \n",
      "7. **Optional**: If you plan to use Kafka with Python, you might want to install the `confluent-kafka` library by running:\n",
      "   - `pip install confluent-kafka`\n",
      "\n",
      "Please confirm if you need specifics related to Docker or any other installation methods for Kafka. Are there any other areas you want to explore?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862feb77-a6e6-4ebd-8167-37954ac73a07",
   "metadata": {},
   "source": [
    "Now we can use this MCP server with any MCP Client. For example, Cursor.\n",
    "\n",
    "Add this server to .cursor/mcp.json:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"faqmcp\": {\n",
    "      \"command\": \"uv\",\n",
    "      \"args\": [\n",
    "        \"run\",\n",
    "        \"--project\", \"faq-mcp\",\n",
    "        \"python\",\n",
    "        \"faq-mcp/main.py\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "If we run our MCP server with SSE transport, we configure it this way:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"faqmcp\": {\n",
    "      \"url\": \"http://localhost:8000/sse\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "If you don't get asked if you want to enable it, go to Preferenes -> Cursor settings -> MCP and Integrations, find your MCP server and enable it.\n",
    "\n",
    "Examples of prompts:\n",
    "\n",
    "- \"Write code for module 1, check the FAQ for requirements\"\n",
    "- \"Implement kafka connection with Python. Use FAQ to do comprehensive research first and then explain your choices.\"\n",
    "\n",
    "Note: this isn't really a good usecase for Cursor. A more powerful usecase would be adding search for some frameworks. LLMs have some knowledge cutoff, while frameworks keep developing. So having access to fresh information is important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
