{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5742e051",
   "metadata": {},
   "source": [
    "# Introduction to Pydantic AI\n",
    "\n",
    "https://ai.pydantic.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7157a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m239 packages\u001b[0m \u001b[2min 2.40s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2K\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`transformers==4.57.0` is yanked (reason: \"Error in the setup causing installation issues\")\u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)                                                  \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------\u001b[0m\u001b[0m     0 B/7.47 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/7.47 KiB   \u001b[1A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/32.25 KiB  \u001b[2A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/32.25 KiB  \u001b[2A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/32.25 KiB  \u001b[2A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/32.25 KiB  \u001b[3A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/32.25 KiB  \u001b[3A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/32.25 KiB  \u001b[3A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/32.25 KiB  \u001b[3A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[4A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[4A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[4A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[4A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[4A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-proto          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[5A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.64 KiB/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-proto          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[5A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.64 KiB/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-proto          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[5A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.64 KiB/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-proto          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[5A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.64 KiB/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-proto          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[5A\n",
      "\u001b[2mopentelemetry-util-http      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mhttpx-sse                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.64 KiB/7.64 KiB\n",
      "\u001b[2mopentelemetry-instrumentation\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-proto          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)----------------------\u001b[0m\u001b[0m     0 B/76.06 KiB  \u001b[5A\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mcachetools                              \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/11.02 KiB\n",
      "\u001b[2mpydantic-ai                             \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/11.42 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.93 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[14A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/52)2m------------------------------\u001b[0m\u001b[0m     0 B/76.06 KiB                                                                       \u001b[14A\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mpydantic-ai                             \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/11.42 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.93 KiB\n",
      "\u001b[2mtenacity                                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/27.59 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[16A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)2m------------------------------\u001b[0m\u001b[0m     0 B/76.06 KiB                                                                       \u001b[16A\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mpydantic-ai                             \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/11.42 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.93 KiB\n",
      "\u001b[2mtenacity                                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/27.59 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2K\u001b[16A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)2m------------------------------\u001b[0m\u001b[0m     0 B/76.06 KiB                                                                       \u001b[16A\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mpydantic-ai                             \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/11.42 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/17.93 KiB\n",
      "\u001b[2mjmespath                                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/19.78 KiB\n",
      "\u001b[2mtenacity                                \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/27.59 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/32.25 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/70.83 KiB\n",
      "\u001b[2mtypes-protobuf                          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 48.00 KiB/76.06 KiB\n",
      "\u001b[2K\u001b[20A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)-\u001b[2m---------------------------\u001b[0m\u001b[0m 46.91 KiB/476.21 KiB                                                                    \u001b[20A\n",
      "\u001b[2maiosignal                               \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.31 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.47 KiB\n",
      "\u001b[2mmdurl                                   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.75 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/10.80 KiB\n",
      "\u001b[2mpydantic-ai                             \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/11.42 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 16.00 KiB/17.93 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/19.12 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)-----------------------\u001b[2m-----\u001b[0m\u001b[0m 16.00 KiB/19.78 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/6.90 KiB\n",
      "\u001b[2maiosignal                               \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.31 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.32 KiB/7.47 KiB\n",
      "\u001b[2mmdurl                                   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/9.75 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 16.00 KiB/17.93 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)2m------------------------------\u001b[0m\u001b[0m     0 B/19.12 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/6.90 KiB\n",
      "\u001b[2maiosignal                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.31 KiB/7.31 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.32 KiB/7.47 KiB\n",
      "\u001b[2mmdurl                                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 9.75 KiB/9.75 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.40 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 16.00 KiB/17.93 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)2m------------------------------\u001b[0m\u001b[0m     0 B/19.12 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/6.90 KiB\n",
      "\u001b[2maiosignal                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.31 KiB/7.31 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.32 KiB/7.47 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.40 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 16.00 KiB/17.93 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/19.12 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)-----------------------\u001b[2m-----\u001b[0m\u001b[0m 16.00 KiB/19.78 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/6.90 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 1.32 KiB/7.47 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.40 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 16.00 KiB/17.93 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/19.12 KiB\n",
      "\u001b[2mjmespath                                \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 16.00 KiB/19.78 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/52)2m------------------------------\u001b[0m\u001b[0m     0 B/26.90 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.47 KiB/7.47 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.40 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.84 KiB/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.91 KiB/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-common\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 17.93 KiB/17.93 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mjmespath                                \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 19.78 KiB/19.78 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 16.00 KiB/26.90 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.47 KiB/7.47 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.40 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.84 KiB/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.91 KiB/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mjmespath                                \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 19.78 KiB/19.78 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 16.00 KiB/26.90 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 16.00 KiB/27.01 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mopentelemetry-util-http                 \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.47 KiB/7.47 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.80 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.84 KiB/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.91 KiB/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 16.00 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 16.00 KiB/27.01 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.80 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-instrumentation-httpx     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.84 KiB/14.84 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.91 KiB/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 16.00 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 16.00 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----------------------------\u001b[2m\u001b[0m\u001b[0m 27.59 KiB/27.59 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.80 KiB/10.80 KiB\n",
      "\u001b[2maiohappyeyeballs                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 14.91 KiB/14.91 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mtenacity                                \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.59 KiB/27.59 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------------------\u001b[2m----\u001b[0m\u001b[0m 27.81 KiB/32.25 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mpyperclip                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.80 KiB/10.80 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mtenacity                                \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.59 KiB/27.59 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 27.81 KiB/32.25 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mtenacity                                \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.59 KiB/27.59 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 32.25 KiB/32.25 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 32.25 KiB/32.25 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)---------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 32.25 KiB/32.25 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)---------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\u001b[22A\n",
      "\u001b[2meval-type-backport                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 5.69 KiB/5.69 KiB\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 32.25 KiB/32.25 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)---------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\u001b[22A\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mopentelemetry-instrumentation           \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 32.25 KiB/32.25 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-----------------------\u001b[2m-----\u001b[0m\u001b[0m 34.68 KiB/42.68 KiB\u001b[22A\n",
      "\u001b[2mag-ui-protocol                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 6.90 KiB/6.90 KiB\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.68 KiB/42.68 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-------------\u001b[2m---------------\u001b[0m\u001b[0m 22.10 KiB/45.22 KiB\u001b[22A\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mnexus-rpc                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.09 KiB/27.09 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 22.10 KiB/45.22 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-------------------\u001b[2m---------\u001b[0m\u001b[0m 32.00 KiB/47.38 KiB\u001b[22A\n",
      "\u001b[2mzipp                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 10.04 KiB/10.04 KiB\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 32.00 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 22.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 32.00 KiB/47.38 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----------\u001b[2m------------------\u001b[0m\u001b[0m 18.76 KiB/48.26 KiB\u001b[22A\n",
      "\u001b[2mopentelemetry-exporter-otlp-proto-http  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 16.00 KiB/19.12 KiB\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 17.86 KiB/27.01 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 36.03 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 29.83 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 32.00 KiB/42.45 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 22.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 32.00 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 18.76 KiB/48.26 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-------------------------\u001b[2m---\u001b[0m\u001b[0m 46.88 KiB/53.84 KiB\u001b[22A\n",
      "\u001b[2mpydantic-graph                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 26.90 KiB/26.90 KiB\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.01 KiB/27.01 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 36.03 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 38.19 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.45 KiB/42.45 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 34.76 KiB/48.26 KiB\n",
      "\u001b[2mpydantic-evals                          \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 46.88 KiB/53.84 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)---------------------\u001b[2m-------\u001b[0m\u001b[0m 48.00 KiB/64.19 KiB\u001b[22A\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.01 KiB/27.01 KiB\n",
      "\u001b[2mdocstring-parser                        \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 36.03 KiB/36.03 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 38.19 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.45 KiB/42.45 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 34.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 34.76 KiB/48.26 KiB\n",
      "\u001b[2mpydantic-evals                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 53.84 KiB/53.84 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 48.00 KiB/64.19 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------------\u001b[2m----------\u001b[0m\u001b[0m 45.68 KiB/70.83 KiB\u001b[22A\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.01 KiB/27.01 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 38.19 KiB/38.19 KiB\n",
      "\u001b[2mmultidict                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.45 KiB/42.45 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mpydantic-evals                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 53.84 KiB/53.84 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 48.00 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------------\u001b[2m----------\u001b[0m\u001b[0m 55.41 KiB/83.70 KiB\u001b[22A\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.01 KiB/27.01 KiB\n",
      "\u001b[2mwrapt                                   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 38.19 KiB/38.19 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mpydantic-evals                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 53.84 KiB/53.84 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 50.65 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 55.41 KiB/83.70 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-----------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\u001b[22A\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.01 KiB/27.01 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mpydantic-evals                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 53.84 KiB/53.84 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 50.65 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 62.95 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)--------------------\u001b[2m--------\u001b[0m\u001b[0m 65.77 KiB/91.62 KiB\u001b[22A\n",
      "\u001b[2mimportlib-metadata                      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 27.01 KiB/27.01 KiB\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 50.65 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 62.95 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 65.77 KiB/91.62 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)--------------\u001b[2m--------------\u001b[0m\u001b[0m 46.91 KiB/92.79 KiB\u001b[22A\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 50.65 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 62.95 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 65.77 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 46.91 KiB/92.79 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 59.48 KiB/128.85 KiB\u001b[22A\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mgenai-prices                            \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 47.38 KiB/47.38 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 50.65 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 62.95 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 65.77 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 46.91 KiB/92.79 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 59.48 KiB/128.85 KiB\u001b[22A\n",
      "\u001b[2margcomplete                             \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 42.68 KiB/42.68 KiB\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 38.10 KiB/45.22 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 64.19 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 62.95 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 65.77 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 59.48 KiB/128.85 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 61.68 KiB/136.06 KiB\u001b[22A\n",
      "\u001b[2mpropcache                               \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 45.22 KiB/45.22 KiB\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 64.19 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 62.95 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 59.48 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 61.68 KiB/136.06 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 93.75 KiB/156.53 KiB\u001b[22A\n",
      "\u001b[2mfrozenlist                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 48.26 KiB/48.26 KiB\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 64.19 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 65.04 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 59.48 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 61.68 KiB/136.06 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 93.75 KiB/156.53 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)----\u001b[2m------------------------\u001b[0m\u001b[0m 34.98 KiB/203.08 KiB\u001b[22A\n",
      "\u001b[2mopentelemetry-api                       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 64.19 KiB/64.19 KiB\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 61.44 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 65.04 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 59.48 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 61.68 KiB/136.06 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 93.75 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 34.98 KiB/203.08 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 94.87 KiB/216.12 KiB\u001b[22A\n",
      "\u001b[2mopentelemetry-proto                     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 70.83 KiB/70.83 KiB\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 65.04 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 59.48 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 61.68 KiB/136.06 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 93.75 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 46.24 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 94.87 KiB/216.12 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-----------\u001b[2m-----------------\u001b[0m\u001b[0m 94.33 KiB/222.80 KiB\u001b[22A\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 65.04 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 36.51 KiB/85.27 KiB\n",
      "\u001b[2myarl                                    \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/91.62 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 77.18 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 61.68 KiB/136.06 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 93.75 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 46.24 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 94.87 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 94.33 KiB/222.80 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)---------\u001b[2m-------------------\u001b[0m\u001b[0m 80.00 KiB/232.91 KiB\u001b[22A\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 81.04 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 52.51 KiB/85.27 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 77.18 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 80.00 KiB/136.06 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 109.75 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 61.41 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 110.87 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 110.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 96.00 KiB/232.91 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 110.78 KiB/237.69 KiB\u001b[22A\n",
      "\u001b[2ms3transfer                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 83.70 KiB/83.70 KiB\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 52.51 KiB/85.27 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 93.18 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 96.00 KiB/136.06 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 109.75 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.41 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 126.87 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 125.44 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 112.00 KiB/232.91 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 110.78 KiB/237.69 KiB\u001b[22A\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 52.51 KiB/85.27 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 62.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 93.18 KiB/128.85 KiB\n",
      "\u001b[2mboto3                                   \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 96.00 KiB/136.06 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 109.75 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 77.41 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 126.87 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 125.44 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 112.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 126.78 KiB/237.69 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-------\u001b[2m---------------------\u001b[0m\u001b[0m 78.00 KiB/287.63 KiB\u001b[22A\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 84.51 KiB/85.27 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-sdk                       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 125.18 KiB/128.85 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 154.64 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 106.64 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 171.09 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 158.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 160.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 158.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 113.07 KiB/287.63 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)--------\u001b[2m--------------------\u001b[0m\u001b[0m 96.00 KiB/295.90 KiB\u001b[22A\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 84.51 KiB/85.27 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/92.79 KiB\n",
      "\u001b[2minvoke                                  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 154.64 KiB/156.53 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 106.64 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 171.09 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 158.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 160.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 158.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 113.07 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 96.00 KiB/295.90 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-------\u001b[2m---------------------\u001b[0m\u001b[0m 94.04 KiB/346.71 KiB\u001b[22A\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 84.51 KiB/85.27 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 106.64 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 171.09 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 158.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 160.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 158.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 113.07 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 96.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 94.04 KiB/346.71 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-----------\u001b[2m-----------------\u001b[0m\u001b[0m 144.20 KiB/352.30 KiB\u001b[22A\n",
      "\u001b[2mmarkdown-it-py                          \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 84.51 KiB/85.27 KiB\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 106.64 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 171.09 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 158.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 176.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 174.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 124.23 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 96.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 110.04 KiB/346.71 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (5/52)-----------\u001b[2m-----------------\u001b[0m\u001b[0m 144.20 KiB/352.30 KiB\u001b[22A\n",
      "\u001b[2mlogfire-api                             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 78.91 KiB/92.79 KiB\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 106.64 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 187.09 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 158.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 176.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 174.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 124.23 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 96.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 110.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 144.20 KiB/352.30 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)--\u001b[2m-------------------------\u001b[0m\u001b[0m 64.00 KiB/432.42 KiB\u001b[22A\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 125.41 KiB/203.08 KiB\n",
      "\u001b[2mgoogle-auth                             \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 187.09 KiB/216.12 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 174.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 179.93 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 190.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 140.23 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 96.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 126.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 160.20 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 80.00 KiB/432.42 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)--------------\u001b[2m-------------\u001b[0m\u001b[0m 254.91 KiB/476.21 KiB\u001b[22A\n",
      "\u001b[2mopentelemetry-semantic-conventions      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 189.41 KiB/203.08 KiB\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 206.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 208.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 222.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 174.00 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 144.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 158.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 179.90 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 96.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 286.91 KiB/476.21 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)----\u001b[2m-----------------------\u001b[0m\u001b[0m 222.91 KiB/977.15 KiB\u001b[22A\n",
      "\u001b[2mlogfire                                 \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 222.33 KiB/222.80 KiB\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 208.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 222.88 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 174.00 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 160.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 158.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 179.90 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 96.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 286.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 222.91 KiB/977.15 KiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 191.39 KiB/11.75 MiB\u001b[22A\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 224.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 237.69 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 206.00 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 160.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 195.90 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 128.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 302.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 238.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 207.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 240.00 KiB/13.46 MiB                                                                    \u001b[22A\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 224.00 KiB/232.91 KiB\n",
      "\u001b[2mrich                                    \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 237.69 KiB/237.69 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 222.00 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 160.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 195.90 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 128.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 302.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 238.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 207.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[22A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 240.00 KiB/13.46 MiB                                                                    \u001b[22A\n",
      "\u001b[2mgoogle-genai                            \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 224.00 KiB/232.91 KiB\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 222.00 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 160.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 195.90 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 128.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 302.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 238.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 207.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[20A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 240.00 KiB/13.46 MiB                                                                    \u001b[20A\n",
      "\u001b[2mgoogleapis-common-protos                \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 237.89 KiB/287.63 KiB\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 176.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 174.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 211.90 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 144.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 318.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 254.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 239.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[18A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 256.00 KiB/13.46 MiB                                                                    \u001b[18A\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 208.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 206.04 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 288.20 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 240.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 382.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 322.50 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 303.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[16A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 320.00 KiB/13.46 MiB                                                                    \u001b[16A\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 208.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 211.00 KiB/346.71 KiB\n",
      "\u001b[2mpydantic-ai-slim                        \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 288.20 KiB/352.30 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 240.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 382.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 334.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 303.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[16A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 320.00 KiB/13.46 MiB                                                                    \u001b[16A\n",
      "\u001b[2mcohere                                  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 240.00 KiB/295.90 KiB\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 243.00 KiB/346.71 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 272.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 446.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 414.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 383.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[14A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 400.00 KiB/13.46 MiB                                                                    \u001b[14A\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 243.00 KiB/346.71 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 288.00 KiB/432.42 KiB\n",
      "\u001b[2maiohttp                                 \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 462.91 KiB/476.21 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 446.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 383.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[12A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 432.00 KiB/13.46 MiB                                                                    \u001b[12A\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 259.00 KiB/346.71 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 304.00 KiB/432.42 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 462.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 410.86 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[10A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)2m-----------------------------\u001b[0m\u001b[0m 448.00 KiB/13.46 MiB                                                                    \u001b[10A\n",
      "\u001b[2manthropic                               \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 270.04 KiB/346.71 KiB\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 320.00 KiB/432.42 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 542.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 463.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[10A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)[2m----------------------------\u001b[0m\u001b[0m 528.00 KiB/13.46 MiB                                                                    \u001b[10A\n",
      "\u001b[2mmistralai                               \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 336.00 KiB/432.42 KiB\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 606.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 527.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[8A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (37/52)\u001b[2m----------------------------\u001b[0m\u001b[0m 587.00 KiB/13.46 MiB                                                                    \u001b[8A\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 654.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 591.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (49/52)\u001b[2m----------------------------\u001b[0m\u001b[0m 635.00 KiB/13.46 MiB                                                                    \u001b[6A\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 782.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 751.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (49/52)\u001b[2m----------------------------\u001b[0m\u001b[0m 746.89 KiB/13.46 MiB                                                                    \u001b[6A\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 782.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 751.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (49/52)\u001b[2m----------------------------\u001b[0m\u001b[0m 762.89 KiB/13.46 MiB                                                                    \u001b[6A\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 782.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 767.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (49/52)\u001b[2m----------------------------\u001b[0m\u001b[0m 762.89 KiB/13.46 MiB                                                                    \u001b[6A\n",
      "\u001b[2mfastavro                                \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 974.91 KiB/977.15 KiB\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 991.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[6A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (49/52)-\u001b[2m---------------------------\u001b[0m\u001b[0m 987.00 KiB/13.46 MiB                                                                    \u001b[6A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 1007.39 KiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (49/52)-\u001b[2m---------------------------\u001b[0m\u001b[0m 1003.00 KiB/13.46 MiB                                                                   \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 1.78 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)--\u001b[2m--------------------------\u001b[0m\u001b[0m 1.46 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.00 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---\u001b[2m-------------------------\u001b[0m\u001b[0m 1.99 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 2.33 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----\u001b[2m------------------------\u001b[0m\u001b[0m 2.31 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.97 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----\u001b[2m------------------------\u001b[0m\u001b[0m 2.67 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.36 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-----\u001b[2m-----------------------\u001b[0m\u001b[0m 3.00 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.48 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-----\u001b[2m-----------------------\u001b[0m\u001b[0m 3.00 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.50 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-----\u001b[2m-----------------------\u001b[0m\u001b[0m 3.00 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.61 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)------\u001b[2m----------------------\u001b[0m\u001b[0m 3.16 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 3.61 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-------\u001b[2m---------------------\u001b[0m\u001b[0m 4.00 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 4.00 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)--------\u001b[2m--------------------\u001b[0m\u001b[0m 4.08 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 4.00 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---------\u001b[2m-------------------\u001b[0m\u001b[0m 4.50 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 4.28 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---------\u001b[2m-------------------\u001b[0m\u001b[0m 4.67 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 4.81 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---------\u001b[2m-------------------\u001b[0m\u001b[0m 4.67 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 5.02 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---------\u001b[2m-------------------\u001b[0m\u001b[0m 4.83 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 5.03 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-----------\u001b[2m-----------------\u001b[0m\u001b[0m 5.40 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.45 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-----------\u001b[2m-----------------\u001b[0m\u001b[0m 5.40 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 5.68 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 5.85 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 6.26 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)------------\u001b[2m----------------\u001b[0m\u001b[0m 6.08 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.38 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-------------\u001b[2m---------------\u001b[0m\u001b[0m 6.65 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 6.96 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)--------------\u001b[2m--------------\u001b[0m\u001b[0m 6.75 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 7.06 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---------------\u001b[2m-------------\u001b[0m\u001b[0m 7.39 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.77 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---------------\u001b[2m-------------\u001b[0m\u001b[0m 7.60 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.77 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 7.80 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.77 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 7.91 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.77 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 7.98 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.77 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 7.99 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 7.77 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----------------\u001b[2m------------\u001b[0m\u001b[0m 8.00 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 8.42 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-----------------\u001b[2m-----------\u001b[0m\u001b[0m 8.38 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 9.33 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-------------------\u001b[2m---------\u001b[0m\u001b[0m 9.39 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 9.84 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)--------------------\u001b[2m--------\u001b[0m\u001b[0m 9.62 MiB/13.46 MiB                                                                      \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.05 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)---------------------\u001b[2m-------\u001b[0m\u001b[0m 10.14 MiB/13.46 MiB                                                                     \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 10.57 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)----------------------\u001b[2m------\u001b[0m\u001b[0m 10.72 MiB/13.46 MiB                                                                     \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 11.12 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)-----------------------\u001b[2m-----\u001b[0m\u001b[0m 11.00 MiB/13.46 MiB                                                                     \u001b[4A\n",
      "\u001b[2mtemporalio                              \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.41 MiB/11.75 MiB\n",
      "\u001b[2K\u001b[4A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)------------------------\u001b[2m----\u001b[0m\u001b[0m 11.48 MiB/13.46 MiB                                                                     \u001b[4A\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)------------------------\u001b[2m----\u001b[0m\u001b[0m 11.48 MiB/13.46 MiB                                                                     \u001b[2A\n",
      "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (50/52)--------------------------\u001b[2m--\u001b[0m\u001b[0m 12.20 MiB/13.46 MiB                                                                     \u001b[2A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m52 packages\u001b[0m \u001b[2min 2.78s\u001b[0m\u001b[0m                                                B/13.46 MiB                                                                     \u001b[2A\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m56 packages\u001b[0m \u001b[2min 160ms\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mag-ui-protocol\u001b[0m\u001b[2m==0.1.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.71.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1margcomplete\u001b[0m\u001b[2m==3.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.40.55\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.40.55\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcohere\u001b[0m\u001b[2m==5.19.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1meval-type-backport\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastavro\u001b[0m\u001b[2m==1.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgenai-prices\u001b[0m\u001b[2m==0.0.34\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.41.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.45.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.70.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minvoke\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogfire\u001b[0m\u001b[2m==4.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogfire-api\u001b[0m\u001b[2m==4.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistralai\u001b[0m\u001b[2m==1.9.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnexus-rpc\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-instrumentation\u001b[0m\u001b[2m==0.58b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-instrumentation-httpx\u001b[0m\u001b[2m==0.58b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.58b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-util-http\u001b[0m\u001b[2m==0.58b0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-ai\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-ai-slim\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-evals\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-graph\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyperclip\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtemporalio\u001b[0m\u001b[2m==1.18.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtypes-protobuf\u001b[0m\u001b[2m==6.32.1.20250918\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==1.17.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.23.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add pydantic-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3acfe",
   "metadata": {},
   "source": [
    "## Setting Up the Data and Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e5fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f4b04",
   "metadata": {},
   "source": [
    "## Search tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c6e12",
   "metadata": {},
   "source": [
    "Create the search index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1069e34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x11bb85fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9fad0",
   "metadata": {},
   "source": [
    "Define search functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ff4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    title: str\n",
    "    description: str\n",
    "    filename: str\n",
    "\n",
    "def search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa609138",
   "metadata": {},
   "source": [
    "## File Reading tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba407cc",
   "metadata": {},
   "source": [
    "Set up the file index for quick access to complete documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd822e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {}\n",
    "\n",
    "for doc in parsed_data:\n",
    "    filename = doc['filename']\n",
    "    file_index[filename] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b30c1",
   "metadata": {},
   "source": [
    "Create the file reading function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3db54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def read_file(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the content of a file from the repository.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name or path of the file to read.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The file content as a string if the file exists;\n",
    "        otherwise, returns None.\n",
    "    \"\"\"\n",
    "    if filename in file_index:\n",
    "        return file_index[filename]['content']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436098e",
   "metadata": {},
   "source": [
    "## Agent instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722826fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are an assistant that helps improve and generate high-quality documentation for the project.\n",
    "\n",
    "You have access to the following tools:\n",
    "- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\n",
    "- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\n",
    "\n",
    "Critical Rule\n",
    "\n",
    "Before generating or finalizing any code example or technical explanation, you must always call `read_file`\n",
    "to cross-check the correctness of the code.\n",
    "Do not rely solely on search results or assumptions — always verify by reading the actual file content.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "When answering a question:\n",
    "1. Provide file references for all source materials.  \n",
    "   Use this format:  \n",
    "   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\n",
    "2. If the topic is covered in multiple documents, cite all relevant sources.\n",
    "3. Include code examples whenever they clarify or demonstrate the concept.\n",
    "4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\n",
    "5. If documentation is missing or unclear, infer from context and note that explicitly.\n",
    "\n",
    "Example Citation\n",
    "\n",
    "See the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c32116",
   "metadata": {},
   "source": [
    "## Creating the PydanticAI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165eb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = [search, read_file]\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name='docs_agent',\n",
    "    instructions=instructions,\n",
    "    tools=agent_tools,\n",
    "    model='gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d3dca",
   "metadata": {},
   "source": [
    "### Running Individual Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95011e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await agent.run(\n",
    "    user_prompt=\"how do I run llm as a judge evals?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f160617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request\n",
      "user-prompt\n",
      "\n",
      "response\n",
      "tool-call\n",
      "\n",
      "request\n",
      "tool-return\n",
      "\n",
      "response\n",
      "tool-call\n",
      "\n",
      "request\n",
      "tool-return\n",
      "\n",
      "response\n",
      "text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in results.new_messages():\n",
    "    print(message.kind)\n",
    "\n",
    "    for part in message.parts:\n",
    "        print(part.part_kind)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47354335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To run an LLM (Large Language Model) as a judge for evaluations (evals), you can follow these steps based on the tutorial from Evidently AI. The process involves setting up evaluation criteria, generating datasets, and then using the LLM to assess the quality of responses. Here’s a streamlined overview of the steps:\n",
      "\n",
      "### 1. Prerequisites\n",
      "- **Python Knowledge**: Basic understanding of Python is necessary.\n",
      "- **OpenAI API Key**: Ensure you have an API key to access the LLM.\n",
      "\n",
      "### 2. Installation\n",
      "First, you'll need to install the Evidently library:\n",
      "```bash\n",
      "pip install evidently\n",
      "```\n",
      "\n",
      "### 3. Import Required Libraries\n",
      "In your Python script or notebook, import the necessary modules:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from evidently import Dataset\n",
      "from evidently import DataDefinition\n",
      "from evidently import Report\n",
      "from evidently.descriptors import *\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "import os\n",
      "\n",
      "# Set up the OpenAI API key\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 4. Create an Evaluation Dataset\n",
      "Create a toy dataset with questions, target responses, new responses, and manual labels:\n",
      "```python\n",
      "data = [\n",
      "    # Add your sample data here as tuples\n",
      "]\n",
      "\n",
      "columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\n",
      "golden_dataset = pd.DataFrame(data, columns=columns)\n",
      "\n",
      "# Create the dataset definition\n",
      "definition = DataDefinition(\n",
      "    text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
      "    categorical_columns=[\"label\"]\n",
      ")\n",
      "\n",
      "# Create Evidently Dataset\n",
      "eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
      "```\n",
      "\n",
      "### 5. Configure the LLM as a Judge\n",
      "Set up the evaluator prompt using a descriptor that defines the criteria for evaluation:\n",
      "```python\n",
      "correctness_prompt = BinaryClassificationPromptTemplate(\n",
      "    criteria = \"\"\"An ANSWER is correct when it matches the REFERENCE in all facts and details, \n",
      "                  even if worded differently. The ANSWER is incorrect if it contradicts the REFERENCE.\"\"\",\n",
      "    target_category=\"incorrect\",\n",
      "    non_target_category=\"correct\"\n",
      "    # Add more configurations as needed\n",
      ")\n",
      "\n",
      "# Add the evaluator to your dataset\n",
      "eval_dataset.add_descriptors(descriptors=[\n",
      "    LLMEval(\"new_response\",\n",
      "            template=correctness_prompt,\n",
      "            provider=\"openai\",\n",
      "            model=\"gpt-4o-mini\",\n",
      "            alias=\"Correctness\")\n",
      "])\n",
      "```\n",
      "\n",
      "### 6. Run the Evaluation\n",
      "Run the report to evaluate the dataset:\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "print(my_eval)\n",
      "```\n",
      "\n",
      "### 7. Review and Iterate\n",
      "You can review the results, adapt the prompt for future evaluations, or even change the LLM being used based on the performance of the initial setup.\n",
      "\n",
      "### Additional Resources\n",
      "- For detailed examples and variations, refer to the complete tutorial available in the [Evidently documentation on GitHub](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx).\n",
      "\n",
      "This process guides you through setting up the evaluation, running an LLM as a judge, and leveraging its output to assess response quality effectively. For the exact code and nuances, be sure to consult the examples and the provided documentation.\n"
     ]
    }
   ],
   "source": [
    "print(results.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2073f518",
   "metadata": {},
   "source": [
    "You can also access the complete message history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24924b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='how do I run llm as a judge evals?', timestamp=datetime.datetime(2025, 10, 18, 17, 7, 36, 184965, tzinfo=datetime.timezone.utc))], instructions='You are an assistant that helps improve and generate high-quality documentation for the project.\\n\\nYou have access to the following tools:\\n- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\\n- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\\n\\nCritical Rule\\n\\nBefore generating or finalizing any code example or technical explanation, you must always call `read_file`\\nto cross-check the correctness of the code.\\nDo not rely solely on search results or assumptions — always verify by reading the actual file content.\\n\\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\\n> \"Unable to verify with read_file.\"\\n\\nWhen answering a question:\\n1. Provide file references for all source materials.  \\n   Use this format:  \\n   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\\n2. If the topic is covered in multiple documents, cite all relevant sources.\\n3. Include code examples whenever they clarify or demonstrate the concept.\\n4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\\n5. If documentation is missing or unclear, infer from context and note that explicitly.\\n\\nExample Citation\\n\\nSee the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).'),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='search', args='{\"query\":\"run llm as a judge evals\"}', tool_call_id='call_vCITDKp7JV2r9e5FSmAUfsBl')], usage=RequestUsage(input_tokens=503, output_tokens=20, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 10, 18, 17, 7, 37, tzinfo=TzInfo(0)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CS4gz6A1FvDMAy670F3LuBkyQWXo7', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='search', content=[{'start': 0, 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 1000, 'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 2000, 'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 20000, 'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 19000, 'content': 'w to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically tr', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}], tool_call_id='call_vCITDKp7JV2r9e5FSmAUfsBl', timestamp=datetime.datetime(2025, 10, 18, 17, 7, 37, 879981, tzinfo=datetime.timezone.utc))], instructions='You are an assistant that helps improve and generate high-quality documentation for the project.\\n\\nYou have access to the following tools:\\n- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\\n- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\\n\\nCritical Rule\\n\\nBefore generating or finalizing any code example or technical explanation, you must always call `read_file`\\nto cross-check the correctness of the code.\\nDo not rely solely on search results or assumptions — always verify by reading the actual file content.\\n\\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\\n> \"Unable to verify with read_file.\"\\n\\nWhen answering a question:\\n1. Provide file references for all source materials.  \\n   Use this format:  \\n   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\\n2. If the topic is covered in multiple documents, cite all relevant sources.\\n3. Include code examples whenever they clarify or demonstrate the concept.\\n4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\\n5. If documentation is missing or unclear, infer from context and note that explicitly.\\n\\nExample Citation\\n\\nSee the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).'),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='read_file', args='{\"filename\":\"examples/LLM_judge.mdx\"}', tool_call_id='call_zOWoTRMLyG4KeF7rIegz3cU0')], usage=RequestUsage(input_tokens=3168, output_tokens=21, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 10, 18, 17, 7, 38, tzinfo=TzInfo(0)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CS4h0Mph7xqyEPv6neR5XCtVwPETg', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='read_file', content='import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I add another user to my account?\",\\n     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\\n     \"To add a secondary user, go to \\'Account Settings\\', select \\'Manage Users\\', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\\n     \"incorrect\", \"contradiction (incorrect answer)\"],\\n  \\n    [\"Is it possible to link multiple bank accounts?\",\\n     \"Yes, you can link multiple bank accounts by going to \\'Account Settings\\' in the menu and selecting \\'Add Bank Account\\'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\\n     \"You can add multiple bank accounts by visiting \\'Accounts\\' in the menu and choosing \\'Add Bank Account\\'. Enter your bank details as prompted and complete the verification process for each account to link them successfully.\",\\n     \"incorrect\", \"contradiction (incorrect menu item)\"],\\n  \\n    [\"Can I use your service for cryptocurrency transactions?\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Hi, can I get a detailed report of my monthly transactions?\",\\n     \"Yes, you can generate a detailed monthly report of your transactions by logging into your account, going to \\'Transaction History\\', and selecting \\'Generate Report\\'. You can customize the report by selecting specific dates or transaction types.\",\\n     \"You can get a detailed monthly report by logging into your account, navigating to \\'Transaction History\\', and clicking on \\'Generate Report\\'. Customize your report by choosing the date range and types of transactions you want to include.\",\\n     \"correct\", \"\"],\\n  \\n    [\"I am traveling to the US. Can I use the app there?\",\\n     \"Yes, you can use the app in the US just like you do at home. Ensure you have an internet connection. You may also want to update your app to the latest version before traveling for optimal performance.\",\\n     \"The app will work in the US without any issues. Just make sure you have access to the internet. For the best experience, update your app to the latest version before you travel.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I link my payment account to a new mobile number?\",\\n     \"To link a new mobile number, log in to your account, go to \\'Account Settings\\', select \\'Mobile Number\\', and follow the instructions to verify your new number. You will need to enter the new number and verify it via a code sent to your phone.\",\\n     \"To add a new number, navigate to the \\'Account Settings\\' section, select \\'Mobile Number\\' and proceed with the steps to add and confirm the new number. Enter the new mobile number and verify it using the code sent to your phone.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Can I receive notifications for transactions in real-time?\",\\n     \"Yes, you can enable real-time notifications for transactions by going to \\'Account Settings\\', then \\'Notifications\\', and turning on \\'Transaction Alerts\\'. You can choose to receive alerts via SMS, email, or push notifications on your mobile device.\",\\n     \"To receive real-time notifications for transactions, log into your account, go to \\'Account Settings\\', select \\'Notifications\\', and enable \\'Transaction Alerts\\'. Choose your preferred notification method between email or push notifications.\",\\n     \"incorrect\", \"omits information (sms notification)\"],\\n  \\n    [\"Hey, can I set up automatic transfers to my savings account?\",\\n     \"Yes, you can set up automatic transfers by going to \\'Account Settings\\', selecting \\'Automatic Transfers\\', and specifying the amount and frequency. You can choose to transfer weekly, bi-weekly, or monthly. Make sure to save the settings to activate the transfers.\",\\n     \"You can arrange automatic transfers by going to \\'Account Settings\\', choosing \\'Automatic Transfers\\', and setting the desired amount and frequency. Don\\'t forget to save the changes to enable the automatic transfers.\",\\n     \"incorrect\", \"omits information (limited frequency of transfers available)\"],\\n  \\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How can I update my billing address?\",\\n     \"To update your billing address, log into your account, go to \\'Account Settings\\', select \\'Billing Information\\', and enter your new address. Make sure to save the changes once you are done.\",\\n     \"To update your billing address, log into your account, navigate to \\'Account Settings\\', and select \\'Billing Information\\'. Enter your new address and ensure all fields are filled out correctly. Save the changes, and you will receive a confirmation email with the updated address details.\",\\n     \"incorrect\", \"adds new information (confirmation email)\"],\\n  \\n    [\"How do I contact customer support?\",\\n     \"You can contact customer support by logging into your account, going to the \\'Help\\' section, and selecting \\'Contact Us\\'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\\n     \"To contact customer support, log into your account and go to the \\'Help\\' section. Select \\'Contact Us\\' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\\n     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\\n  \\n    [\"What should I do if my card is lost or stolen?\",\\n     \"If your card is lost or stolen, immediately log into your account, go to \\'Card Management\\', and select \\'Report Lost/Stolen\\'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\\n     \"If your card is lost or stolen, navigate to \\'Card Management\\' in your account, and select \\'Report Lost/Stolen\\'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I enable two-factor authentication (2FA)?\",\\n     \"To enable two-factor authentication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).', tool_call_id='call_zOWoTRMLyG4KeF7rIegz3cU0', timestamp=datetime.datetime(2025, 10, 18, 17, 7, 39, 647690, tzinfo=datetime.timezone.utc))], instructions='You are an assistant that helps improve and generate high-quality documentation for the project.\\n\\nYou have access to the following tools:\\n- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\\n- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\\n\\nCritical Rule\\n\\nBefore generating or finalizing any code example or technical explanation, you must always call `read_file`\\nto cross-check the correctness of the code.\\nDo not rely solely on search results or assumptions — always verify by reading the actual file content.\\n\\nIf `read_file` cannot be used or the file content is unavailable, clearly state:\\n> \"Unable to verify with read_file.\"\\n\\nWhen answering a question:\\n1. Provide file references for all source materials.  \\n   Use this format:  \\n   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\\n2. If the topic is covered in multiple documents, cite all relevant sources.\\n3. Include code examples whenever they clarify or demonstrate the concept.\\n4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\\n5. If documentation is missing or unclear, infer from context and note that explicitly.\\n\\nExample Citation\\n\\nSee the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).'),\n",
       " ModelResponse(parts=[TextPart(content='To run an LLM (Large Language Model) as a judge for evaluations (evals), you can follow these steps based on the tutorial from Evidently AI. The process involves setting up evaluation criteria, generating datasets, and then using the LLM to assess the quality of responses. Here’s a streamlined overview of the steps:\\n\\n### 1. Prerequisites\\n- **Python Knowledge**: Basic understanding of Python is necessary.\\n- **OpenAI API Key**: Ensure you have an API key to access the LLM.\\n\\n### 2. Installation\\nFirst, you\\'ll need to install the Evidently library:\\n```bash\\npip install evidently\\n```\\n\\n### 3. Import Required Libraries\\nIn your Python script or notebook, import the necessary modules:\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.descriptors import *\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\nimport os\\n\\n# Set up the OpenAI API key\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n### 4. Create an Evaluation Dataset\\nCreate a toy dataset with questions, target responses, new responses, and manual labels:\\n```python\\ndata = [\\n    # Add your sample data here as tuples\\n]\\n\\ncolumns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\ngolden_dataset = pd.DataFrame(data, columns=columns)\\n\\n# Create the dataset definition\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n)\\n\\n# Create Evidently Dataset\\neval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\\n```\\n\\n### 5. Configure the LLM as a Judge\\nSet up the evaluator prompt using a descriptor that defines the criteria for evaluation:\\n```python\\ncorrectness_prompt = BinaryClassificationPromptTemplate(\\n    criteria = \"\"\"An ANSWER is correct when it matches the REFERENCE in all facts and details, \\n                  even if worded differently. The ANSWER is incorrect if it contradicts the REFERENCE.\"\"\",\\n    target_category=\"incorrect\",\\n    non_target_category=\"correct\"\\n    # Add more configurations as needed\\n)\\n\\n# Add the evaluator to your dataset\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness_prompt,\\n            provider=\"openai\",\\n            model=\"gpt-4o-mini\",\\n            alias=\"Correctness\")\\n])\\n```\\n\\n### 6. Run the Evaluation\\nRun the report to evaluate the dataset:\\n```python\\nreport = Report([TextEvals()])\\nmy_eval = report.run(eval_dataset, None)\\nprint(my_eval)\\n```\\n\\n### 7. Review and Iterate\\nYou can review the results, adapt the prompt for future evaluations, or even change the LLM being used based on the performance of the initial setup.\\n\\n### Additional Resources\\n- For detailed examples and variations, refer to the complete tutorial available in the [Evidently documentation on GitHub](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx).\\n\\nThis process guides you through setting up the evaluation, running an LLM as a judge, and leveraging its output to assess response quality effectively. For the exact code and nuances, be sure to consult the examples and the provided documentation.')], usage=RequestUsage(input_tokens=7885, cache_read_tokens=3072, output_tokens=693, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 10, 18, 17, 7, 40, tzinfo=TzInfo(0)), provider_name='openai', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-CS4h2vaqgq2scorB9qrFWP8mH7uky', finish_reason='stop')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5e190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await agent.run(\n",
    "    user_prompt=\"show me a complete example for llm as a judge reports\",\n",
    "    message_history=results.all_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83abf794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a complete example of how to run an LLM as a judge for evaluations, using the Evidently library. This example follows the steps outlined in the tutorial, allowing you to evaluate responses and generate reports. \n",
      "\n",
      "### Step-by-Step Example\n",
      "\n",
      "#### 1. Installation\n",
      "\n",
      "Make sure to install the Evidently library:\n",
      "```bash\n",
      "pip install evidently\n",
      "```\n",
      "\n",
      "#### 2. Imports\n",
      "\n",
      "Import the necessary modules:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from evidently import Dataset, DataDefinition, Report\n",
      "from evidently.descriptors import LLMEval, ExactMatch\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "import os\n",
      "\n",
      "# Set your OpenAI API key\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "#### 3. Create the Evaluation Dataset\n",
      "\n",
      "Create a toy Q&A dataset:\n",
      "```python\n",
      "data = [\n",
      "    [\"How can I reset my password?\", \n",
      "     \"To reset your password, click 'Forgot Password' on the login page.\", \n",
      "     \"To reset my password, click 'Forgot Password' on the login screen.\", \n",
      "     \"correct\", \"\"],\n",
      "  \n",
      "    [\"Where can I find my transaction history?\", \n",
      "     \"You can view your transaction history by logging into your account.\", \n",
      "     \"Log into your account and check your transaction history.\", \n",
      "     \"correct\", \"\"],\n",
      "  \n",
      "    [\"How do I add another user to my account?\", \n",
      "     \"Currently, it is not possible to add multiple users.\", \n",
      "     \"To add a new user, go to Account Settings.\", \n",
      "     \"incorrect\", \"contradicts the information.\"],\n",
      "  \n",
      "    [\"How do I contact customer support?\", \n",
      "     \"You can contact support via our help section.\", \n",
      "     \"You can reach customer support through the help section.\", \n",
      "     \"correct\", \"\"]\n",
      "]\n",
      "\n",
      "columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\n",
      "golden_dataset = pd.DataFrame(data, columns=columns)\n",
      "\n",
      "# Create an Evidently dataset object\n",
      "definition = DataDefinition(\n",
      "    text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
      "    categorical_columns=[\"label\"]\n",
      ")\n",
      "\n",
      "eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
      "```\n",
      "\n",
      "#### 4. Configure the LLM as a Judge\n",
      "\n",
      "Set up the evaluator prompt template:\n",
      "```python\n",
      "correctness = BinaryClassificationPromptTemplate(\n",
      "    criteria=\"\"\"An ANSWER is correct when it matches the REFERENCE in all facts and details. \n",
      "                The ANSWER is incorrect if it contradicts the REFERENCE or omits key details.\n",
      "                REFERENCE:\n",
      "                =====\n",
      "                {target_response}\n",
      "                =====\"\"\",\n",
      "    target_category=\"incorrect\",\n",
      "    non_target_category=\"correct\",\n",
      "    uncertainty=\"unknown\",\n",
      "    include_reasoning=True,\n",
      "    pre_messages=[(\"system\", \"You are an expert evaluator.\")]\n",
      ")\n",
      "\n",
      "# Add the LLM judge to your dataset\n",
      "eval_dataset.add_descriptors(descriptors=[\n",
      "    LLMEval(\"new_response\",\n",
      "            template=correctness,\n",
      "            provider=\"openai\",\n",
      "            model=\"gpt-4\",\n",
      "            alias=\"Correctness\",\n",
      "            additional_columns={\"target_response\": \"target_response\"}),\n",
      "])\n",
      "```\n",
      "\n",
      "#### 5. Run the Evaluation\n",
      "\n",
      "Preview the scored dataset:\n",
      "```python\n",
      "scored_data = eval_dataset.as_dataframe()\n",
      "print(scored_data)\n",
      "```\n",
      "\n",
      "#### 6. Get the Report\n",
      "\n",
      "Generate a report to summarize results:\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(eval_dataset, None)\n",
      "print(my_eval)\n",
      "```\n",
      "\n",
      "### 7. Evaluate the LLM’s Quality\n",
      "\n",
      "Evaluate the accuracy of the LLM judge itself:\n",
      "```python\n",
      "df = eval_dataset.as_dataframe()\n",
      "\n",
      "# Setup a new data definition for classification\n",
      "classification_definition = DataDefinition(\n",
      "    classification=[BinaryClassification(\n",
      "        target=\"label\",\n",
      "        prediction_labels=\"Correctness\",\n",
      "        pos_label=\"incorrect\")],\n",
      "    categorical_columns=[\"label\", \"Correctness\"])\n",
      "\n",
      "class_dataset = Dataset.from_pandas(\n",
      "    pd.DataFrame(df),\n",
      "    data_definition=classification_definition)\n",
      "\n",
      "# Generate a report for classification metrics\n",
      "class_report = Report([ClassificationPreset()])\n",
      "class_eval = class_report.run(class_dataset, None)\n",
      "print(class_eval)\n",
      "```\n",
      "\n",
      "### 8. Conclusion\n",
      "\n",
      "This example provides a comprehensive pipeline on how to evaluate responses using an LLM as a judge. You can adapt the templates and datasets to fit your specific needs. \n",
      "\n",
      "For full details, refer to the original tutorial in the [Evidently documentation](https://github.com/evidentlyai/docs/blob/main/examples/LLM_judge.mdx).\n"
     ]
    }
   ],
   "source": [
    "print(results.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb28597",
   "metadata": {},
   "source": [
    "### Usage and Cost Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6bd28",
   "metadata": {},
   "source": [
    "Check token usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcd6b9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunUsage(input_tokens=21911, cache_read_tokens=17024, output_tokens=956, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, requests=2, tool_calls=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91130591",
   "metadata": {},
   "source": [
    "Calculate the cost using ToyAIKit's pricing utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f1a93d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=0.00328665, output_cost=0.0005736, total_cost=0.00386025)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()\n",
    "\n",
    "usage = results.usage()\n",
    "\n",
    "pricing.calculate_cost(\n",
    "    model=agent.model.model_name,\n",
    "    input_tokens=usage.input_tokens,\n",
    "    output_tokens=usage.output_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c555caa",
   "metadata": {},
   "source": [
    "### Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed89dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import PydanticAIRunner\n",
    "\n",
    "chat_interface = IPythonChatInterface()\n",
    "runner = PydanticAIRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac2432b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: how do I run llm as a judge evals?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"run llm as judge evals\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"run llm as judge evals\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'start': 0, 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 1000, 'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 2000, 'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 20000, 'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}, {'start': 19000, 'content': 'w to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically tr', 'title': 'LLM as a judge', 'description': 'How to create and evaluate an LLM judge.', 'filename': 'examples/LLM_judge.mdx'}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>read_file(\"{\\\"filename\\\":\\\"examples/LLM_judge.mdx\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"filename\\\":\\\"examples/LLM_judge.mdx\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>import CloudSignup from '/snippets/cloud_signup.mdx';\n",
       "import CreateProject from '/snippets/create_project.mdx';\n",
       "\n",
       "In this tutorial, we'll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\n",
       "\n",
       "<Info>\n",
       "  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\n",
       "</Info>\n",
       "\n",
       "We'll explore two ways to use an LLM as a judge:\n",
       "\n",
       "- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\n",
       "- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there's no reference available.\n",
       "\n",
       "We will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\n",
       "\n",
       "<Info>\n",
       "**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\n",
       "</Info>\n",
       "\n",
       "## Tutorial scope\n",
       "\n",
       "Here's what we'll do:\n",
       "\n",
       "- **Create an evaluation dataset**. Create a toy Q&A dataset.\n",
       "- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\n",
       "- **Evaluate the judge**. Compare the LLM judge's evaluations with manual labels.\n",
       "\n",
       "We'll start with the reference-based evaluator that determines whether a new response is correct (it's more complex since it requires passing two columns to the prompt). Then, we'll create a simpler judge focused on verbosity.\n",
       "\n",
       "To complete the tutorial, you will need:\n",
       "\n",
       "- Basic Python knowledge.\n",
       "- An OpenAI API key to use for the LLM evaluator.\n",
       "\n",
       "We recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\n",
       "\n",
       "<Info>\n",
       "  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\n",
       "</Info>\n",
       "\n",
       "## 1.  Installation and Imports\n",
       "\n",
       "Install Evidently:\n",
       "\n",
       "```python\n",
       "pip install evidently\n",
       "```\n",
       "\n",
       "Import the required modules:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "from evidently import Dataset\n",
       "from evidently import DataDefinition\n",
       "from evidently import Report\n",
       "from evidently import BinaryClassification\n",
       "from evidently.descriptors import *\n",
       "from evidently.presets import TextEvals, ValueStats, ClassificationPreset\n",
       "from evidently.metrics import *\n",
       "\n",
       "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
       "```\n",
       "\n",
       "Pass your OpenAI key as an environment variable:\n",
       "\n",
       "```python\n",
       "import os\n",
       "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
       "```\n",
       "\n",
       "<Info>\n",
       "**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \n",
       "</Info>\n",
       "\n",
       "## 2.  Create the Dataset\n",
       "\n",
       "First, we'll create a toy Q&A dataset with customer support question that includes:\n",
       "\n",
       "- **Questions**. The inputs sent to the LLM app.\n",
       "- **Target responses**. The approved responses you consider accurate.\n",
       "- **New responses**. Imitated new responses from the system.\n",
       "- **Manual labels with explanation**. Labels that say if response is correct or not.\n",
       "\n",
       "Why add the labels? It's a good idea to be the judge yourself before you write a prompt. This helps:\n",
       "\n",
       "- Formulate better criteria. You discover nuances that help you write a better prompt.\n",
       "- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\n",
       "\n",
       "Ultimately, an LLM judge is a small ML system, and it needs its own evals\\!\n",
       "\n",
       "**Generate the dataframe**. Here's how you can create this dataset in one go:\n",
       "\n",
       "<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\n",
       "  ```python\n",
       "  data = [\n",
       "    [\"Hi there, how do I reset my password?\",\n",
       "     \"To reset your password, click on 'Forgot Password' on the login page and follow the instructions sent to your registered email.\",\n",
       "     \"To change your password, select 'Forgot Password' on the login screen and follow the steps sent to your registered email address. If you don't receive the email, check your spam folder or contact support for assistance.\",\n",
       "     \"incorrect\", \"adds new information (contact support)\"],\n",
       "  \n",
       "    [\"Where can I find my transaction history?\",\n",
       "     \"You can view your transaction history by logging into your account and navigating to the 'Transaction History' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\n",
       "     \"Log into your account and go to 'Transaction History' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How do I add another user to my account?\",\n",
       "     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\n",
       "     \"To add a secondary user, go to 'Account Settings', select 'Manage Users', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\n",
       "     \"incorrect\", \"contradiction (incorrect answer)\"],\n",
       "  \n",
       "    [\"Is it possible to link multiple bank accounts?\",\n",
       "     \"Yes, you can link multiple bank accounts by going to 'Account Settings' in the menu and selecting 'Add Bank Account'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\n",
       "     \"You can add multiple bank accounts by visiting 'Accounts' in the menu and choosing 'Add Bank Account'. Enter your bank details as prompted and complete the verification process for each account to link them successfully.\",\n",
       "     \"incorrect\", \"contradiction (incorrect menu item)\"],\n",
       "  \n",
       "    [\"Can I use your service for cryptocurrency transactions?\",\n",
       "     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\n",
       "     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"Hi, can I get a detailed report of my monthly transactions?\",\n",
       "     \"Yes, you can generate a detailed monthly report of your transactions by logging into your account, going to 'Transaction History', and selecting 'Generate Report'. You can customize the report by selecting specific dates or transaction types.\",\n",
       "     \"You can get a detailed monthly report by logging into your account, navigating to 'Transaction History', and clicking on 'Generate Report'. Customize your report by choosing the date range and types of transactions you want to include.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"I am traveling to the US. Can I use the app there?\",\n",
       "     \"Yes, you can use the app in the US just like you do at home. Ensure you have an internet connection. You may also want to update your app to the latest version before traveling for optimal performance.\",\n",
       "     \"The app will work in the US without any issues. Just make sure you have access to the internet. For the best experience, update your app to the latest version before you travel.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How do I link my payment account to a new mobile number?\",\n",
       "     \"To link a new mobile number, log in to your account, go to 'Account Settings', select 'Mobile Number', and follow the instructions to verify your new number. You will need to enter the new number and verify it via a code sent to your phone.\",\n",
       "     \"To add a new number, navigate to the 'Account Settings' section, select 'Mobile Number' and proceed with the steps to add and confirm the new number. Enter the new mobile number and verify it using the code sent to your phone.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"Can I receive notifications for transactions in real-time?\",\n",
       "     \"Yes, you can enable real-time notifications for transactions by going to 'Account Settings', then 'Notifications', and turning on 'Transaction Alerts'. You can choose to receive alerts via SMS, email, or push notifications on your mobile device.\",\n",
       "     \"To receive real-time notifications for transactions, log into your account, go to 'Account Settings', select 'Notifications', and enable 'Transaction Alerts'. Choose your preferred notification method between email or push notifications.\",\n",
       "     \"incorrect\", \"omits information (sms notification)\"],\n",
       "  \n",
       "    [\"Hey, can I set up automatic transfers to my savings account?\",\n",
       "     \"Yes, you can set up automatic transfers by going to 'Account Settings', selecting 'Automatic Transfers', and specifying the amount and frequency. You can choose to transfer weekly, bi-weekly, or monthly. Make sure to save the settings to activate the transfers.\",\n",
       "     \"You can arrange automatic transfers by going to 'Account Settings', choosing 'Automatic Transfers', and setting the desired amount and frequency. Don't forget to save the changes to enable the automatic transfers.\",\n",
       "     \"incorrect\", \"omits information (limited frequency of transfers available)\"],\n",
       "  \n",
       "    [\"Hi there, how do I reset my password?\",\n",
       "     \"To reset your password, click on 'Forgot Password' on the login page and follow the instructions sent to your registered email.\",\n",
       "     \"To change your password, select 'Forgot Password' on the login screen and follow the steps sent to your registered email address. If you don't receive the email, check your spam folder.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How can I update my billing address?\",\n",
       "     \"To update your billing address, log into your account, go to 'Account Settings', select 'Billing Information', and enter your new address. Make sure to save the changes once you are done.\",\n",
       "     \"To update your billing address, log into your account, navigate to 'Account Settings', and select 'Billing Information'. Enter your new address and ensure all fields are filled out correctly. Save the changes, and you will receive a confirmation email with the updated address details.\",\n",
       "     \"incorrect\", \"adds new information (confirmation email)\"],\n",
       "  \n",
       "    [\"How do I contact customer support?\",\n",
       "     \"You can contact customer support by logging into your account, going to the 'Help' section, and selecting 'Contact Us'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\n",
       "     \"To contact customer support, log into your account and go to the 'Help' section. Select 'Contact Us' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\n",
       "     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\n",
       "  \n",
       "    [\"What should I do if my card is lost or stolen?\",\n",
       "     \"If your card is lost or stolen, immediately log into your account, go to 'Card Management', and select 'Report Lost/Stolen'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\n",
       "     \"If your card is lost or stolen, navigate to 'Card Management' in your account, and select 'Report Lost/Stolen'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How do I enable two-factor authentication (2FA)?\",\n",
       "     \"To enable two-factor authentication, log into your account, go to 'Security Settings', and select 'Enable 2FA'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\n",
       "     \"To enable two-factor authentication, log into your account, navigate to 'Security Settings', and choose 'Enable 2FA'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\n",
       "     \"incorrect\", \"adds new information (backup codes)\"]\n",
       "  ]\n",
       "  \n",
       "  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\n",
       "  \n",
       "  golden_dataset = pd.DataFrame(data, columns=columns)\n",
       "  ```\n",
       "</Accordion>\n",
       "\n",
       "<Note>\n",
       "  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\n",
       "</Note>\n",
       "\n",
       "**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\n",
       "\n",
       "```python\n",
       "definition = DataDefinition(\n",
       "    text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
       "    categorical_columns=[\"label\"]\n",
       "    )\n",
       "\n",
       "eval_dataset = Dataset.from_pandas(\n",
       "    golden_dataset,\n",
       "    data_definition=definition)\n",
       "```\n",
       "\n",
       "To preview the dataset:\n",
       "\n",
       "```python\n",
       "pd.set_option('display.max_colwidth', None)\n",
       "golden_dataset.head(5)\n",
       "```\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_data_preview-min.png)\n",
       "\n",
       "Here's the distribution of examples: we have both correct and incorrect responses.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\n",
       "\n",
       "<Accordion title=\"How to preview\" defaultOpen={false}>\n",
       "  Run this to preview the distribution of the column.\n",
       "\n",
       "  ```python\n",
       "  report = Report([\n",
       "    ValueStats(column=\"label\")\n",
       "  ])\n",
       "  \n",
       "  my_eval = report.run(eval_dataset, None)\n",
       "  my_eval\n",
       "  \n",
       "  # my_eval.dict()\n",
       "  # my_eval.json()\n",
       "  ```\n",
       "</Accordion>\n",
       "\n",
       "## 3. Correctness evaluator\n",
       "\n",
       "Now it's time to set up an LLM judge! We'll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\n",
       "\n",
       "**Configure the evaluator prompt**. We'll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here's how to define the prompt template for correctness:\n",
       "\n",
       "```python\n",
       "correctness = BinaryClassificationPromptTemplate(\n",
       "        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\n",
       "        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\n",
       "        REFERENCE:\n",
       "        =====\n",
       "        {target_response}\n",
       "        =====\"\"\",\n",
       "        target_category=\"incorrect\",\n",
       "        non_target_category=\"correct\",\n",
       "        uncertainty=\"unknown\",\n",
       "        include_reasoning=True,\n",
       "        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\n",
       "        )\n",
       "```\n",
       "\n",
       "<Info>\n",
       "  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don't need to ask for these details explicitly, or worry about parsing the output structure — that's built into the template. You only need to add the criteria. You can also use a multi-class template.\n",
       "</Info>\n",
       "\n",
       "In this example, we've set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\n",
       "\n",
       "**Score your data**. To add this new descriptor to your dataset, run:\n",
       "\n",
       "```python\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    LLMEval(\"new_response\",\n",
       "            template=correctness,\n",
       "            provider = \"openai\",\n",
       "            model = \"gpt-4o-mini\",\n",
       "            alias=\"Correctness\",\n",
       "            additional_columns={\"target_response\": \"target_response\"}),\n",
       "    ])\n",
       "```\n",
       "\n",
       "**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\n",
       "\n",
       "```python\n",
       "eval_dataset.as_dataframe()\n",
       "```\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\n",
       "\n",
       "<Info>\n",
       "  **Note**: your explanations will vary since LLMs are non-deterministic.\n",
       "</Info>\n",
       "\n",
       "If you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\n",
       "\n",
       "```python\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\n",
       "```\n",
       "\n",
       "**Get a Report.** Summarize the result by generating an Evidently Report.\n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    TextEvals()\n",
       "])\n",
       "\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "my_eval\n",
       "```\n",
       "\n",
       "This will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_report-min.png)\n",
       "\n",
       "Since we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\n",
       "\n",
       "## 4. Evaluate the LLM Eval quality\n",
       "\n",
       "This part is a bit meta: we're going to evaluate the quality of our LLM evaluator itself\\! We can treat it as a simple **binary classification** problem.\n",
       "\n",
       "**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\n",
       "\n",
       "```python\n",
       "df=eval_dataset.as_dataframe()\n",
       "\n",
       "definition_2 = DataDefinition(\n",
       "    classification=[BinaryClassification(\n",
       "        target=\"label\",\n",
       "        prediction_labels=\"Correctness\",\n",
       "        pos_label = \"incorrect\")],\n",
       "    categorical_columns=[\"label\", \"Correctness\"])\n",
       "\n",
       "class_dataset = Dataset.from_pandas(\n",
       "    pd.DataFrame(df),\n",
       "    data_definition=definition_2)\n",
       "```\n",
       "\n",
       "<Info>\n",
       "  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\n",
       "</Info>\n",
       "\n",
       "**Get a Report**. Let's use a`ClassificationPreset()` that combines several classification metrics:\n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    ClassificationPreset()\n",
       "])\n",
       "\n",
       "my_eval = report.run(class_dataset, None)\n",
       "my_eval\n",
       "\n",
       "# or my_eval.as_dict()\n",
       "```\n",
       "\n",
       "We can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\! You can also refine the prompt to try to improve them.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\n",
       "\n",
       "## 5. Verbosity evaluator\n",
       "\n",
       "Next, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\n",
       "\n",
       "Here's how to set up the prompt template for verbosity:\n",
       "\n",
       "```python\n",
       "verbosity = BinaryClassificationPromptTemplate(\n",
       "        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\n",
       "            A concise response should:\n",
       "            - Provide the necessary information without unnecessary details or repetition.\n",
       "            - Be brief yet comprehensive enough to address the query.\n",
       "            - Use simple and direct language to convey the message effectively.\"\"\",\n",
       "        target_category=\"concise\",\n",
       "        non_target_category=\"verbose\",\n",
       "        uncertainty=\"unknown\",\n",
       "        include_reasoning=True,\n",
       "        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\n",
       "        )\n",
       "```\n",
       "\n",
       "Add this new descriptor to our existing dataset:\n",
       "\n",
       "```python\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    LLMEval(\"new_response\",\n",
       "            template=verbosity,\n",
       "            provider = \"openai\",\n",
       "            model = \"gpt-4o-mini\",\n",
       "            alias=\"Verbosity\")\n",
       "    ])\n",
       "```\n",
       "\n",
       "Run the Report and view the summary results: \n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    TextEvals()\n",
       "])\n",
       "\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "my_eval\n",
       "```\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_verbosity-min.png)\n",
       "\n",
       "You can also view the dataframe using `eval_dataset.as_dataframe()`\n",
       "\n",
       "<Info>\n",
       "  Don't fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you've got your golden dataset\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\n",
       "</Info>\n",
       "\n",
       "## What's next?\n",
       "\n",
       "The LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\n",
       "\n",
       "To be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\n",
       "\n",
       "### Set up Evidently Cloud\n",
       "\n",
       "<CloudSignup />\n",
       "\n",
       "Import the components to connect with Evidently Cloud:\n",
       "\n",
       "```python\n",
       "from evidently.ui.workspace import CloudWorkspace\n",
       "```\n",
       "\n",
       "### Create a Project\n",
       "\n",
       "<CreateProject />\n",
       "\n",
       "### Send your eval\n",
       "\n",
       "Since you already created the eval, you can simply upload it to the Evidently Cloud.\n",
       "\n",
       "```python\n",
       "ws.add_run(project.id, my_eval, include_data=True)\n",
       "```\n",
       "\n",
       "You can then go to the Evidently Cloud, open your Project and explore the Report.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_cloud-min.png)\n",
       "\n",
       "<Info>\n",
       "  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\n",
       "</Info>\n",
       "\n",
       "# Reference documentation\n",
       "\n",
       "See this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To run an LLM as a judge for evaluations, you can follow this structured approach outlined in the tutorial provided in the documentation. Here's a concise guide:</p>\n",
       "<h2>Overview</h2>\n",
       "<p>You can evaluate responses using an LLM as a judge in two ways:</p>\n",
       "<ol>\n",
       "<li><strong>Reference-based</strong>: Compare new responses against a known correct reference.</li>\n",
       "<li><strong>Open-ended</strong>: Assess responses against custom criteria when no reference is available.</li>\n",
       "</ol>\n",
       "<h2>Steps to Implement LLM Judge</h2>\n",
       "<h3>1. Install Necessary Packages</h3>\n",
       "<p>You will need to install the Evidently library if you haven't already. Use the following command:</p>\n",
       "<pre><code class=\"language-bash\">pip install evidently\n",
       "</code></pre>\n",
       "<h3>2. Import Required Libraries</h3>\n",
       "<p>Import the necessary modules in your Python environment:</p>\n",
       "<pre><code class=\"language-python\">import pandas as pd\n",
       "import numpy as np\n",
       "from evidently import Dataset, DataDefinition, Report\n",
       "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
       "</code></pre>\n",
       "<h3>3. Set Up OpenAI API Key</h3>\n",
       "<p>You need an OpenAI API key to utilize the LLM:</p>\n",
       "<pre><code class=\"language-python\">import os\n",
       "os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR_KEY&quot;\n",
       "</code></pre>\n",
       "<h3>4. Create an Evaluation Dataset</h3>\n",
       "<p>You can create a toy dataset for evaluation. It may include columns for questions, target responses, and new responses with labels:</p>\n",
       "<pre><code class=\"language-python\">data = [\n",
       "    # Sample data format\n",
       "    [&quot;What is the capital of France?&quot;, &quot;Paris&quot;, &quot;Lyon&quot;, &quot;incorrect&quot;, &quot;incorrect answer&quot;],\n",
       "    # Add more rows as needed\n",
       "]\n",
       "columns = [&quot;question&quot;, &quot;target_response&quot;, &quot;new_response&quot;, &quot;label&quot;, &quot;comment&quot;]\n",
       "golden_dataset = pd.DataFrame(data, columns=columns)\n",
       "</code></pre>\n",
       "<h3>5. Define Dataset Structure</h3>\n",
       "<p>Create an Evidently <code>Dataset</code> object with appropriate column mappings:</p>\n",
       "<pre><code class=\"language-python\">definition = DataDefinition(\n",
       "    text_columns=[&quot;question&quot;, &quot;target_response&quot;, &quot;new_response&quot;],\n",
       "    categorical_columns=[&quot;label&quot;]\n",
       ")\n",
       "eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
       "</code></pre>\n",
       "<h3>6. Configure the LLM Judge</h3>\n",
       "<p>Set up the prompt for the LLM judge. For example, to evaluate correctness:</p>\n",
       "<pre><code class=\"language-python\">correctness = BinaryClassificationPromptTemplate(\n",
       "    criteria=&quot;&quot;&quot;An ANSWER is correct when it is the same as the REFERENCE in all facts ...&quot;&quot;&quot;,\n",
       "    target_category=&quot;incorrect&quot;,\n",
       "    non_target_category=&quot;correct&quot;\n",
       ")\n",
       "</code></pre>\n",
       "<p>Then, add this descriptor to your dataset:</p>\n",
       "<pre><code class=\"language-python\">eval_dataset.add_descriptors(descriptors=[\n",
       "    LLMEval(&quot;new_response&quot;,\n",
       "            template=correctness,\n",
       "            provider=&quot;openai&quot;,\n",
       "            model=&quot;gpt-4o-mini&quot;,\n",
       "            alias=&quot;Correctness&quot;)\n",
       "])\n",
       "</code></pre>\n",
       "<h3>7. Generate a Report</h3>\n",
       "<p>Summarize the results using an Evidently report to visualize the judge's performance:</p>\n",
       "<pre><code class=\"language-python\">report = Report([TextEvals()])\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "</code></pre>\n",
       "<h3>8. Evaluate the LLM's Quality</h3>\n",
       "<p>You can also evaluate the quality of the LLM evaluator itself by treating it as a binary classification problem and generating appropriate metrics.</p>\n",
       "<h3>Resources</h3>\n",
       "<p>For more details and examples, you can refer to the full implementation in the tutorial available <a href=\"https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb\">here</a> and the complete <a href=\"https://github.com/evidentlyai/docs/blob/main/metrics/customize_llm_judge\">documentation on LLM judges</a>.</p>\n",
       "<p>This setup will allow you to effectively utilize an LLM to evaluate responses as a judge based on your custom criteria or reference datasets.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: show me a complete example for llm as a judge reports\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>read_file(\"{\\\"filename\\\":\\\"examples/LLM_judge.mdx\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"filename\\\":\\\"examples/LLM_judge.mdx\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>import CloudSignup from '/snippets/cloud_signup.mdx';\n",
       "import CreateProject from '/snippets/create_project.mdx';\n",
       "\n",
       "In this tutorial, we'll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\n",
       "\n",
       "<Info>\n",
       "  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\n",
       "</Info>\n",
       "\n",
       "We'll explore two ways to use an LLM as a judge:\n",
       "\n",
       "- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\n",
       "- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there's no reference available.\n",
       "\n",
       "We will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\n",
       "\n",
       "<Info>\n",
       "**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\n",
       "</Info>\n",
       "\n",
       "## Tutorial scope\n",
       "\n",
       "Here's what we'll do:\n",
       "\n",
       "- **Create an evaluation dataset**. Create a toy Q&A dataset.\n",
       "- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\n",
       "- **Evaluate the judge**. Compare the LLM judge's evaluations with manual labels.\n",
       "\n",
       "We'll start with the reference-based evaluator that determines whether a new response is correct (it's more complex since it requires passing two columns to the prompt). Then, we'll create a simpler judge focused on verbosity.\n",
       "\n",
       "To complete the tutorial, you will need:\n",
       "\n",
       "- Basic Python knowledge.\n",
       "- An OpenAI API key to use for the LLM evaluator.\n",
       "\n",
       "We recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\n",
       "\n",
       "<Info>\n",
       "  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\n",
       "</Info>\n",
       "\n",
       "## 1.  Installation and Imports\n",
       "\n",
       "Install Evidently:\n",
       "\n",
       "```python\n",
       "pip install evidently\n",
       "```\n",
       "\n",
       "Import the required modules:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "from evidently import Dataset\n",
       "from evidently import DataDefinition\n",
       "from evidently import Report\n",
       "from evidently import BinaryClassification\n",
       "from evidently.descriptors import *\n",
       "from evidently.presets import TextEvals, ValueStats, ClassificationPreset\n",
       "from evidently.metrics import *\n",
       "\n",
       "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
       "```\n",
       "\n",
       "Pass your OpenAI key as an environment variable:\n",
       "\n",
       "```python\n",
       "import os\n",
       "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
       "```\n",
       "\n",
       "<Info>\n",
       "**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \n",
       "</Info>\n",
       "\n",
       "## 2.  Create the Dataset\n",
       "\n",
       "First, we'll create a toy Q&A dataset with customer support question that includes:\n",
       "\n",
       "- **Questions**. The inputs sent to the LLM app.\n",
       "- **Target responses**. The approved responses you consider accurate.\n",
       "- **New responses**. Imitated new responses from the system.\n",
       "- **Manual labels with explanation**. Labels that say if response is correct or not.\n",
       "\n",
       "Why add the labels? It's a good idea to be the judge yourself before you write a prompt. This helps:\n",
       "\n",
       "- Formulate better criteria. You discover nuances that help you write a better prompt.\n",
       "- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\n",
       "\n",
       "Ultimately, an LLM judge is a small ML system, and it needs its own evals\\!\n",
       "\n",
       "**Generate the dataframe**. Here's how you can create this dataset in one go:\n",
       "\n",
       "<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\n",
       "  ```python\n",
       "  data = [\n",
       "    [\"Hi there, how do I reset my password?\",\n",
       "     \"To reset your password, click on 'Forgot Password' on the login page and follow the instructions sent to your registered email.\",\n",
       "     \"To change your password, select 'Forgot Password' on the login screen and follow the steps sent to your registered email address. If you don't receive the email, check your spam folder or contact support for assistance.\",\n",
       "     \"incorrect\", \"adds new information (contact support)\"],\n",
       "  \n",
       "    [\"Where can I find my transaction history?\",\n",
       "     \"You can view your transaction history by logging into your account and navigating to the 'Transaction History' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\n",
       "     \"Log into your account and go to 'Transaction History' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How do I add another user to my account?\",\n",
       "     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\n",
       "     \"To add a secondary user, go to 'Account Settings', select 'Manage Users', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\n",
       "     \"incorrect\", \"contradiction (incorrect answer)\"],\n",
       "  \n",
       "    [\"Is it possible to link multiple bank accounts?\",\n",
       "     \"Yes, you can link multiple bank accounts by going to 'Account Settings' in the menu and selecting 'Add Bank Account'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\n",
       "     \"You can add multiple bank accounts by visiting 'Accounts' in the menu and choosing 'Add Bank Account'. Enter your bank details as prompted and complete the verification process for each account to link them successfully.\",\n",
       "     \"incorrect\", \"contradiction (incorrect menu item)\"],\n",
       "  \n",
       "    [\"Can I use your service for cryptocurrency transactions?\",\n",
       "     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\n",
       "     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"Hi, can I get a detailed report of my monthly transactions?\",\n",
       "     \"Yes, you can generate a detailed monthly report of your transactions by logging into your account, going to 'Transaction History', and selecting 'Generate Report'. You can customize the report by selecting specific dates or transaction types.\",\n",
       "     \"You can get a detailed monthly report by logging into your account, navigating to 'Transaction History', and clicking on 'Generate Report'. Customize your report by choosing the date range and types of transactions you want to include.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"I am traveling to the US. Can I use the app there?\",\n",
       "     \"Yes, you can use the app in the US just like you do at home. Ensure you have an internet connection. You may also want to update your app to the latest version before traveling for optimal performance.\",\n",
       "     \"The app will work in the US without any issues. Just make sure you have access to the internet. For the best experience, update your app to the latest version before you travel.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How do I link my payment account to a new mobile number?\",\n",
       "     \"To link a new mobile number, log in to your account, go to 'Account Settings', select 'Mobile Number', and follow the instructions to verify your new number. You will need to enter the new number and verify it via a code sent to your phone.\",\n",
       "     \"To add a new number, navigate to the 'Account Settings' section, select 'Mobile Number' and proceed with the steps to add and confirm the new number. Enter the new mobile number and verify it using the code sent to your phone.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"Can I receive notifications for transactions in real-time?\",\n",
       "     \"Yes, you can enable real-time notifications for transactions by going to 'Account Settings', then 'Notifications', and turning on 'Transaction Alerts'. You can choose to receive alerts via SMS, email, or push notifications on your mobile device.\",\n",
       "     \"To receive real-time notifications for transactions, log into your account, go to 'Account Settings', select 'Notifications', and enable 'Transaction Alerts'. Choose your preferred notification method between email or push notifications.\",\n",
       "     \"incorrect\", \"omits information (sms notification)\"],\n",
       "  \n",
       "    [\"Hey, can I set up automatic transfers to my savings account?\",\n",
       "     \"Yes, you can set up automatic transfers by going to 'Account Settings', selecting 'Automatic Transfers', and specifying the amount and frequency. You can choose to transfer weekly, bi-weekly, or monthly. Make sure to save the settings to activate the transfers.\",\n",
       "     \"You can arrange automatic transfers by going to 'Account Settings', choosing 'Automatic Transfers', and setting the desired amount and frequency. Don't forget to save the changes to enable the automatic transfers.\",\n",
       "     \"incorrect\", \"omits information (limited frequency of transfers available)\"],\n",
       "  \n",
       "    [\"Hi there, how do I reset my password?\",\n",
       "     \"To reset your password, click on 'Forgot Password' on the login page and follow the instructions sent to your registered email.\",\n",
       "     \"To change your password, select 'Forgot Password' on the login screen and follow the steps sent to your registered email address. If you don't receive the email, check your spam folder.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How can I update my billing address?\",\n",
       "     \"To update your billing address, log into your account, go to 'Account Settings', select 'Billing Information', and enter your new address. Make sure to save the changes once you are done.\",\n",
       "     \"To update your billing address, log into your account, navigate to 'Account Settings', and select 'Billing Information'. Enter your new address and ensure all fields are filled out correctly. Save the changes, and you will receive a confirmation email with the updated address details.\",\n",
       "     \"incorrect\", \"adds new information (confirmation email)\"],\n",
       "  \n",
       "    [\"How do I contact customer support?\",\n",
       "     \"You can contact customer support by logging into your account, going to the 'Help' section, and selecting 'Contact Us'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\n",
       "     \"To contact customer support, log into your account and go to the 'Help' section. Select 'Contact Us' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\n",
       "     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\n",
       "  \n",
       "    [\"What should I do if my card is lost or stolen?\",\n",
       "     \"If your card is lost or stolen, immediately log into your account, go to 'Card Management', and select 'Report Lost/Stolen'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\n",
       "     \"If your card is lost or stolen, navigate to 'Card Management' in your account, and select 'Report Lost/Stolen'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\n",
       "     \"correct\", \"\"],\n",
       "  \n",
       "    [\"How do I enable two-factor authentication (2FA)?\",\n",
       "     \"To enable two-factor authentication, log into your account, go to 'Security Settings', and select 'Enable 2FA'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\n",
       "     \"To enable two-factor authentication, log into your account, navigate to 'Security Settings', and choose 'Enable 2FA'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\n",
       "     \"incorrect\", \"adds new information (backup codes)\"]\n",
       "  ]\n",
       "  \n",
       "  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\n",
       "  \n",
       "  golden_dataset = pd.DataFrame(data, columns=columns)\n",
       "  ```\n",
       "</Accordion>\n",
       "\n",
       "<Note>\n",
       "  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\n",
       "</Note>\n",
       "\n",
       "**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\n",
       "\n",
       "```python\n",
       "definition = DataDefinition(\n",
       "    text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
       "    categorical_columns=[\"label\"]\n",
       "    )\n",
       "\n",
       "eval_dataset = Dataset.from_pandas(\n",
       "    golden_dataset,\n",
       "    data_definition=definition)\n",
       "```\n",
       "\n",
       "To preview the dataset:\n",
       "\n",
       "```python\n",
       "pd.set_option('display.max_colwidth', None)\n",
       "golden_dataset.head(5)\n",
       "```\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_data_preview-min.png)\n",
       "\n",
       "Here's the distribution of examples: we have both correct and incorrect responses.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\n",
       "\n",
       "<Accordion title=\"How to preview\" defaultOpen={false}>\n",
       "  Run this to preview the distribution of the column.\n",
       "\n",
       "  ```python\n",
       "  report = Report([\n",
       "    ValueStats(column=\"label\")\n",
       "  ])\n",
       "  \n",
       "  my_eval = report.run(eval_dataset, None)\n",
       "  my_eval\n",
       "  \n",
       "  # my_eval.dict()\n",
       "  # my_eval.json()\n",
       "  ```\n",
       "</Accordion>\n",
       "\n",
       "## 3. Correctness evaluator\n",
       "\n",
       "Now it's time to set up an LLM judge! We'll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\n",
       "\n",
       "**Configure the evaluator prompt**. We'll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here's how to define the prompt template for correctness:\n",
       "\n",
       "```python\n",
       "correctness = BinaryClassificationPromptTemplate(\n",
       "        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\n",
       "        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\n",
       "        REFERENCE:\n",
       "        =====\n",
       "        {target_response}\n",
       "        =====\"\"\",\n",
       "        target_category=\"incorrect\",\n",
       "        non_target_category=\"correct\",\n",
       "        uncertainty=\"unknown\",\n",
       "        include_reasoning=True,\n",
       "        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\n",
       "        )\n",
       "```\n",
       "\n",
       "<Info>\n",
       "  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don't need to ask for these details explicitly, or worry about parsing the output structure — that's built into the template. You only need to add the criteria. You can also use a multi-class template.\n",
       "</Info>\n",
       "\n",
       "In this example, we've set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\n",
       "\n",
       "**Score your data**. To add this new descriptor to your dataset, run:\n",
       "\n",
       "```python\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    LLMEval(\"new_response\",\n",
       "            template=correctness,\n",
       "            provider = \"openai\",\n",
       "            model = \"gpt-4o-mini\",\n",
       "            alias=\"Correctness\",\n",
       "            additional_columns={\"target_response\": \"target_response\"}),\n",
       "    ])\n",
       "```\n",
       "\n",
       "**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\n",
       "\n",
       "```python\n",
       "eval_dataset.as_dataframe()\n",
       "```\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\n",
       "\n",
       "<Info>\n",
       "  **Note**: your explanations will vary since LLMs are non-deterministic.\n",
       "</Info>\n",
       "\n",
       "If you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\n",
       "\n",
       "```python\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\n",
       "```\n",
       "\n",
       "**Get a Report.** Summarize the result by generating an Evidently Report.\n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    TextEvals()\n",
       "])\n",
       "\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "my_eval\n",
       "```\n",
       "\n",
       "This will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_report-min.png)\n",
       "\n",
       "Since we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\n",
       "\n",
       "## 4. Evaluate the LLM Eval quality\n",
       "\n",
       "This part is a bit meta: we're going to evaluate the quality of our LLM evaluator itself\\! We can treat it as a simple **binary classification** problem.\n",
       "\n",
       "**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\n",
       "\n",
       "```python\n",
       "df=eval_dataset.as_dataframe()\n",
       "\n",
       "definition_2 = DataDefinition(\n",
       "    classification=[BinaryClassification(\n",
       "        target=\"label\",\n",
       "        prediction_labels=\"Correctness\",\n",
       "        pos_label = \"incorrect\")],\n",
       "    categorical_columns=[\"label\", \"Correctness\"])\n",
       "\n",
       "class_dataset = Dataset.from_pandas(\n",
       "    pd.DataFrame(df),\n",
       "    data_definition=definition_2)\n",
       "```\n",
       "\n",
       "<Info>\n",
       "  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\n",
       "</Info>\n",
       "\n",
       "**Get a Report**. Let's use a`ClassificationPreset()` that combines several classification metrics:\n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    ClassificationPreset()\n",
       "])\n",
       "\n",
       "my_eval = report.run(class_dataset, None)\n",
       "my_eval\n",
       "\n",
       "# or my_eval.as_dict()\n",
       "```\n",
       "\n",
       "We can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\! You can also refine the prompt to try to improve them.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\n",
       "\n",
       "## 5. Verbosity evaluator\n",
       "\n",
       "Next, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\n",
       "\n",
       "Here's how to set up the prompt template for verbosity:\n",
       "\n",
       "```python\n",
       "verbosity = BinaryClassificationPromptTemplate(\n",
       "        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\n",
       "            A concise response should:\n",
       "            - Provide the necessary information without unnecessary details or repetition.\n",
       "            - Be brief yet comprehensive enough to address the query.\n",
       "            - Use simple and direct language to convey the message effectively.\"\"\",\n",
       "        target_category=\"concise\",\n",
       "        non_target_category=\"verbose\",\n",
       "        uncertainty=\"unknown\",\n",
       "        include_reasoning=True,\n",
       "        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\n",
       "        )\n",
       "```\n",
       "\n",
       "Add this new descriptor to our existing dataset:\n",
       "\n",
       "```python\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    LLMEval(\"new_response\",\n",
       "            template=verbosity,\n",
       "            provider = \"openai\",\n",
       "            model = \"gpt-4o-mini\",\n",
       "            alias=\"Verbosity\")\n",
       "    ])\n",
       "```\n",
       "\n",
       "Run the Report and view the summary results: \n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    TextEvals()\n",
       "])\n",
       "\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "my_eval\n",
       "```\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_verbosity-min.png)\n",
       "\n",
       "You can also view the dataframe using `eval_dataset.as_dataframe()`\n",
       "\n",
       "<Info>\n",
       "  Don't fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you've got your golden dataset\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\n",
       "</Info>\n",
       "\n",
       "## What's next?\n",
       "\n",
       "The LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\n",
       "\n",
       "To be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\n",
       "\n",
       "### Set up Evidently Cloud\n",
       "\n",
       "<CloudSignup />\n",
       "\n",
       "Import the components to connect with Evidently Cloud:\n",
       "\n",
       "```python\n",
       "from evidently.ui.workspace import CloudWorkspace\n",
       "```\n",
       "\n",
       "### Create a Project\n",
       "\n",
       "<CreateProject />\n",
       "\n",
       "### Send your eval\n",
       "\n",
       "Since you already created the eval, you can simply upload it to the Evidently Cloud.\n",
       "\n",
       "```python\n",
       "ws.add_run(project.id, my_eval, include_data=True)\n",
       "```\n",
       "\n",
       "You can then go to the Evidently Cloud, open your Project and explore the Report.\n",
       "\n",
       "![](/images/examples/llm_judge_tutorial_cloud-min.png)\n",
       "\n",
       "<Info>\n",
       "  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\n",
       "</Info>\n",
       "\n",
       "# Reference documentation\n",
       "\n",
       "See this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here's a complete example for evaluating responses using an LLM as a judge, which includes generating reports for both correctness and verbosity. The following code snippet utilizes the Evidently library and assumes you have access to an OpenAI API key.</p>\n",
       "<h3>Complete Example: LLM as a Judge</h3>\n",
       "<pre><code class=\"language-python\"># Step 1: Install Evidently\n",
       "!pip install evidently\n",
       "\n",
       "# Step 2: Import Required Libraries\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import os\n",
       "from evidently import Dataset, DataDefinition, Report\n",
       "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
       "from evidently.metrics import *\n",
       "from evidently.descriptors import *\n",
       "\n",
       "# Step 3: Set Environment Variable for OpenAI API Key\n",
       "os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR_OPENAI_API_KEY&quot;\n",
       "\n",
       "# Step 4: Create an Evaluation Dataset (Toy Q&amp;A)\n",
       "data = [\n",
       "    [&quot;Hi there, how do I reset my password?&quot;,\n",
       "     &quot;To reset your password, click on 'Forgot Password' on the login page and follow the instructions sent to your registered email.&quot;,\n",
       "     &quot;To change your password, select 'Forgot Password' on the login screen and follow the steps sent to your registered email address. If you don't receive the email, check your spam folder.&quot;,\n",
       "     &quot;correct&quot;, &quot;correct response&quot;],\n",
       "    \n",
       "    [&quot;Where can I find my transaction history?&quot;,\n",
       "     &quot;You can view your transaction history by logging into your account and navigating to the 'Transaction History' section.&quot;,\n",
       "     &quot;Logging into your account will allow you to view your transaction history.&quot;,\n",
       "     &quot;incorrect&quot;, &quot;incorrect response&quot;],\n",
       "    \n",
       "    # Add more rows for more questions\n",
       "]\n",
       "\n",
       "columns = [&quot;question&quot;, &quot;target_response&quot;, &quot;new_response&quot;, &quot;label&quot;, &quot;comment&quot;]\n",
       "golden_dataset = pd.DataFrame(data, columns=columns)\n",
       "\n",
       "# Step 5: Define Dataset Structure\n",
       "definition = DataDefinition(\n",
       "    text_columns=[&quot;question&quot;, &quot;target_response&quot;, &quot;new_response&quot;],\n",
       "    categorical_columns=[&quot;label&quot;]\n",
       ")\n",
       "\n",
       "eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
       "\n",
       "# Step 6: Configure the Correctness Evaluator Prompt\n",
       "correctness = BinaryClassificationPromptTemplate(\n",
       "    criteria=&quot;&quot;&quot;An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\n",
       "    The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\n",
       "    REFERENCE:\n",
       "    =====\n",
       "    {target_response}\n",
       "    =====&quot;&quot;&quot;,\n",
       "    target_category=&quot;incorrect&quot;,\n",
       "    non_target_category=&quot;correct&quot;,\n",
       "    uncertainty=&quot;unknown&quot;,\n",
       "    include_reasoning=True,\n",
       "    pre_messages=[(&quot;system&quot;, &quot;You are an expert evaluator. You will be given an ANSWER and REFERENCE&quot;)],\n",
       ")\n",
       "\n",
       "# Step 7: Score the Data for Correctness\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    LLMEval(&quot;new_response&quot;,\n",
       "            template=correctness,\n",
       "            provider=&quot;openai&quot;,\n",
       "            model=&quot;gpt-4&quot;,\n",
       "            alias=&quot;Correctness&quot;,\n",
       "            additional_columns={&quot;target_response&quot;: &quot;target_response&quot;}),\n",
       "])\n",
       "\n",
       "# Step 8: Generate and View the Report\n",
       "report = Report([TextEvals()])\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "print(my_eval.as_dict())  # Output the report as a dictionary\n",
       "\n",
       "# Step 9: Configure the Verbosity Evaluator Prompt\n",
       "verbosity = BinaryClassificationPromptTemplate(\n",
       "    criteria=&quot;&quot;&quot;Conciseness refers to the quality of being brief and to the point while still providing all necessary information.\n",
       "    A concise response should provide necessary information without unnecessary details or repetition.&quot;&quot;&quot;,\n",
       "    target_category=&quot;concise&quot;,\n",
       "    non_target_category=&quot;verbose&quot;,\n",
       "    uncertainty=&quot;unknown&quot;,\n",
       "    include_reasoning=True,\n",
       "    pre_messages=[(&quot;system&quot;, &quot;You are an expert text evaluator.&quot;)]\n",
       ")\n",
       "\n",
       "# Step 10: Score the Data for Verbosity\n",
       "eval_dataset.add_descriptors(descriptors=[\n",
       "    LLMEval(&quot;new_response&quot;,\n",
       "            template=verbosity,\n",
       "            provider=&quot;openai&quot;,\n",
       "            model=&quot;gpt-4&quot;,\n",
       "            alias=&quot;Verbosity&quot;)\n",
       "])\n",
       "\n",
       "# Step 11: Generate and View the Verbosity Report\n",
       "verbosity_report = Report([TextEvals()])\n",
       "verbosity_eval = verbosity_report.run(eval_dataset, None)\n",
       "print(verbosity_eval.as_dict())\n",
       "</code></pre>\n",
       "<h3>Explanation of Key Steps</h3>\n",
       "<ol>\n",
       "<li><strong>Installation and Setup</strong>: Install the Evidently library and import necessary packages. Set up your OpenAI API key.</li>\n",
       "<li><strong>Create the Dataset</strong>: Construct a toy Q&amp;A dataset with columns for questions, target responses, new user-generated responses, and their labels.</li>\n",
       "<li><strong>Define Data Structure</strong>: Use <code>DataDefinition</code> to map your columns and set them accordingly for evaluation.</li>\n",
       "<li><strong>Configure the Judge</strong>: Set up the prompt that the LLM will use for both correctness and verbosity evaluations.</li>\n",
       "<li><strong>Adding Descriptors</strong>: Integrate LLM evaluations into your dataset for both correctness and verbosity.</li>\n",
       "<li><strong>Generate Reports</strong>: Create reports to evaluate the results from the LLM assessments.</li>\n",
       "</ol>\n",
       "<h3>Output</h3>\n",
       "<p>The script will output the evaluation results for both correctness and verbosity in dictionary format, allowing for easy access and understanding of the results.</p>\n",
       "<p>For further reading on LLM judges and detailed documentation, please refer to the following sources:</p>\n",
       "<ul>\n",
       "<li>Complete documentation on LLM judges: <a href=\"https://github.com/evidentlyai/docs/blob/main/metrics/customize_llm_judge\">LLM Judges Docs</a>.</li>\n",
       "<li>To view a live example, check the tutorial notebook: <a href=\"https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb\">Jupyter Notebook Example</a>.</li>\n",
       "</ul>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: what are drift thresholds and how do I configure them?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"drift thresholds configure\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"drift thresholds configure\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'start': 3000, 'content': 'cept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n</Info>\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **Two datasets**. You must always pass both: the current one will be compared to the reference.\\n\\n* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\\n</Info>\\n\\n## Report customization\\n\\nYou have multiple customization options.\\n\\n**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'metrics/preset_data_drift.mdx'}, {'start': 4000, 'content': 'lumns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\\n\\n**Change drift parameters.** You can modify how drift detection works:\\n\\n* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\\n\\n* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\\n\\n* **Implement a custom method**. You can implement a custom drift method as Python function.\\n\\n<Info>\\n  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\\n</Info>\\n\\n**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\\n\\n* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\"prediction\")` to your Report so that you see the drift in this value in a separate widget.\\n\\n* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\\n\\n* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\\n\\n<Info>\\n  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\\n</Info>', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'metrics/preset_data_drift.mdx'}, {'start': 7000, 'content': 'ch as articles.\\n\\n<Tip>\\n  **Text descriptors drift**. If you work with raw text data, you can also check for distribution drift in text descriptors (such as text length, etc.) To use this method, first compute the selected [text descriptors](/docs/library/descriptors). Then, use numerical / categorical drift detection methods as usual.\\n</Tip>\\n\\n\\n## Resources\\n\\nTo build up a better intuition for which tests are better in different kinds of use cases, you can read our in-depth blogs with experimental code:\\n\\n* [Which test is the best? We compared 5 methods to detect data drift on large datasets](https://evidentlyai.com/blog/data-drift-detection-large-datasets).\\n\\n* [Shift happens: how to detect drift in ML embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\\n\\nAdditional links:\\n\\n* [How to interpret data and prediction drift together?](https://evidentlyai.com/blog/data-and-prediction-drift)\\n\\n* [Do I need to monitor data drift if I can measure the ML model quality?](https://evidentlyai.com/blog/ml-monitoring-do-i-need-data-drift)\\n\\n* [\"My data drifted. What\\'s next?\" How to handle ML model drift in production.](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\\n\\n* [What is the difference between outlier detection and data drift detection?](https://evidentlyai.com/blog/ml-monitoring-drift-detection-vs-outlier-detection)', 'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'metrics/explainer_drift.mdx'}, {'start': 1000, 'content': 'Target value, it will be evaluated together with other columns.\\n\\n* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\\n\\nThe table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\\n\\n![](/images/metrics/preset_data_drift-min.png)\\n\\nIf you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_data_drift-min.png)\\n\\n<Info>\\n  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\\n</Info>\\n\\n## Use case\\n\\nYou can evaluate data drift in different scenarios.\\n\\n* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\\n\\n* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\\n\\n* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\\n\\n* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\\n\\n<Info>\\n  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Con', 'title': 'Data Drift', 'description': 'Overview of the Data Drift Preset.', 'filename': 'metrics/preset_data_drift.mdx'}, {'start': 0, 'content': 'In some tests and metrics, Evidently uses the default Data Drift Detection algorithm. It helps detect the distribution drift in the individual columns (features, prediction, or target). This page describes how the **default** algorithm works.\\n\\nThis applies to: `DataDriftPreset`, `ValueDrift`, `DriftedColumnsCount`.\\n\\n<Info>\\n  This is an explainer page. For API reference, check the guide on [setting data drift parameters](/metrics/customize_data_drift).\\n</Info>\\n\\n## How it works\\n\\nEvidently compares the distributions of the values in a given column (or columns) of the two datasets. You should pass these datasets as **reference** and **current**. Evidently applies several statistical tests and drift detection methods to detect if the distribution has changed significantly. It returns a \"drift detected\" or \"not detected\" result.\\n\\nThere is a default logic to choosing the appropriate drift test for each column. It is based on:\\n\\n* column type: categorical, numerical, text data \\n\\n* the number of observations in the reference dataset\\n\\n* the number of unique values in the column (n\\\\_unique)\\n\\nOn top of this, you can set a rule to detect dataset-level drift based on the number of columns that are drifted.\\n\\n## Data requirements\\n\\n**Two datasets**. You always need to pass two datasets: current (dataset evaluated for drift) and reference (dataset that serves as a benchmark).\\n\\n**Non-empty columns**. To evaluate data or prediction drift in the dataset, you need to ensure that the columns you test for drift are not empty. If these columns are empty in either reference or current data, Evidently will not calculate distribution drift and will raise an error.\\n\\n**Empty values.** If some columns contain empty or infinite values (+-np.inf), these values will be filtered out when calculating distribution drift in the corresponding column.\\n\\n<Note>\\n  By default, drift tests do **not** react to changes or increases in the number of empty values. Since the high number of nulls can be an important ', 'title': 'Data drift', 'description': 'How data drift detection works', 'filename': 'metrics/explainer_drift.mdx'}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>read_file(\"{\\\"filename\\\":\\\"metrics/preset_data_drift.mdx\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"filename\\\":\\\"metrics/preset_data_drift.mdx\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>**Pre-requisites**:\n",
       "* You know how to use [Data Definition ](/docs/library/data_definition)to prepare the data.\n",
       "* You know how to create [Reports](/docs/library/report).\n",
       "  \n",
       "**Report.** To run a Preset on your data, comparing `current` data to `ref` data:\n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    DataDriftPreset(),\n",
       "])\n",
       "\n",
       "my_eval = report.run(current, ref)\n",
       "```\n",
       "\n",
       "**Test Suite.** To add Tests with explicit pass/fail for each column:\n",
       "\n",
       "```python\n",
       "report = Report([\n",
       "    DataDriftPreset(),\n",
       "],\n",
       "include_tests=True)\n",
       "\n",
       "my_eval = report.run(current, ref)\n",
       "```\n",
       "\n",
       "## Overview\n",
       "\n",
       "The`DataDriftPreset` lets you evaluate shift in data distribution between the two datasets to detect if there are significant changes.&#x20;\n",
       "\n",
       "![](/images/metrics/preset_data_drift_2-min.png)\n",
       "\n",
       "* **Column drift.** Checks for shifts in each column. The [drift detection method](/metrics/explainer_drift) is chosen automatically based on the column type and number of observations.\n",
       "\n",
       "* **Target / Prediction Drift**. If you dataset includes Prediction or Target value, it will be evaluated together with other columns.\n",
       "\n",
       "* **Overall dataset drift.** Returns the share of drifting columns in the Dataset. By default, Dataset Drift is detected if at least 50% of columns drift.\n",
       "\n",
       "The table shows the drifting columns first. You can also choose to sort the rows by the feature name or type, and open up individual columns to see distribution details.\n",
       "\n",
       "![](/images/metrics/preset_data_drift-min.png)\n",
       "\n",
       "If you choose to enable Tests, you will get an additional Test Suite view:\n",
       "\n",
       "![](/images/metrics/test_preset_data_drift-min.png)\n",
       "\n",
       "<Info>\n",
       "  **Data Drift Explainer.** Read about [Data Drift Methods](/metrics/explainer_drift) and default algorithm.\n",
       "</Info>\n",
       "\n",
       "## Use case\n",
       "\n",
       "You can evaluate data drift in different scenarios.\n",
       "\n",
       "* **To monitor the ML model performance without ground truth.** When you do not have true labels or actuals, you can monitor **feature drift** and **prediction drift** to check if the model still operates in a familiar environment. These are proxy metrics. If you detect drift in features or prediction, you can trigger labelling and retraining, or decide to pause and switch to a different decision method.\n",
       "\n",
       "* **When you are debugging the ML model quality decay.** If you observe a drop in the model quality, you can evaluate Data Drift to explore the change in the feature patterns, e.g., to understand the change in the environment or discover the appearance of a new segment.\n",
       "\n",
       "* **To understand model drift in an offline environment.** You can explore the historical data drift to understand past changes and define the optimal drift detection approach and retraining strategy.\n",
       "\n",
       "* **To decide on the model retraining.** Before feeding fresh data into the model, you might want to verify whether it even makes sense. If there is no data drift, the environment is stable, and retraining might not be necessary.\n",
       "\n",
       "<Info>\n",
       "  For conceptual explanation, read about [Data Drift](https://www.evidentlyai.com/ml-in-production/data-drift) and [Concept Drift](https://www.evidentlyai.com/ml-in-production/concept-drift). To build intuition about different drift detection methods, check these research blogs: [numerical](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) data, [embeddings](https://www.evidentlyai.com/blog/embedding-drift-detection).\n",
       "</Info>\n",
       "\n",
       "## Data requirements\n",
       "\n",
       "* **Input columns**. You can provide any input columns. They must be non-empty.\n",
       "\n",
       "* **Two datasets**. You must always pass both: the current one will be compared to the reference.\n",
       "\n",
       "* (Optional) **Set column types**. The Preset evaluates drift for numerical, categorical, or text data. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical and categorical features. You must always map text data.\n",
       "\n",
       "<Info>\n",
       "  **Data schema mapping**. Use the [data definition](/docs/library/data_definition) to map your input data.\n",
       "</Info>\n",
       "\n",
       "## Report customization\n",
       "\n",
       "You have multiple customization options.\n",
       "\n",
       "**Select columns**. You can apply Drift Detection only to some columns in the Dataset, for example, only to the important features. Use the `columns` parameter.\n",
       "\n",
       "**Change drift parameters.** You can modify how drift detection works:\n",
       "\n",
       "* **Change methods**. Evidently has a large number of drift detection methods, including PSI, K-L divergence, Jensen-Shannon distance, Wasserstein distance, etc. You can also pick tests by column.\n",
       "\n",
       "* **Change thresholds**. You can specify different drift detection conditions on the dataset or column level.\n",
       "\n",
       "* **Implement a custom method**. You can implement a custom drift method as Python function.\n",
       "\n",
       "<Info>\n",
       "  **Drift detection parameters**. Learn available methods and parameters in [Drift Customization. ](/metrics/customize_data_drift).\n",
       "</Info>\n",
       "\n",
       "**Modify Report composition**. You can add other Metrics to the Report to get a more comprehensive evaluation. Here are some recommended options.\n",
       "\n",
       "* **Single out the Target/Prediction column.** If you want to evaluate drift in the Prediction column separately, you can add `ValueDrift(\"prediction\")` to your Report so that you see the drift in this value in a separate widget.\n",
       "\n",
       "* **Add data quality checks**. Add `DataSummaryPreset` to get descriptive stats and run Tests like detecting missing values. Data drift check drops nulls (and compares the distributions of non-empty features), so you may want to run these Tests separately.\n",
       "\n",
       "* **Check for correlation changes**. You can also consider adding checks on changes in correlations between the features.\n",
       "\n",
       "<Info>\n",
       "  **Custom Report**. Check how to create a [Report](/docs/library/report) and add [Tests](/docs/library/tests) conditions.\n",
       "</Info></pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Drift thresholds are parameters used in drift detection methodologies to determine whether a significant change in data distribution has occurred between two datasets—typically a reference dataset and a current dataset. Configuring these thresholds properly is crucial for effectively monitoring your machine learning models and ensuring they maintain their performance over time.</p>\n",
       "<h3>Overview of Drift Thresholds</h3>\n",
       "<ol>\n",
       "<li><p><strong>Purpose</strong>: Drift thresholds help to decide what constitutes a &quot;drift&quot; in the data by specifying acceptable levels of change. If the change exceeds this threshold, it indicates that the data distribution has changed significantly enough to warrant attention or action.</p>\n",
       "</li>\n",
       "<li><p><strong>Types of Drifts</strong>: Drift thresholds can be configured for various dimensions:</p>\n",
       "<ul>\n",
       "<li><strong>Column-level Drift</strong>: Assessing changes in individual feature distributions.</li>\n",
       "<li><strong>Overall Dataset Drift</strong>: Evaluating the percentage of columns that exhibit drift.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Drift Detection Methods</strong>: Different interpretations of drift can be measured using methods like:</p>\n",
       "<ul>\n",
       "<li>Population Stability Index (PSI)</li>\n",
       "<li>Kullback-Leibler divergence</li>\n",
       "<li>Jensen-Shannon distance</li>\n",
       "<li>Wasserstein distance</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Configuring Drift Thresholds</h3>\n",
       "<p>To configure drift thresholds in an Evidently report, you can follow these steps:</p>\n",
       "<ol>\n",
       "<li><p><strong>Set Up Your Data</strong>: You must have two datasets—current and reference—ready to compare.</p>\n",
       "<pre><code class=\"language-python\">from evidently import Dataset, Report\n",
       "current = ... # Your current dataset\n",
       "ref = ...     # Your reference dataset\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create a Report</strong>: Use the <code>DataDriftPreset</code> to compare the datasets and set your drift detection thresholds.</p>\n",
       "<pre><code class=\"language-python\">from evidently.metrics import DataDriftPreset\n",
       "\n",
       "report = Report([\n",
       "    DataDriftPreset(),\n",
       "])\n",
       "\n",
       "my_eval = report.run(current, ref)  # Run your evaluation\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Customize Drift Detection</strong>: Modify the parameters for drift detection. This includes specifying different methods and thresholds at the column or dataset level.</p>\n",
       "<pre><code class=\"language-python\">report = Report([\n",
       "    DataDriftPreset(),\n",
       "], include_tests=True)  # Include tests to evaluate pass/fail for each column\n",
       "\n",
       "my_eval = report.run(current, ref)  # Evaluate\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Change Drift Parameters</strong>: You can set specific thresholds or select enhanced drift detection methods.</p>\n",
       "<pre><code class=\"language-python\"># Specify different drift thresholds\n",
       "from evidently.metrics import DataDrift\n",
       "drift_detection = DataDrift(current, ref, thresholds=your_thresholds)  # Pass your thresholds\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Evaluate and Interpret Results</strong>: Analyze the generated report to see which features exhibit drift and to determine what actions may need to be taken adjust to the changes.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Example Configuration of Drift Thresholds</h3>\n",
       "<p>Here's how to incorporate settings for drift thresholds into your report evaluation:</p>\n",
       "<pre><code class=\"language-python\">report = Report([\n",
       "    DataDriftPreset()\n",
       "],\n",
       "include_tests=True)  # This will provide explicit pass/fail for each column based on thresholds\n",
       "\n",
       "# Run the drift detection\n",
       "my_eval = report.run(current_data, reference_data)\n",
       "\n",
       "# Access results\n",
       "print(my_eval.as_dict())  # Examine results for drift detection\n",
       "</code></pre>\n",
       "<h3>Resources and Further Reading</h3>\n",
       "<ul>\n",
       "<li>For more details on how to set data drift parameters and understanding drift methods, refer to the <a href=\"https://docs.evidentlyai.com/metrics/customize_data_drift\">Evidently Drift Customization documentation</a>.</li>\n",
       "<li>To understand the underlying concepts and best practices in monitoring data drift, check <a href=\"https://evidentlyai.com/ml-in-production/data-drift\">this guide on Data Drift</a>.</li>\n",
       "</ul>\n",
       "<p>By appropriately configuring drift thresholds, you can effectively monitor changes in your data and respond proactively to ensure the reliability of your machine learning models.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb90e9-6486-4c5c-a7fe-0db53a1de4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
