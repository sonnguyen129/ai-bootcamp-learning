{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d98d9a",
   "metadata": {},
   "source": [
    "# Writing Your First Agent Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182953a2",
   "metadata": {},
   "source": [
    "So far we tested our agent by doing \"vide checks\": we run it, look at the output, and tweak until we're satisfied.\n",
    "\n",
    "In this lesson we'll make it more reliable by adding proper tests.\n",
    "\n",
    "In the previous lesson, we created our Python project. We implemented a class with tools and created our agent.\n",
    "\n",
    "That's main.py we have so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import search_agent\n",
    "import asyncio\n",
    "\n",
    "agent = search_agent.create_agent()\n",
    "agent_callback = search_agent.NamedCallback(agent)\n",
    "\n",
    "\n",
    "async def run_agent(user_prompt: str):\n",
    "\n",
    "    results = await agent.run(\n",
    "        user_prompt=user_prompt,\n",
    "        event_stream_handler=agent_callback\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "result = asyncio.run(run_agent(\"What is LLM evaluation?\"))\n",
    "\n",
    "print(result.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db04383",
   "metadata": {},
   "source": [
    "## Adding Basic Tool Cal Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d48f8",
   "metadata": {},
   "source": [
    "We want the agent to invoke the search tool. Let's add a check to ensure that it happens.\n",
    "\n",
    "First, we write a function that extract tool calls from messages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    name: str\n",
    "    args: dict\n",
    "\n",
    "\n",
    "def get_tool_calls(result) -> list[ToolCall]:\n",
    "    calls = []\n",
    "\n",
    "    for m in result.new_messages():\n",
    "        for p in m.parts:\n",
    "            kind = p.part_kind\n",
    "            if kind == 'tool-call': \n",
    "                call = ToolCall(\n",
    "                    name=p.tool_name,\n",
    "                    args=json.loads(p.args)\n",
    "                )\n",
    "                calls.append(call)\n",
    "\n",
    "    return calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1e9e6",
   "metadata": {},
   "source": [
    "Run the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = asyncio.run(run_agent(\"What is LLM evaluation?\"))\n",
    "\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc60f6",
   "metadata": {},
   "source": [
    "Now we can do the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = get_tool_calls(result)\n",
    "assert len(tool_calls) > 0, \"No tool calls found\"\n",
    "\n",
    "print(\"TOOL CALLS:\", tool_calls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc6f7b",
   "metadata": {},
   "source": [
    "Here we use assert to test that at least one tool call was made, ensuring our agent is actively using its search functionality.\n",
    "\n",
    "You can run this to verify it works (or it doesn't work).\n",
    "\n",
    "However, this approach isn't how we should write tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61689ebe",
   "metadata": {},
   "source": [
    "## Setting Up Proper Testing Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce92d57",
   "metadata": {},
   "source": [
    "We want to split the production code - in our case, main.py, - from tests. Our main.py shouldn't be a testing ground.\n",
    "\n",
    "Let's take the testing logic out and put it inside a proper test.\n",
    "\n",
    "We'll use pytest for that - a Python framework for testing.\n",
    "\n",
    "Install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60970ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add --dev pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b7a1d",
   "metadata": {},
   "source": [
    "Note that we add it as a dev dependency. Later when others install our project, they'll use uv sync --dev to include these dependencies too (if needed).\n",
    "\n",
    "Run pytest to see the current state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737188c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf220b",
   "metadata": {},
   "source": [
    "`Output: collected 0 items`\n",
    "\n",
    "Let's create our test structure.\n",
    "\n",
    "Create tests/ and tests/__init__.py (empty file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tests\n",
    "!touch tests/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14974fcb",
   "metadata": {},
   "source": [
    "Next, create `conftest.py` in the tests directory.\n",
    "\n",
    "This file contains shared configuration and fixtures (data) for all tests. It's always executed by pytest before everything else.\n",
    "\n",
    "We use it to ensure our project modules can be imported properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the project root (parent of tests/) is on sys.path so tests can import project modules\n",
    "project_root = Path(__file__).resolve().parents[1]\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aba044",
   "metadata": {},
   "source": [
    "If we don't do it, we won't be able to import our code properly.\n",
    "\n",
    "We wouldn't need it if we kept all our code in a separate folder (e.g. doc_agent) but we will keep things simpler in this lesson.\n",
    "\n",
    "Now, create `utils.py` (`test/utils.py`) in the tests directory. We separate utility functions into their own module to keep tests clean and easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8623a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    name: str\n",
    "    args: dict\n",
    "\n",
    "\n",
    "def get_tool_calls(result) -> List[ToolCall]:\n",
    "    \"\"\"Extract tool-call parts from an agent result and return them as ToolCall objects.\"\"\"\n",
    "    calls: List[ToolCall] = []\n",
    "\n",
    "    for m in result.new_messages():\n",
    "        for p in m.parts:\n",
    "            kind = p.part_kind\n",
    "            if kind == 'tool-call':\n",
    "                call = ToolCall(\n",
    "                    name=p.tool_name,\n",
    "                    args=json.loads(p.args)\n",
    "                )\n",
    "                calls.append(call)\n",
    "\n",
    "    return calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3029744c",
   "metadata": {},
   "source": [
    "## Refactoring Main Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36d9ca",
   "metadata": {},
   "source": [
    "Now remove the testing code from main.py. We can also add a sync version for easier testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28072f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import search_agent\n",
    "import asyncio\n",
    "\n",
    "\n",
    "agent = search_agent.create_agent()\n",
    "agent_callback = search_agent.NamedCallback(agent)\n",
    "\n",
    "\n",
    "async def run_agent(user_prompt: str):\n",
    "    results = await agent.run(\n",
    "        user_prompt=user_prompt,\n",
    "        event_stream_handler=agent_callback\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_agent_sync(user_prompt: str):\n",
    "    return asyncio.run(run_agent(user_prompt))\n",
    "\n",
    "\n",
    "def main():\n",
    "    result = run_agent_sync(\"LLM as a Judge\")\n",
    "    print(result.output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cfe7f5",
   "metadata": {},
   "source": [
    "## Creating the First Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c63518f",
   "metadata": {},
   "source": [
    "Now create a test in `test_agent.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709541c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import run_agent_sync\n",
    "from tests.utils import get_tool_calls\n",
    "\n",
    "\n",
    "def test_agent_tool_calls_present():\n",
    "    result = run_agent_sync(\"LLM as a Judge\")\n",
    "    print(result.output)\n",
    "\n",
    "    tool_calls = get_tool_calls(result)\n",
    "    assert len(tool_calls) > 0, \"No tool calls found\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43e259",
   "metadata": {},
   "source": [
    "We use the `get_tool_calls` function from `utils.py` here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8789e",
   "metadata": {},
   "source": [
    "## Running Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99257cc8",
   "metadata": {},
   "source": [
    "Let's run this test now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471162c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d5fde",
   "metadata": {},
   "source": [
    "To save us some time, we can also create a `Makefile` so we don't need to type the full command every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4179bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test:\n",
    "\tuv run pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a11f3",
   "metadata": {},
   "source": [
    "Make sure to use tabs, not spaces for indentation in Makefiles. This is a Makefile requirement that can cause errors if not followed.\n",
    "\n",
    "Add `.PHONY: test` at the top of the `Makefile` to indicate that \"test\" is not a file target but a command. This prevents conflicts if someone creates a file named \"test\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558cd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    ".PHONY: test\n",
    "\n",
    "test:\n",
    "\tuv run pytest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f7b16",
   "metadata": {},
   "source": [
    "Execute the test with the command:\n",
    "\n",
    "```bash\n",
    "make test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061d1775",
   "metadata": {},
   "source": [
    "## Fixing Warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d222d",
   "metadata": {},
   "source": [
    "You might see this warning:\n",
    "\n",
    "```\n",
    "DeprecationWarning: Specifying a model name without a provider prefix is deprecated. Instead of 'gpt-4o-mini', use 'openai:gpt-4o-mini'.\n",
    "```\n",
    "\n",
    "Let's fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent():\n",
    "    tools = search_tools.prepare_search_tools()\n",
    "\n",
    "    return Agent(\n",
    "        name=\"search\",\n",
    "        instructions=search_instructions,\n",
    "        tools=[tools.search],\n",
    "        model=\"openai:gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73f4f7",
   "metadata": {},
   "source": [
    "## Viewing Test Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c8097",
   "metadata": {},
   "source": [
    "By default, pytest only shows output from failing tests. If you want to see all test output, run with -s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run pytest -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cd665",
   "metadata": {},
   "source": [
    "For now, let's add this flag. We can remove it later once our tests are stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac320e3b",
   "metadata": {},
   "source": [
    "## Testing Multiple Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da62c12",
   "metadata": {},
   "source": [
    "Now let's examine our test. If it's passing, that's good!\n",
    "\n",
    "Let's make it more specific by requiring at least 3 searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_makes_3_calls():\n",
    "    result = run_agent_sync(\"What is LLM evaluation?\")\n",
    "\n",
    "    tool_calls = get_tool_calls(result)\n",
    "    assert len(tool_calls) >= 3, \"Less than 3 tool calls found\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a62d70",
   "metadata": {},
   "source": [
    "This test will likely fail initially. Let's update our agent instructions to ensure multiple searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You're a helpful assistant that can answer questions by searching the \n",
    "documentation.\n",
    "\n",
    "Make at least 3 searches. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40ab2b",
   "metadata": {},
   "source": [
    "If this still doesn't work reliably, we need a more explicit approach. Here's a ChatGPT-improved prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a helpful assistant that answers questions by searching the documentation.\n",
    "For every user query, you must perform at least 3 separate searches to gather sufficient context and confirm accuracy.\n",
    "Each search should explore a different angle or keyword variation of the user's request.\n",
    "After performing all searches, synthesize the findings into a single, clear, and accurate response.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301d3a3",
   "metadata": {},
   "source": [
    "Now the test should pass, showing output like:\n",
    "\n",
    "```\n",
    "TOOL CALL (search): search({\"query\": \"LLM evaluation definition\"})\n",
    "TOOL CALL (search): search({\"query\": \"how to evaluate LLM models\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM performance metrics\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cfabc",
   "metadata": {},
   "source": [
    "## Testing Citations and References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d4de7",
   "metadata": {},
   "source": [
    "Let's add another test for proper citation format. We want the agent to include references in a specific format.\n",
    "\n",
    "Update the instructions to require proper citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a helpful assistant that answers questions by searching the documentation.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. For every user query, you must perform at least 3 separate searches to gather enough context and verify accuracy.  \n",
    "2. Each search should use a different angle, phrasing, or keyword variation of the user's query.  \n",
    "3. The search results return filenames (e.g., examples/GitHub_actions.mdx).  \n",
    "   When citing sources, convert filenames into full GitHub URLs using the following pattern:  \n",
    "   https://github.com/evidentlyai/docs/blob/main/<filename>  \n",
    "   Example:  \n",
    "   examples/GitHub_actions.mdx → https://github.com/evidentlyai/docs/blob/main/examples/GitHub_actions.mdx  \n",
    "4. After performing all searches, write a concise, accurate answer that synthesizes the findings.  \n",
    "5. At the end of your response, include a \"References\" section listing all the sources you used, one per line, in the format:\n",
    "\n",
    "## References\n",
    "\n",
    "- [Title or Filename](https://github.com/evidentlyai/docs/blob/main/path/to/file.mdx)\n",
    "- ...\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026a09f",
   "metadata": {},
   "source": [
    "Then test that the agent follows the citation format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f936bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"## References\" in output\n",
    "assert \"(https://github.com/evidentlyai/docs/blob/main/\" in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3ad898",
   "metadata": {},
   "source": [
    "## Testing Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018a297",
   "metadata": {},
   "source": [
    "Let's create another test (test_agent_code) for code-related questions:\n",
    "\n",
    "\"How do I implement LLM as a Judge eval?\"\n",
    "\n",
    "We expect to have some Python code blocks there. We can explicitly test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"```python\" in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027e961",
   "metadata": {},
   "source": [
    "We have now two tests. If we want to run this specific test, use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dea33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run pytest tests/test_agent.py::test_agent_code -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f337cbe",
   "metadata": {},
   "source": [
    "## Benefits of Agent Testing\n",
    "\n",
    "It's important to test your agents. We can tweak some parameters while improving one thing, and accidentally break something else.\n",
    "\n",
    "But with tests, if you change prompts or configuration later, tests ensure your requirements are still satisfied. Manual \"vibe checking\" can miss subtle regressions.\n",
    "\n",
    "So when you modify one aspect of your agent, tests verify that other functionality still works correctly. Without tests, you might forget to check all the different capabilities.\n",
    "\n",
    "We run tests automatically and ongoingly, so we can catch issues before they reach production. This is especially important for agents where behavior can be unpredictable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
