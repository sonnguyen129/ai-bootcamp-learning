{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766ff332",
   "metadata": {},
   "source": [
    "# Testing with Structured Output\n",
    "\n",
    "Making your output structured is always a good idea. It forces the LLM to generate all the required fields in the output, so it won't \"forget\" about important things.\n",
    "\n",
    "And in addition to this and other benefits, testing your agent becomes much simpler.\n",
    "\n",
    "Let's make our agent output more structured and predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c2631",
   "metadata": {},
   "source": [
    "# Defining the Output Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d7d74",
   "metadata": {},
   "source": [
    "Let's use this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    title: str\n",
    "    filename: str\n",
    "\n",
    "class Section(BaseModel):\n",
    "    heading: str\n",
    "    content: str\n",
    "    references: list[Reference]\n",
    "\n",
    "class SearchResultArticle(BaseModel):\n",
    "    title: str\n",
    "    sections: list[Section]\n",
    "    references: list[Reference]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc638246",
   "metadata": {},
   "source": [
    "# Updating Agent Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd09ea",
   "metadata": {},
   "source": [
    "Since we don't need the full URL anymore (we can build it ourselves), we no longer need to ask the LLM to do it.\n",
    "\n",
    "Let's tweak the prompt slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9eda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a helpful assistant that answers questions by searching\n",
    "the documentation.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- For every user query, you must perform at least 3 separate searches\n",
    "    to gather enough context and verify accuracy.  \n",
    "- Each search should use a different angle, phrasing, or keyword\n",
    "    variation of the user's query. \n",
    "- After performing all searches, write a concise, accurate answer\n",
    "    that synthesizes the findings.  \n",
    "- For each section, include references listing all the sources\n",
    "    you used to write that section.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b367036",
   "metadata": {},
   "source": [
    "# Configuring Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54960f86",
   "metadata": {},
   "source": [
    "Don't forget to add output_type to the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777166a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent():\n",
    "    tools = search_tools.prepare_search_tools()\n",
    "\n",
    "    return Agent(\n",
    "        name=\"search\",\n",
    "        instructions=search_instructions,\n",
    "        tools=[tools.search],\n",
    "        model=\"openai:gpt-4o-mini\",\n",
    "        output_type=SearchResultArticle,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fade2",
   "metadata": {},
   "source": [
    "# Output Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801dd640",
   "metadata": {},
   "source": [
    "We can add this method for printing formatted articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_article(\n",
    "        self,\n",
    "        base_url: str = \"https://github.com/evidentlyai/docs/blob/main\"\n",
    ") -> str:\n",
    "    output = [f\"# {self.title}\\n\"]\n",
    "\n",
    "    for section in self.sections:\n",
    "        output.append(f\"## {section.heading}\\n\")\n",
    "        output.append(f\"{section.content}\\n\")\n",
    "\n",
    "        if section.references:\n",
    "            output.append(\"### References\\n\")\n",
    "            for ref in section.references:\n",
    "                output.append(f\"- {ref.title} ({base_url}/{ref.filename})\\n\")\n",
    "\n",
    "        output.append(\"\\n\")\n",
    "\n",
    "    if self.references:\n",
    "        output.append(\"## All References\\n\")\n",
    "        for ref in self.references:\n",
    "            output.append(f\"- {ref.title} ({ref.filename})\\n\")\n",
    "\n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63a7aa",
   "metadata": {},
   "source": [
    "# Enhanced Testing with Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b40e2a",
   "metadata": {},
   "source": [
    "Now in tests we can access the fields of the new object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d211b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_code():\n",
    "    result = run_agent_sync(\"How do I implement LLM as a Judge eval?\")\n",
    "\n",
    "    tool_calls = get_tool_calls(result)\n",
    "    assert len(tool_calls) >= 3, \"Less than 3 tool calls found\"\n",
    "\n",
    "    article = result.output\n",
    "    print(article.format_article())\n",
    "\n",
    "    assert len(article.sections) > 0, \"No sections found\"\n",
    "    assert len(article.references) > 0, \"No references found\"\n",
    "\n",
    "    code_found = False\n",
    "\n",
    "    for section in article.sections:\n",
    "        if \"```python\" in section.content:\n",
    "            code_found = True\n",
    "\n",
    "        assert len(section.references) > 0, f\"No references found in section {section.heading}\"\n",
    "\n",
    "    assert code_found, \"No code block found in any section\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e95e6",
   "metadata": {},
   "source": [
    "# Additional Test Ideas\n",
    "\n",
    "We can add more checks.\n",
    "\n",
    "For example, we can check that files from references actually exist in the documentation.\n",
    "\n",
    "When building a deep research agent, we recorded the search queries in output. But we had a problem that these search queries didn't match the actual tool calls. With testing it's easy to catch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0ebf4",
   "metadata": {},
   "source": [
    "# Test-Driven Development Approach\n",
    "\n",
    "We can continue adding more tests and more scenarios with different prompts.\n",
    "\n",
    "The key principle is: when you discover that something doesn't work the way it's intended, you write a test that reproduces this behavior. Then you fix the issue, and finally you run all tests to make sure you don't break anything else.\n",
    "\n",
    "This approach ensures your agent remains reliable and predictable as you make improvements and add new features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
