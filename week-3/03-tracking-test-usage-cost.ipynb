{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d3e222",
   "metadata": {},
   "source": [
    "# Tracking Test Usage and Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0453f7",
   "metadata": {},
   "source": [
    "Because we use OpenAI, running tests isn't free. Each test makes API calls that cost money.\n",
    "\n",
    "It would be nice to know how much money we actually spend on running our test cases.\n",
    "\n",
    "One thing you should do is to create a separate key for tests only. This way you can easily monitor how much you spend on tests in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75ccdd",
   "metadata": {},
   "source": [
    "# Usage Tracking Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b790748",
   "metadata": {},
   "source": [
    "We can get usage information from `run_results.usage()` and collect all the usages from all test runs to see the total costs.\n",
    "\n",
    "An alternative would be monkey-patching the run method from Pydantic AI's Agent class, so during tests all usages are automatically collected. (You can do the same with any other framework you use.)\n",
    "\n",
    "At the end, when we finish running all the tests, we can use it to generate a comprehensive cost report.\n",
    "\n",
    "Let's create a file `patch_agent.py` (`test/patch_agent.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62adc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import inspect\n",
    "from typing import Any\n",
    "\n",
    "# Don't import pydantic_ai types here\n",
    "\n",
    "# Central in-process collector\n",
    "_USAGE_RECORDS = {}\n",
    "\n",
    "\n",
    "def _record_usage_from_result(agent, result):\n",
    "    try:\n",
    "        model_name = agent.model.model_name\n",
    "        usage = result.usage()\n",
    "\n",
    "        if usage is None:\n",
    "            return\n",
    "\n",
    "        if model_name not in _USAGE_RECORDS:\n",
    "            _USAGE_RECORDS[model_name] = []\n",
    "        _USAGE_RECORDS[model_name].append(usage)\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "\n",
    "def _wrap_run_callable(orig_run):\n",
    "    \"\"\"Return an async wrapper which calls orig_run and records usage.\"\"\"\n",
    "\n",
    "    @wraps(orig_run)\n",
    "    async def wrapped(self, *args, **kwargs):\n",
    "        result = await orig_run(self, *args, **kwargs)\n",
    "        try:\n",
    "            _record_usage_from_result(self, result)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return result\n",
    "\n",
    "    return wrapped\n",
    "\n",
    "\n",
    "def install_usage_collector() -> bool:\n",
    "    installed = False\n",
    "    \n",
    "    from pydantic_ai import Agent as PAAgent  # type: ignore\n",
    "\n",
    "    try:\n",
    "        if not hasattr(PAAgent, \"_orig_run_for_tests\"):\n",
    "            orig_run = getattr(PAAgent, \"run\", None)\n",
    "            setattr(PAAgent, \"_orig_run_for_tests\", orig_run)\n",
    "            if orig_run is not None and inspect.iscoroutinefunction(orig_run):\n",
    "                PAAgent.run = _wrap_run_callable(orig_run)\n",
    "            installed = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return installed\n",
    "\n",
    "\n",
    "def get_usage_aggregated():\n",
    "    from pydantic_ai import RunUsage  # type: ignore\n",
    "\n",
    "    aggregated = {}\n",
    "\n",
    "    for model_name, usage_list in _USAGE_RECORDS.items():\n",
    "        total = RunUsage()\n",
    "\n",
    "        for usage in usage_list:\n",
    "            total.incr(usage)\n",
    "\n",
    "        aggregated[model_name] = total\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def print_report_usage() -> Any:\n",
    "    from toyaikit.pricing import PricingConfig\n",
    "    pricing = PricingConfig()\n",
    "    print('\\n=== USAGE REPORT ===')\n",
    "\n",
    "    aggregated = get_usage_aggregated()\n",
    "\n",
    "    for model_name, usage in aggregated.items():\n",
    "        cost = pricing.calculate_cost(\n",
    "            model=model_name,\n",
    "            input_tokens=usage.input_tokens,\n",
    "            output_tokens=usage.output_tokens\n",
    "        )\n",
    "        print(f'Model: {model_name}, Cost: {cost}')\n",
    "\n",
    "    print('====================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ffd6d",
   "metadata": {},
   "source": [
    "Here is what's happening:\n",
    "\n",
    "- _USAGE_RECORDS is a global dictionary that stores usage data for each model.\n",
    "- _record_usage_from_result() extracts usage information from an agent result and stores it in our global collector.\n",
    "- _wrap_run_callable() creates a wrapper around the original agent run method. It calls the original run method and then records the usage data.\n",
    "- install_usage_collector() monkey-patches the Pydantic AI Agent class. It replaces the original run method with our wrapped version that collects usage data.\n",
    "- get_usage_aggregated() combines all individual usage records for each model into totals.\n",
    "- print_report_usage() generates and displays a cost report using ToyAIKit's pricing information. It shows the total cost for each model used during testing.\n",
    "\n",
    "We need are using ToyAIKit for calculating the prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0341b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add toyaikit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03892ca7",
   "metadata": {},
   "source": [
    "# Integration with Test Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb8ca2",
   "metadata": {},
   "source": [
    "Create `conftest.py` (`tests/conftest.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import patch_agent\n",
    "patch_agent.install_usage_collector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4b290",
   "metadata": {},
   "source": [
    "It's very important to add this at the top. We want to make sure we install the usage collector before the first instance of Agent is created. Otherwise our monkey-patching won't work properly.\n",
    "\n",
    "Finally, we show the cost report when the test session finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992dd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytest_sessionfinish(session, exitstatus):\n",
    "    from tests import patch_agent\n",
    "    patch_agent.print_report_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a9db6",
   "metadata": {},
   "source": [
    "This pytest hook runs automatically after all tests complete. It provides a summary of total API costs incurred during the test session, helping you monitor and budget for testing expenses.\n",
    "\n",
    "You can also save these results somewhere, so later you can analyze the costs.\n",
    "\n",
    "That's how we can monitor costs of a single test set run.\n",
    "\n",
    "So the complete `tests/conftest.py` looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5daee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import patch_agent\n",
    "patch_agent.install_usage_collector()\n",
    "\n",
    "def pytest_sessionfinish(session, exitstatus):\n",
    "    from tests import patch_agent\n",
    "    patch_agent.print_report_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
