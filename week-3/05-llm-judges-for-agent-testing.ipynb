{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9d167b",
   "metadata": {},
   "source": [
    "# LLM Judges for Agent Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774c90b",
   "metadata": {},
   "source": [
    "In this lesson we will see how we can use LLMs to test other LLMs.\n",
    "\n",
    "I tried asking this question: \"what is llm as a judge evaluation\".\n",
    "\n",
    "The result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8793803",
   "metadata": {},
   "source": [
    "```bash\n",
    "User: what is llm as a judge evaluation\n",
    "TOOL CALL (search): search({\"query\": \"LLM judge evaluation in legal settings\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM in judicial decision making\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM legal profession evaluation use cases\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d826709",
   "metadata": {},
   "outputs": [],
   "source": [
    "This \"legal\" and \"judicial\" interpretation doesn't make any sense for our documentation use case.\n",
    "\n",
    "Also sometimes the agent goes overboard with searches:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c37f93",
   "metadata": {},
   "source": [
    "```bash\n",
    "TOOL CALL (search): search({\"query\": \"LLM as a judge evaluation\"})\n",
    "TOOL CALL (search): search({\"query\": \"Evaluating language models as judges\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM legal evaluation performance\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM evaluator use cases\"})\n",
    "TOOL CALL (search): search({\"query\": \"how to use LLM for evaluation\"})\n",
    "TOOL CALL (search): search({\"query\": \"language model judge effectiveness\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM evaluation methodology\"})\n",
    "TOOL CALL (search): search({\"query\": \"training LLM for evaluations\"})\n",
    "TOOL CALL (search): search({\"query\": \"applications of LLM in assessments\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM effectiveness in judicial evaluations\"})\n",
    "TOOL CALL (search): search({\"query\": \"LLM evaluation framework best practices\"})\n",
    "TOOL CALL (search): search({\"query\": \"using language models for accuracy evaluation\"})\n",
    "TOOL CALL (search): search({\"query\": \"custom language model evaluations\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9599c",
   "metadata": {},
   "source": [
    "Probably because it can't find what it's looking for (judicial stuff), it keeps searching for more and more variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d9a03",
   "metadata": {},
   "source": [
    "# Simple Keyword-Based Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ea8e8",
   "metadata": {},
   "source": [
    "We already know how to write a basic test that checks that words like \"judicial\" and \"legal\" don't appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_judicial_terms_in_search_queries():\n",
    "    result = run_agent_sync(\"what is llm as a judge evaluation\")\n",
    "\n",
    "    tool_calls = get_tool_calls(result)\n",
    "\n",
    "    forbidden_terms = ['judicial', 'court', 'law', 'legal']\n",
    "\n",
    "    for call in tool_calls:\n",
    "        query = call.args['query'].lower()\n",
    "\n",
    "        for term in forbidden_terms:\n",
    "            assert term not in query, f\"Forbidden term '{term}' found in search query: {query}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77f5a1",
   "metadata": {},
   "source": [
    "But we can't anticipate all possible legal terminology in advance, so some words like \"jurisprudence\" can slip through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f084f02",
   "metadata": {},
   "source": [
    "# Creating an LLM Judge\n",
    "\n",
    "Instead, we can create a Judge - an LLM that evaluates the output of another LLM according to specific criteria. It will be smart enough to recognize if \"jurisprudence\" sneaks in.\n",
    "\n",
    "Let's create another test file: `test_agent_judge.py` (`test/test_agent_judge.py`). It will contain our judge as well as the tests.\n",
    "\n",
    "First, define the judge structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_instructions = \"\"\"\n",
    "you are an expert judge evaluating the performance of an AI search agent.\n",
    "\"\"\"\n",
    "\n",
    "class JudgeCriterion(BaseModel):\n",
    "    criterion_description: str\n",
    "    passed: bool\n",
    "    judgement: str\n",
    "\n",
    "\n",
    "class JudgeFeedback(BaseModel):\n",
    "    criteria: list[JudgeCriterion]\n",
    "    feedback: str\n",
    "\n",
    "\n",
    "def create_judge():\n",
    "    judge = Agent(\n",
    "        name=\"judge\",\n",
    "        instructions=judge_instructions,\n",
    "        model=\"openai:gpt-4o-mini\",\n",
    "        output_type=JudgeFeedback,\n",
    "    )\n",
    "    return judge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd516598",
   "metadata": {},
   "source": [
    "This creates an agent that focuses solely on evaluation. (Technically, it's not an agent because it doesn't have any tools, but whatever.)\n",
    "\n",
    "The structured output ensures we get consistent feedback across all criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150b2b2",
   "metadata": {},
   "source": [
    "# Agent Evaluation Funcion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300fc4b",
   "metadata": {},
   "source": [
    "Create this evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bec343",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_agent_performance(\n",
    "        criteria: list[str],\n",
    "        result: AgentRunResult,\n",
    "        output_transformer: callable = None\n",
    ") -> JudgeFeedback:\n",
    "    judge = create_judge()\n",
    "\n",
    "    tool_calls = get_tool_calls(result)\n",
    "\n",
    "    output = result.output\n",
    "    if output_transformer is not None:\n",
    "        output = output_transformer(output)\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Evaluate the agent's performance based on the following criteria:\n",
    "    <CRITERIA>\n",
    "    {'\\n'.join(criteria)}\n",
    "    </CRITERIA>\n",
    "    \n",
    "    The agent's final output was:\n",
    "    <AGENT_OUTPUT>\n",
    "    {output}\n",
    "    </AGENT_OUTPUT>\n",
    "    \n",
    "    Tool calls:\n",
    "    <TOOL_CALLS>\n",
    "    {'\\n'.join([str(c) for c in tool_calls])}\n",
    "    </TOOL_CALLS>\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Judge evaluating with prompt:\")\n",
    "    print(\"-----\")\n",
    "    print(user_prompt)\n",
    "    print(\"-----\")\n",
    "\n",
    "    eval_results = await judge.run(\n",
    "        user_prompt=user_prompt\n",
    "    )\n",
    "\n",
    "    return eval_results.output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea97fb7",
   "metadata": {},
   "source": [
    "This function takes evaluation criteria as input and creates a comprehensive prompt for the judge.\n",
    "\n",
    "The output_transformer allows us to format the agent's output (like converting structured data to readable text) before evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940e795",
   "metadata": {},
   "source": [
    "# Using the Judge in Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426529b8",
   "metadata": {},
   "source": [
    "Use it in your tests like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ceaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.asyncio\n",
    "async def test_no_judicial_terms_in_search_queries():\n",
    "    criteria = [\n",
    "        \"agent makes 3 search calls\",\n",
    "        \"the references are relevant to the topic\",\n",
    "        \"each section has references\",\n",
    "        \"all search queries are free of judicial terms (except 'judge')\",\n",
    "        \"the article contains properly formatted python code examples\",\n",
    "    ]\n",
    "\n",
    "    result = await run_agent(\"what is llm as a judge evaluation\")\n",
    "\n",
    "    eval_results = await evaluate_agent_performance(\n",
    "        criteria,\n",
    "        result,\n",
    "        output_transformer=lambda x: x.format_article()\n",
    "    )\n",
    "\n",
    "    print(eval_results)\n",
    "\n",
    "    for criterion in eval_results.criteria:\n",
    "        print(criterion)\n",
    "        assert criterion.passed, f\"Criterion failed: {criterion.criterion_description}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99016591",
   "metadata": {},
   "source": [
    "This approach allows you to define complex evaluation criteria in natural language rather than trying to code every possible edge case.\n",
    "\n",
    "I'd also move it out into a separate file, like judge.py.\n",
    "\n",
    "To run async tests with pytest, we need to install a pytest plugin:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add --dev pytest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5536c",
   "metadata": {},
   "source": [
    "# Filtering Internal Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f1b17",
   "metadata": {},
   "source": [
    "When running this test, I noticed it includes a final_result tool call in the prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6c5c5",
   "metadata": {},
   "source": [
    "```bash\n",
    "Tool calls:\n",
    "<TOOL_CALLS>\n",
    "ToolCall(name='search', args={'query': 'LLM as a judge evaluation'})\n",
    "ToolCall(name='search', args={'query': 'judge evaluation using LLM'})\n",
    "ToolCall(name='search', args={'query': 'legal evaluation LLM applications'})\n",
    "ToolCall(name='final_result', args={'title': 'LLM as a Judge in Evaluation Processes', 'sections' ...})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcbdb7",
   "metadata": {},
   "source": [
    "This is an internal virtual tool that Pydantic AI uses for structured output.\n",
    "\n",
    "See [here](https://ai.pydantic.dev/output/):\n",
    "\n",
    "By default, Pydantic AI leverages the model's tool calling capability to make it return\n",
    "\n",
    "structured data.\n",
    "\n",
    "Other libraries (like LangChain) also use this pattern.\n",
    "\n",
    "We don't need it in our evaluation: it consumes input tokens and not really useful. So let's filter it out: (This is in `utils.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai._output import DEFAULT_OUTPUT_TOOL_NAME\n",
    "\n",
    "def get_tool_calls(result) -> List[ToolCall]:\n",
    "    \"\"\"Extract tool-call parts from an agent result and return them as ToolCall objects.\"\"\"\n",
    "    calls: List[ToolCall] = []\n",
    "\n",
    "    for m in result.new_messages():\n",
    "        for p in m.parts:\n",
    "            kind = p.part_kind\n",
    "\n",
    "            if kind == 'tool-call':\n",
    "                if p.tool_name == DEFAULT_OUTPUT_TOOL_NAME:\n",
    "                    continue\n",
    "\n",
    "                call = ToolCall(\n",
    "                    name=p.tool_name,\n",
    "                    args=json.loads(p.args)\n",
    "                )\n",
    "\n",
    "                calls.append(call)\n",
    "\n",
    "    return calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59639aff",
   "metadata": {},
   "source": [
    "# Improving Agent Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36aff70",
   "metadata": {},
   "source": [
    "The initial test fails with feedback like:\n",
    "\n",
    "\"The agent's performance was close to meeting the criteria but fell short in a few key areas, including the number of search calls, the use of judicial terms in search queries, and the lack of Python code examples.\"\n",
    "\n",
    "Let's fix this by updating the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d308231",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a search assistant for the Evidently documentation.\n",
    "\n",
    "Evidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\n",
    "It provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n",
    "\n",
    "Your task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- For every user query, you must perform at least 3 separate searches\n",
    "    to gather enough context and verify accuracy.  \n",
    "- Each search should use a different angle, phrasing, or keyword\n",
    "    variation of the user's query. \n",
    "- Keep all searches relevant to Evidently and centered on technical\n",
    "    or conceptual details from its documentation.\n",
    "- After performing all searches, write a concise, accurate answer\n",
    "    that synthesizes the findings.  \n",
    "- For each section, include references listing all the sources\n",
    "    you used to write that section.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985e14f",
   "metadata": {},
   "source": [
    "We added more context about the Evidently library so the agent knows what kind of search terms to formulate.\n",
    "\n",
    "Point 3 specifically guides the agent to stay relevant to the documentation context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce24f89",
   "metadata": {},
   "source": [
    "# Test Results and Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031deaa0",
   "metadata": {},
   "source": [
    "Let's test the updated agent.\n",
    "\n",
    "The original test (without a judge) now passes.\n",
    "\n",
    "However, the judge test fails on the Python code requirement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992221c",
   "metadata": {},
   "source": [
    "```bash\n",
    "E           AssertionError: Criterion failed: The article contains properly formatted python code examples\n",
    "E           assert False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc52e40",
   "metadata": {},
   "source": [
    "In this particular case, we might not actually need Python code. We can force it in instructions, but it might lead to hallucination or excessive searching. So let's drop this criterion from this particular test.\n",
    "\n",
    "The test now passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af377ec",
   "metadata": {},
   "source": [
    "# Cost Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111bd94",
   "metadata": {},
   "source": [
    "Since we use Pydantic AI for both the agent and the judge, our cost report includes the total expenses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f71ac9",
   "metadata": {},
   "source": [
    "```bash\n",
    "Model: gpt-4o-mini\n",
    "Cost: CostInfo(input_cost=0.0052, output_cost=0.0014, total_cost=0.0066)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87cc11e",
   "metadata": {},
   "source": [
    "# Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748a7d6",
   "metadata": {},
   "source": [
    "I noticed that the agent now includes \"Evidently\" in all search queries, which isn't always necessary. We can create additional tests to catch this behavior, see them fail, and then refine our prompts accordingly.\n",
    "\n",
    "Other evaluation criteria we could implement:\n",
    "\n",
    "- Reference Quality: Check that references are relevant to their sections\n",
    "\n",
    "- Content Completeness: Ensure each section has sufficient content\n",
    "\n",
    "- Search Efficiency: Verify the agent doesn't make redundant searches\n",
    "\n",
    "For some of the checks, we might even give the judge agent additional tools to perform more sophisticated evaluations. Then it will become a real agent.\n",
    "\n",
    "This gives us a more flexible and powerful mechanism for testing our agents.\n",
    "\n",
    "Note that judges are not a replacement for \"usual\" testing, but an addition. For many cases you probably don't need a judge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
